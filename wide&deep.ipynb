{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \n",
    "    \"occupation\", \"relationship\", \"race\", \"gender\", \"capital_gain\", \"capital_loss\", \n",
    "    \"hours_per_week\", \"native_country\", \"income_bracket\"] ##数据所有的列\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    \"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \n",
    "    \"race\", \"gender\", \"native_country\"\n",
    "] ##类别数据\n",
    "\n",
    "NUMBER_COLUMNS = [\n",
    "    \"age\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"\n",
    "]##连续数据列\n",
    "\n",
    "LABEL_COLUMN = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##数据处理，特征工程\n",
    "def featrue_preprocessing():\n",
    "    train_data = pd.read_csv('./data/adult.data', names=COLUMNS)\n",
    "    ##这里直接删除缺失的数据吧\n",
    "    train_data.dropna(how='any',axis=0)\n",
    "    train_data['train'] = 1\n",
    "    test_data = pd.read_csv('./data/adult.test', names=COLUMNS)\n",
    "    ##这里直接删除缺失的数据吧\n",
    "    test_data.dropna(how='any',axis=0)\n",
    "    test_data['train'] = 0\n",
    "    \n",
    "    data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "    data = data.replace(' ?', ' Never-worked')\n",
    "    print(data['workclass'].unique())\n",
    "#     return data\n",
    "    data = data.dropna(axis=0,how='any')\n",
    "    print(data['workclass'].unique())\n",
    "    ##将收入档次income_bracket离散化\n",
    "    data[LABEL_COLUMN] = data['income_bracket'].apply(lambda x: '>50' in x).astype(int)\n",
    "    y = data[LABEL_COLUMN].values\n",
    "    \n",
    "    data = data.drop(['income_bracket'], axis=1)\n",
    "    for c in CATEGORICAL_COLUMNS:\n",
    "        le = LabelEncoder()\n",
    "        data[c] = le.fit_transform(data[c])\n",
    "    train_size = len(train_data)\n",
    "    train_X = data[data['train'] == 1].drop(['label'], axis=1)\n",
    "    train_y = data[data['train'] == 1]['label'].values\n",
    "    test_X = data[data['train'] == 0].drop(['label'], axis=1)\n",
    "    test_y = data[data['train'] == 0]['label'].values\n",
    "    train_X_cate = train_X[CATEGORICAL_COLUMNS].to_numpy()\n",
    "    test_X_cate = test_X[CATEGORICAL_COLUMNS].to_numpy()\n",
    "    train_X_num = train_X[NUMBER_COLUMNS].to_numpy()\n",
    "    test_X_num = test_X[NUMBER_COLUMNS].to_numpy()\n",
    "    \n",
    "    ##进行标准化处理(归一化，转换成均值为0标准差为1的正态分布)\n",
    "    scaler = StandardScaler()\n",
    "    train_X_num = scaler.fit_transform(train_X_num)\n",
    "    test_X_num = scaler.fit_transform(test_X_num),\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "    train_cate_poly = poly.fit_transform(train_X_cate)\n",
    "    test_cate_poly = poly.fit_transform(test_X_cate)\n",
    "#     data = pd.concat([data, pd.DataFrame(np.vstack([train_cate_poly, test_cate_poly]), columns=['poly_featrue_{}'.format(i) for i in range(37)] )] )\n",
    "#     data['poly_featrue'] = np.vstack([train_cate_poly, test_cate_poly])\n",
    "#     data['test_cate_poly'] = test_cate_poly\n",
    "    return {\n",
    "        'train_X': train_X,\n",
    "        'test_X': test_X,\n",
    "        'train_y': train_y,\n",
    "        'test_y': test_y,\n",
    "        'train_X_cate': train_X_cate,\n",
    "        'test_X_cate': test_X_cate,\n",
    "        'train_X_num': train_X_num,\n",
    "        'test_X_num': test_X_num,\n",
    "        'data': data,\n",
    "        'train_cate_poly': train_cate_poly,\n",
    "        'test_cate_poly': test_cate_poly\n",
    "    }\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' State-gov' ' Self-emp-not-inc' ' Private' ' Federal-gov' ' Local-gov'\n",
      " ' Never-worked' ' Self-emp-inc' ' Without-pay' nan]\n",
      "[' State-gov' ' Self-emp-not-inc' ' Private' ' Federal-gov' ' Local-gov'\n",
      " ' Never-worked' ' Self-emp-inc' ' Without-pay']\n"
     ]
    }
   ],
   "source": [
    "##定义输入值的占位符\n",
    "# embed_dim = 16 ##选取特征维度的的四分之一\n",
    "#定义交叉积的维度\n",
    "cross_dim = featrue_preprocessing().get('train_cate_poly').shape[1]\n",
    "\n",
    "def variable_init(data):\n",
    "    embed_dict = {}\n",
    "    for cate in CATEGORICAL_COLUMNS:\n",
    "        embed_shape = int(data[cate].nunique())\n",
    "        if cate not in embed_dict:\n",
    "            embed_dict[cate] = {}\n",
    "        embed_dict[cate]['holder'] = tf.placeholder(tf.int32, [None, 1], name='{}_inputs'.format(cate))\n",
    "        embed_dict[cate]['dims'] = 16\n",
    "        embed_dict[cate]['shape'] = embed_shape\n",
    "    \n",
    "#     for numr in NUMBER_COLUMNS:\n",
    "#         pass\n",
    "    numerical_inputs = tf.placeholder(tf.float32, [None, len(NUMBER_COLUMNS)], name='numerical_inputs')\n",
    "#     cross_featrue_inputs = tf.placeholder(tf.float32, [None, cross_dim], name='cross_featrue_inputs')\n",
    "    label = tf.placeholder(tf.float32, [None, 1], name='label')\n",
    "#     categorical_inputs = tf.placeholder(tf.int32, [None, len(CATEGORICAL_COLUMNS)], name='categorical_inputs')\n",
    "    \n",
    "    return embed_dict, numerical_inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义超参数\n",
    "learning_rate = 0.001\n",
    "\n",
    "batch_size = 128\n",
    "max_steps = 30000\n",
    "epoch = 5\n",
    "##其他的正则化率、学习率衰减率等....暂不使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##初始化类别数据的embedding层\n",
    "\n",
    "###这里初始化有错误\n",
    "\n",
    "def init_embedding(embed_dict):\n",
    "    cate_embed_list = []\n",
    "    with tf.name_scope('categorical_embedding'):\n",
    "        for key_name, item in embed_dict.items():\n",
    "            embed_matrix = tf.Variable(tf.random_normal([item['shape'], item['dims']], stddev=1.0, mean=0.0), name='{}_embed_matrix'.format(key_name))\n",
    "            ##tf.nn.embedding_lookup 查找矩阵的序号对应的向量， 所以embedding_lookup的维度不会发生改变\n",
    "            ##这里的tf.nn.embedding_lookup不是矩阵乘法，所以维度不会发生变化...\n",
    "            embed_layer = tf.nn.embedding_lookup(embed_matrix, item['holder'], name='{}_embed_layer'.format(key_name))  ## shape:item.shape[0], embed_dim\n",
    "            embed_layer = tf.reduce_sum(embed_layer, axis=1, keep_dims=True) ## shape: 1, embed_dim\n",
    "            embed_layer = tf.squeeze(embed_layer,axis=1) ## shape:embed_dim,\n",
    "            cate_embed_list.append(embed_layer)\n",
    "#         embed_matrix = tf.Variable(tf.random_normal([42, 16], stddev=1.0, mean=0.0), name='cate_embed_matrix')\n",
    "#         embed_layer = tf.nn.embedding_lookup(embed_matrix, categorical_inputs, name='cate_embed_layer') ##shape=1x16\n",
    "    return cate_embed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##将所有特征组合起来\n",
    "def deep_net(categorical_featrue, numerical_featrue):\n",
    "#     numerical_featrue = tf.reshape(numerical_featrue, [-1, 8])\n",
    "    print(numerical_featrue)\n",
    "#     numerical_featrue = tf.layers.dense(numerical_featrue, 8, activation=tf.nn.relu, name='numerical_featrue_fc')\n",
    "#     numerical_featrue = tf.reshape(numerical_featrue, [-1, 8, 16])\n",
    "    print(categorical_featrue)\n",
    "#     categorical_featrue.append(numerical_featrue)\n",
    "    all_featrue_layer = tf.concat(categorical_featrue + [numerical_featrue], axis=-1, name='all_featrue_combine')\n",
    "    print(all_featrue_layer)\n",
    "#     all_featrue_layer_relu = tf.nn.relu(all_featrue_layer)\n",
    "    \n",
    "    ##三层全连接\n",
    "    fc1 = tf.layers.dense(all_featrue_layer, 64, activation=tf.nn.relu, name='all_featrue_fc1')\n",
    "    fc2 = tf.layers.dense(fc1, 16, activation=tf.nn.relu, name='all_featrue_fc2')\n",
    "#     fc3 = tf.layers.dense(fc2, 256, activation=tf.nn.relu, name='all_featrue_fc3')\n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_log_out(deep_fc, wide):\n",
    "    wide_and_deep_combine = tf.concat([deep_fc, wide], -1, name='wide_and_deep_combine')\n",
    "    wide_and_deep_out = tf.layers.dense(wide_and_deep_combine, 1, activation=tf.nn.sigmoid, name='wide_and_deep_out')\n",
    "    \n",
    "    return wide_and_deep_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' State-gov' ' Self-emp-not-inc' ' Private' ' Federal-gov' ' Local-gov'\n",
      " ' Never-worked' ' Self-emp-inc' ' Without-pay' nan]\n",
      "[' State-gov' ' Self-emp-not-inc' ' Private' ' Federal-gov' ' Local-gov'\n",
      " ' Never-worked' ' Self-emp-inc' ' Without-pay']\n",
      "Tensor(\"numerical_inputs:0\", shape=(?, 5), dtype=float32)\n",
      "[<tf.Tensor 'categorical_embedding/Squeeze:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'categorical_embedding/Squeeze_1:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'categorical_embedding/Squeeze_2:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'categorical_embedding/Squeeze_3:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'categorical_embedding/Squeeze_4:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'categorical_embedding/Squeeze_5:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'categorical_embedding/Squeeze_6:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'categorical_embedding/Squeeze_7:0' shape=(?, 16) dtype=float32>]\n",
      "Tensor(\"all_featrue_combine:0\", shape=(?, 133), dtype=float32)\n",
      "Tensor(\"all_featrue_fc2/Relu:0\", shape=(?, 16), dtype=float32)\n",
      "Tensor(\"logloss/wide_and_deep_out/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"label:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"Squeeze:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"Squeeze_1:0\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "##构建计算图\n",
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    data_dict = featrue_preprocessing()\n",
    "    data = data_dict.get('data')\n",
    "    embed_dict, numerical_inputs, label = variable_init(data)\n",
    "    \n",
    "    cate_embed_layer = init_embedding(embed_dict)\n",
    "    \n",
    "    deep_fc = deep_net(categorical_featrue=cate_embed_layer, numerical_featrue=numerical_inputs)\n",
    "    print(deep_fc)\n",
    "#     print(cross_featrue_inputs)\n",
    "    with tf.name_scope('logloss'):\n",
    "#         cross_featrue_layer = tf.layers.dense(cross_featrue_inputs, 16, activation=tf.nn.relu, name='cross_featrue_fc')\n",
    "#         print(cross_featrue_layer)\n",
    "#         cross_featrue_layer = tf.squeeze(cross_featrue_layer, axis=1)\n",
    "#         wide_and_deep_combine = tf.concat([deep_fc, cross_featrue_layer], -1, name='wide_and_deep_combine')\n",
    "        wide_and_deep_out = tf.layers.dense(deep_fc, 1, name='wide_and_deep_out')\n",
    "#         out_label = tf.reduce_mean(wide_and_deep_out, axis=1)\n",
    "#         out_label = tf.cast(out_label, tf.int32)\n",
    "#         out_label = tf.squeeze(out_label)\n",
    "        print(wide_and_deep_out)\n",
    "        print(label)\n",
    "        \n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(labels=label, logits=wide_and_deep_out)\n",
    "    loss = tf.reduce_mean(cost)\n",
    "    \n",
    "        ##优化器\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     gradients = optimizer.compute_gradients(loss)\n",
    "#     train_op = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "    y_ = tf.nn.sigmoid(wide_and_deep_out, name='sigmoid_out')\n",
    "    label_acc = tf.squeeze(label, 1)\n",
    "    print(label_acc)\n",
    "    y_acc = tf.squeeze(y_, 1)\n",
    "    print(y_acc)\n",
    "    crorent_prediction = tf.equal(label_acc, y_acc)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(crorent_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size):\n",
    "    for start in range(0, len(X), batch_size):\n",
    "        end = min(start + batch_size, len(X))\n",
    "        yield X[start:end], y[start: end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-06T15:17:45.830366 Batch    0/305 train_loss = 35.191\n",
      "2020-07-06T15:17:45.861726 Batch   20/305 train_loss = 4.249\n",
      "2020-07-06T15:17:45.890488 Batch   40/305 train_loss = 0.691\n",
      "2020-07-06T15:17:45.918894 Batch   60/305 train_loss = 0.375\n",
      "2020-07-06T15:17:45.946573 Batch   80/305 train_loss = 0.420\n",
      "2020-07-06T15:17:45.974415 Batch  100/305 train_loss = 0.319\n",
      "2020-07-06T15:17:46.000895 Batch  120/305 train_loss = 0.337\n",
      "2020-07-06T15:17:46.030886 Batch  140/305 train_loss = 0.365\n",
      "2020-07-06T15:17:46.063170 Batch  160/305 train_loss = 1.700\n",
      "2020-07-06T15:17:46.094384 Batch  180/305 train_loss = 0.295\n",
      "2020-07-06T15:17:46.123881 Batch  200/305 train_loss = 0.413\n",
      "2020-07-06T15:17:46.152633 Batch  220/305 train_loss = 0.417\n",
      "2020-07-06T15:17:46.182957 Batch  240/305 train_loss = 0.565\n",
      "2020-07-06T15:17:46.209679 Batch  260/305 train_loss = 0.433\n",
      "2020-07-06T15:17:46.236583 Batch  280/305 train_loss = 0.854\n",
      "2020-07-06T15:17:46.263011 Batch  300/305 train_loss = 0.515\n",
      "2020-07-06T15:17:46.803511 Batch    0/76 accuracy = 0.82\n",
      "2020-07-06T15:17:53.545357 Batch   20/76 accuracy = 0.80\n",
      "2020-07-06T15:18:00.522180 Batch   40/76 accuracy = 0.88\n",
      "2020-07-06T15:18:07.580543 Batch   60/76 accuracy = 0.85\n",
      "2020-07-06T15:18:12.914164 Batch   15/305 train_loss = 1.450\n",
      "2020-07-06T15:18:12.939836 Batch   35/305 train_loss = 0.446\n",
      "2020-07-06T15:18:12.966195 Batch   55/305 train_loss = 0.342\n",
      "2020-07-06T15:18:12.991743 Batch   75/305 train_loss = 0.363\n",
      "2020-07-06T15:18:13.017533 Batch   95/305 train_loss = 0.353\n",
      "2020-07-06T15:18:13.043506 Batch  115/305 train_loss = 1.244\n",
      "2020-07-06T15:18:13.069936 Batch  135/305 train_loss = 0.377\n",
      "2020-07-06T15:18:13.095879 Batch  155/305 train_loss = 0.764\n",
      "2020-07-06T15:18:13.122358 Batch  175/305 train_loss = 0.354\n",
      "2020-07-06T15:18:13.157198 Batch  195/305 train_loss = 0.311\n",
      "2020-07-06T15:18:13.183428 Batch  215/305 train_loss = 0.311\n",
      "2020-07-06T15:18:13.209632 Batch  235/305 train_loss = 0.783\n",
      "2020-07-06T15:18:13.235511 Batch  255/305 train_loss = 0.495\n",
      "2020-07-06T15:18:13.261102 Batch  275/305 train_loss = 0.543\n",
      "2020-07-06T15:18:13.286642 Batch  295/305 train_loss = 2.927\n",
      "2020-07-06T15:18:15.054953 Batch    4/76 accuracy = 0.84\n",
      "2020-07-06T15:18:22.124389 Batch   24/76 accuracy = 0.84\n",
      "2020-07-06T15:18:29.322739 Batch   44/76 accuracy = 0.84\n",
      "2020-07-06T15:18:36.565660 Batch   64/76 accuracy = 0.79\n",
      "2020-07-06T15:18:40.593004 Batch   10/305 train_loss = 0.524\n",
      "2020-07-06T15:18:40.619095 Batch   30/305 train_loss = 0.255\n",
      "2020-07-06T15:18:40.644549 Batch   50/305 train_loss = 0.480\n",
      "2020-07-06T15:18:40.671294 Batch   70/305 train_loss = 0.467\n",
      "2020-07-06T15:18:40.696421 Batch   90/305 train_loss = 0.324\n",
      "2020-07-06T15:18:40.721683 Batch  110/305 train_loss = 0.398\n",
      "2020-07-06T15:18:40.747450 Batch  130/305 train_loss = 0.363\n",
      "2020-07-06T15:18:40.773056 Batch  150/305 train_loss = 0.477\n",
      "2020-07-06T15:18:40.799160 Batch  170/305 train_loss = 0.484\n",
      "2020-07-06T15:18:40.830255 Batch  190/305 train_loss = 0.328\n",
      "2020-07-06T15:18:40.857794 Batch  210/305 train_loss = 0.271\n",
      "2020-07-06T15:18:40.883673 Batch  230/305 train_loss = 0.330\n",
      "2020-07-06T15:18:40.910735 Batch  250/305 train_loss = 0.272\n",
      "2020-07-06T15:18:40.937010 Batch  270/305 train_loss = 0.335\n",
      "2020-07-06T15:18:40.963091 Batch  290/305 train_loss = 0.838\n",
      "2020-07-06T15:18:44.254345 Batch    8/76 accuracy = 0.81\n",
      "2020-07-06T15:18:51.557643 Batch   28/76 accuracy = 0.82\n",
      "2020-07-06T15:18:58.968810 Batch   48/76 accuracy = 0.86\n",
      "2020-07-06T15:19:06.365719 Batch   68/76 accuracy = 0.91\n",
      "2020-07-06T15:19:09.096518 Batch    5/305 train_loss = 0.419\n",
      "2020-07-06T15:19:09.122855 Batch   25/305 train_loss = 0.294\n",
      "2020-07-06T15:19:09.148301 Batch   45/305 train_loss = 0.549\n",
      "2020-07-06T15:19:09.174775 Batch   65/305 train_loss = 0.292\n",
      "2020-07-06T15:19:09.200979 Batch   85/305 train_loss = 0.422\n",
      "2020-07-06T15:19:09.228081 Batch  105/305 train_loss = 0.549\n",
      "2020-07-06T15:19:09.254356 Batch  125/305 train_loss = 0.357\n",
      "2020-07-06T15:19:09.288317 Batch  145/305 train_loss = 0.312\n",
      "2020-07-06T15:19:09.322077 Batch  165/305 train_loss = 0.386\n",
      "2020-07-06T15:19:09.350923 Batch  185/305 train_loss = 0.347\n",
      "2020-07-06T15:19:09.377712 Batch  205/305 train_loss = 0.352\n",
      "2020-07-06T15:19:09.403466 Batch  225/305 train_loss = 0.880\n",
      "2020-07-06T15:19:09.432072 Batch  245/305 train_loss = 0.579\n",
      "2020-07-06T15:19:09.462427 Batch  265/305 train_loss = 0.550\n",
      "2020-07-06T15:19:09.493088 Batch  285/305 train_loss = 0.452\n",
      "2020-07-06T15:19:14.530051 Batch   12/76 accuracy = 0.87\n",
      "2020-07-06T15:19:22.359317 Batch   32/76 accuracy = 0.86\n",
      "2020-07-06T15:19:30.031946 Batch   52/76 accuracy = 0.86\n",
      "2020-07-06T15:19:37.907490 Batch   72/76 accuracy = 0.83\n",
      "2020-07-06T15:19:39.129042 Batch    0/305 train_loss = 0.511\n",
      "2020-07-06T15:19:39.158919 Batch   20/305 train_loss = 0.395\n",
      "2020-07-06T15:19:39.188808 Batch   40/305 train_loss = 0.398\n",
      "2020-07-06T15:19:39.216492 Batch   60/305 train_loss = 1.553\n",
      "2020-07-06T15:19:39.242935 Batch   80/305 train_loss = 1.737\n",
      "2020-07-06T15:19:39.274417 Batch  100/305 train_loss = 0.422\n",
      "2020-07-06T15:19:39.301520 Batch  120/305 train_loss = 0.284\n",
      "2020-07-06T15:19:39.327444 Batch  140/305 train_loss = 0.297\n",
      "2020-07-06T15:19:39.358584 Batch  160/305 train_loss = 0.389\n",
      "2020-07-06T15:19:39.388885 Batch  180/305 train_loss = 0.335\n",
      "2020-07-06T15:19:39.420774 Batch  200/305 train_loss = 0.398\n",
      "2020-07-06T15:19:39.448314 Batch  220/305 train_loss = 0.325\n",
      "2020-07-06T15:19:39.474225 Batch  240/305 train_loss = 0.473\n",
      "2020-07-06T15:19:39.500574 Batch  260/305 train_loss = 0.424\n",
      "2020-07-06T15:19:39.527652 Batch  280/305 train_loss = 0.431\n",
      "2020-07-06T15:19:39.553323 Batch  300/305 train_loss = 0.413\n",
      "2020-07-06T15:19:46.377673 Batch   16/76 accuracy = 0.83\n",
      "2020-07-06T15:19:54.281509 Batch   36/76 accuracy = 0.80\n",
      "2020-07-06T15:20:02.459817 Batch   56/76 accuracy = 0.84\n",
      "2020-07-06T15:20:10.332316 Batch   15/305 train_loss = 1.152\n",
      "2020-07-06T15:20:10.358558 Batch   35/305 train_loss = 0.413\n",
      "2020-07-06T15:20:10.385568 Batch   55/305 train_loss = 0.308\n",
      "2020-07-06T15:20:10.411078 Batch   75/305 train_loss = 0.361\n",
      "2020-07-06T15:20:10.436318 Batch   95/305 train_loss = 0.314\n",
      "2020-07-06T15:20:10.461752 Batch  115/305 train_loss = 0.870\n",
      "2020-07-06T15:20:10.487696 Batch  135/305 train_loss = 0.394\n",
      "2020-07-06T15:20:10.514337 Batch  155/305 train_loss = 0.499\n",
      "2020-07-06T15:20:10.541085 Batch  175/305 train_loss = 0.240\n",
      "2020-07-06T15:20:10.574847 Batch  195/305 train_loss = 0.317\n",
      "2020-07-06T15:20:10.601988 Batch  215/305 train_loss = 0.326\n",
      "2020-07-06T15:20:10.630184 Batch  235/305 train_loss = 0.460\n",
      "2020-07-06T15:20:10.660561 Batch  255/305 train_loss = 0.345\n",
      "2020-07-06T15:20:10.687026 Batch  275/305 train_loss = 0.566\n",
      "2020-07-06T15:20:10.713268 Batch  295/305 train_loss = 0.398\n",
      "2020-07-06T15:20:11.155491 Batch    0/76 accuracy = 0.84\n",
      "2020-07-06T15:20:19.384650 Batch   20/76 accuracy = 0.81\n",
      "2020-07-06T15:20:27.744874 Batch   40/76 accuracy = 0.88\n",
      "2020-07-06T15:20:36.122906 Batch   60/76 accuracy = 0.83\n",
      "2020-07-06T15:20:42.443278 Batch   10/305 train_loss = 0.391\n",
      "2020-07-06T15:20:42.470424 Batch   30/305 train_loss = 0.315\n",
      "2020-07-06T15:20:42.495459 Batch   50/305 train_loss = 0.422\n",
      "2020-07-06T15:20:42.520614 Batch   70/305 train_loss = 0.629\n",
      "2020-07-06T15:20:42.546385 Batch   90/305 train_loss = 0.279\n",
      "2020-07-06T15:20:42.571959 Batch  110/305 train_loss = 0.319\n",
      "2020-07-06T15:20:42.598446 Batch  130/305 train_loss = 0.338\n",
      "2020-07-06T15:20:42.624024 Batch  150/305 train_loss = 0.411\n",
      "2020-07-06T15:20:42.649598 Batch  170/305 train_loss = 0.372\n",
      "2020-07-06T15:20:42.679619 Batch  190/305 train_loss = 0.342\n",
      "2020-07-06T15:20:42.706797 Batch  210/305 train_loss = 0.275\n",
      "2020-07-06T15:20:42.732409 Batch  230/305 train_loss = 0.241\n",
      "2020-07-06T15:20:42.757760 Batch  250/305 train_loss = 0.258\n",
      "2020-07-06T15:20:42.783929 Batch  270/305 train_loss = 0.296\n",
      "2020-07-06T15:20:42.809593 Batch  290/305 train_loss = 0.368\n",
      "2020-07-06T15:20:44.978211 Batch    4/76 accuracy = 0.88\n",
      "2020-07-06T15:20:53.564266 Batch   24/76 accuracy = 0.84\n",
      "2020-07-06T15:21:01.997802 Batch   44/76 accuracy = 0.86\n",
      "2020-07-06T15:21:11.150773 Batch   64/76 accuracy = 0.81\n",
      "2020-07-06T15:21:16.301015 Batch    5/305 train_loss = 0.367\n",
      "2020-07-06T15:21:16.331587 Batch   25/305 train_loss = 0.305\n",
      "2020-07-06T15:21:16.360309 Batch   45/305 train_loss = 0.468\n",
      "2020-07-06T15:21:16.387919 Batch   65/305 train_loss = 0.249\n",
      "2020-07-06T15:21:16.414677 Batch   85/305 train_loss = 0.368\n",
      "2020-07-06T15:21:16.441602 Batch  105/305 train_loss = 0.254\n",
      "2020-07-06T15:21:16.468549 Batch  125/305 train_loss = 0.374\n",
      "2020-07-06T15:21:16.494997 Batch  145/305 train_loss = 0.302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-06T15:21:16.522819 Batch  165/305 train_loss = 0.473\n",
      "2020-07-06T15:21:16.549743 Batch  185/305 train_loss = 0.318\n",
      "2020-07-06T15:21:16.577327 Batch  205/305 train_loss = 0.325\n",
      "2020-07-06T15:21:16.604657 Batch  225/305 train_loss = 0.646\n",
      "2020-07-06T15:21:16.632688 Batch  245/305 train_loss = 0.410\n",
      "2020-07-06T15:21:16.664269 Batch  265/305 train_loss = 0.505\n",
      "2020-07-06T15:21:16.695715 Batch  285/305 train_loss = 0.392\n",
      "2020-07-06T15:21:20.797417 Batch    8/76 accuracy = 0.84\n",
      "2020-07-06T15:21:30.223374 Batch   28/76 accuracy = 0.84\n",
      "2020-07-06T15:21:39.351429 Batch   48/76 accuracy = 0.86\n",
      "2020-07-06T15:21:48.784305 Batch   68/76 accuracy = 0.91\n",
      "2020-07-06T15:21:52.093030 Batch    0/305 train_loss = 0.379\n",
      "2020-07-06T15:21:52.121549 Batch   20/305 train_loss = 0.442\n",
      "2020-07-06T15:21:52.148225 Batch   40/305 train_loss = 0.332\n",
      "2020-07-06T15:21:52.175556 Batch   60/305 train_loss = 0.399\n",
      "2020-07-06T15:21:52.201708 Batch   80/305 train_loss = 0.369\n",
      "2020-07-06T15:21:52.229012 Batch  100/305 train_loss = 0.279\n",
      "2020-07-06T15:21:52.255373 Batch  120/305 train_loss = 0.282\n",
      "2020-07-06T15:21:52.282322 Batch  140/305 train_loss = 0.312\n",
      "2020-07-06T15:21:52.309798 Batch  160/305 train_loss = 0.258\n",
      "2020-07-06T15:21:52.336439 Batch  180/305 train_loss = 0.283\n",
      "2020-07-06T15:21:52.362972 Batch  200/305 train_loss = 0.404\n",
      "2020-07-06T15:21:52.389701 Batch  220/305 train_loss = 0.296\n",
      "2020-07-06T15:21:52.416426 Batch  240/305 train_loss = 0.369\n",
      "2020-07-06T15:21:52.442921 Batch  260/305 train_loss = 0.388\n",
      "2020-07-06T15:21:52.470173 Batch  280/305 train_loss = 0.313\n",
      "2020-07-06T15:21:52.496301 Batch  300/305 train_loss = 0.379\n",
      "2020-07-06T15:21:58.601798 Batch   12/76 accuracy = 0.87\n",
      "2020-07-06T15:22:08.203905 Batch   32/76 accuracy = 0.85\n",
      "2020-07-06T15:22:17.624053 Batch   52/76 accuracy = 0.83\n",
      "2020-07-06T15:22:27.142183 Batch   72/76 accuracy = 0.85\n",
      "2020-07-06T15:22:28.612671 Batch   15/305 train_loss = 0.902\n",
      "2020-07-06T15:22:28.638760 Batch   35/305 train_loss = 0.410\n",
      "2020-07-06T15:22:28.665486 Batch   55/305 train_loss = 0.316\n",
      "2020-07-06T15:22:28.691931 Batch   75/305 train_loss = 0.340\n",
      "2020-07-06T15:22:28.718219 Batch   95/305 train_loss = 0.297\n",
      "2020-07-06T15:22:28.744446 Batch  115/305 train_loss = 0.687\n",
      "2020-07-06T15:22:28.770758 Batch  135/305 train_loss = 0.374\n",
      "2020-07-06T15:22:28.797385 Batch  155/305 train_loss = 0.471\n",
      "2020-07-06T15:22:28.823389 Batch  175/305 train_loss = 0.220\n",
      "2020-07-06T15:22:28.849589 Batch  195/305 train_loss = 0.300\n",
      "2020-07-06T15:22:28.875652 Batch  215/305 train_loss = 0.314\n",
      "2020-07-06T15:22:28.902353 Batch  235/305 train_loss = 0.419\n",
      "2020-07-06T15:22:28.928616 Batch  255/305 train_loss = 0.619\n",
      "2020-07-06T15:22:28.955005 Batch  275/305 train_loss = 0.335\n",
      "2020-07-06T15:22:28.981578 Batch  295/305 train_loss = 0.355\n",
      "2020-07-06T15:22:37.144125 Batch   16/76 accuracy = 0.83\n",
      "2020-07-06T15:22:46.823877 Batch   36/76 accuracy = 0.80\n",
      "2020-07-06T15:22:56.525545 Batch   56/76 accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# losses = {'train': [], 'test': []}\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(10):\n",
    "        ###将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X, test_X = train_test_split(data_dict['data'], test_size=0.2, random_state=0)\n",
    "#         print(train_X[CATEGORICAL_COLUMNS])\n",
    "        train_X_cate = train_X[CATEGORICAL_COLUMNS].to_numpy()\n",
    "        test_X_cate = test_X[CATEGORICAL_COLUMNS].to_numpy()\n",
    "        train_X_num = train_X[NUMBER_COLUMNS].to_numpy()\n",
    "        test_X_num = test_X[NUMBER_COLUMNS].to_numpy()\n",
    "        train_y = train_X['label'].to_numpy()\n",
    "        test_y = test_X['label'].to_numpy()\n",
    "#         train_cate_ploy = train_X[['poly_featrue_{}'.format(i) for i in range(37)]].to_numpy()\n",
    "#         test_cate_ploy = test_X[['poly_featrue_{}'.format(i) for i in range(37)]].to_numpy()\n",
    "        \n",
    "        \n",
    "        train_batches = get_batch(train_X_cate, train_X_num, batch_size)\n",
    "        test_batches = get_batch(test_X_cate, test_X_num, batch_size)\n",
    "        train_label_batches = get_batch(train_X_cate, train_y, batch_size)\n",
    "#         train_cate_batches = get_batch(train_cate_ploy, train_y, batch_size)\n",
    "        test_label_batches = get_batch(test_X_cate, test_y, batch_size)\n",
    "#         test_cate_batches = get_batch(test_cate_ploy, test_y, batch_size)\n",
    "   \n",
    "        for batch_i in range(len(train_X_cate) // batch_size):\n",
    "            train_cate, train_num = next(train_batches)\n",
    "#             test_cate, test_num = next(test_batches)\n",
    "            _, train_label = next(train_label_batches)\n",
    "#             train_poly, tets_poly = next(train_cate_batches)\n",
    "#             for cate in CATEGORICAL_COLUMNS:\n",
    "            train_label = train_label.reshape(-1, 1)\n",
    "#             print(train_label.shape)\n",
    "            feed_dict = {\n",
    "                numerical_inputs: train_num.reshape(-1, 5),\n",
    "                label: train_label,\n",
    "#                 cross_featrue_inputs: train_poly,\n",
    "            }\n",
    "            cate_feed = {\n",
    "                embed_dict[name]['holder']: np.reshape(train_cate[:, idx],[batch_size, 1]) for idx, name in enumerate(CATEGORICAL_COLUMNS)\n",
    "            }\n",
    "            feed_dict.update(cate_feed)\n",
    "            step, train_loss, _  = sess.run([global_step, loss, train_op], feed_dict=feed_dict)\n",
    "#             print(crorent_prediction__)\n",
    "            if (epoch_i * (len(train_X_cate) // batch_size) + batch_i) % 20 == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{:>3} Batch {:>4}/{} train_loss = {:.3f}'.format(time_str, batch_i, (len(train_X_cate) // batch_size), train_loss))\n",
    "        for batch_i in range(len(test_X_cate) // batch_size):\n",
    "            test_cate, test_num = next(test_batches)\n",
    "#             test_cate, test_num = next(test_batches)\n",
    "            _, test_label = next(test_label_batches)\n",
    "#             print(test_label)\n",
    "#             test_poly, _ = next(test_cate_batches)\n",
    "#             for cate in CATEGORICAL_COLUMNS:\n",
    "            test_label = test_label.reshape(-1, 1)\n",
    "            feed_dict = {\n",
    "                numerical_inputs: np.array(test_num).reshape(-1, 5),\n",
    "                label: test_label,\n",
    "#                 cross_featrue_inputs: test_poly,\n",
    "            }\n",
    "            feed_dict.update({\n",
    "                embed_dict[name]['holder']: np.reshape(test_cate[:, idx],[batch_size, 1]) for idx, name in enumerate(CATEGORICAL_COLUMNS)\n",
    "            })\n",
    "#             step, test_loss, _ = sess.run([global_step, loss, train_op], feed_dict=feed_dict)\n",
    "            y__ = sess.run(y_acc, feed_dict=feed_dict)\n",
    "#             print('pred y:\\n', [1 if i > 0.5 else 0 for i in y__])\n",
    "#             print('vali label:\\n', test_label.flatten())\n",
    "            pred_y = [1 if i > 0.5 else 0 for i in y__]\n",
    "            crorent_prediction = tf.equal(test_label.flatten(), pred_y)\n",
    "\n",
    "            accuracy__ = tf.reduce_mean(tf.cast(crorent_prediction, tf.float32))\n",
    "            test_acc = sess.run(accuracy__)\n",
    "#             print(sess.run(accuracy__))\n",
    "#             break\n",
    "            if (epoch_i * (len(test_X_cate) // batch_size) + batch_i) % 20 == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{:>3} Batch {:>4}/{} accuracy = {:.2f}'.format(time_str, batch_i, (len(test_X_cate) // batch_size), test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tf.Tensor 'workclass_inputs:0' shape=(?, 1) dtype=int32>: array([6, 5, 3, ..., 3, 3, 4]),\n",
       " <tf.Tensor 'education_inputs:0' shape=(?, 1) dtype=int32>: array([ 9,  9, 11, ..., 11, 11, 11]),\n",
       " <tf.Tensor 'marital_status_inputs:0' shape=(?, 1) dtype=int32>: array([4, 2, 0, ..., 6, 4, 2]),\n",
       " <tf.Tensor 'occupation_inputs:0' shape=(?, 1) dtype=int32>: array([0, 3, 5, ..., 0, 0, 3]),\n",
       " <tf.Tensor 'relationship_inputs:0' shape=(?, 1) dtype=int32>: array([1, 0, 1, ..., 4, 3, 5]),\n",
       " <tf.Tensor 'race_inputs:0' shape=(?, 1) dtype=int32>: array([4, 4, 4, ..., 4, 4, 4]),\n",
       " <tf.Tensor 'gender_inputs:0' shape=(?, 1) dtype=int32>: array([1, 1, 1, ..., 0, 1, 0]),\n",
       " <tf.Tensor 'native_country_inputs:0' shape=(?, 1) dtype=int32>: array([39, 39, 39, ..., 39, 39, 39])}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{embed_dict[name]['holder']: data_dict['train_X_cate'][:, idx] for idx, name in enumerate(CATEGORICAL_COLUMNS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, pd.DataFrame(np.vstack([data_dict['test_cate_poly'], data_dict['train_cate_poly']]), columns=['poly_featrue_{}'.format(i) for i in range(37)] )] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>...</th>\n",
       "      <th>poly_featrue_27</th>\n",
       "      <th>poly_featrue_28</th>\n",
       "      <th>poly_featrue_29</th>\n",
       "      <th>poly_featrue_30</th>\n",
       "      <th>poly_featrue_31</th>\n",
       "      <th>poly_featrue_32</th>\n",
       "      <th>poly_featrue_33</th>\n",
       "      <th>poly_featrue_34</th>\n",
       "      <th>poly_featrue_35</th>\n",
       "      <th>poly_featrue_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5943</th>\n",
       "      <td>34</td>\n",
       "      <td>3.0</td>\n",
       "      <td>204461.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17182</th>\n",
       "      <td>57</td>\n",
       "      <td>3.0</td>\n",
       "      <td>206343.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30104</th>\n",
       "      <td>55</td>\n",
       "      <td>3.0</td>\n",
       "      <td>359972.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20646</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17186</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>65</td>\n",
       "      <td>2.0</td>\n",
       "      <td>404601.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45892</th>\n",
       "      <td>61</td>\n",
       "      <td>2.0</td>\n",
       "      <td>244856.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42614</th>\n",
       "      <td>41</td>\n",
       "      <td>5.0</td>\n",
       "      <td>344624.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43568</th>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>104489.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19426</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78147 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  workclass    fnlwgt  education  education_num  marital_status  \\\n",
       "5943    34        3.0  204461.0        9.0           13.0             2.0   \n",
       "17182   57        3.0  206343.0       11.0            9.0             5.0   \n",
       "30104   55        3.0  359972.0        9.0           13.0             2.0   \n",
       "20646  NaN        NaN       NaN        NaN            NaN             NaN   \n",
       "17186  NaN        NaN       NaN        NaN            NaN             NaN   \n",
       "...    ...        ...       ...        ...            ...             ...   \n",
       "21243   65        2.0  404601.0       11.0            9.0             2.0   \n",
       "45892   61        2.0  244856.0        9.0           13.0             2.0   \n",
       "42614   41        5.0  344624.0       15.0           10.0             2.0   \n",
       "43568   47        5.0  104489.0        9.0           13.0             2.0   \n",
       "19426  NaN        NaN       NaN        NaN            NaN             NaN   \n",
       "\n",
       "       occupation  relationship  race  gender  ...  poly_featrue_27  \\\n",
       "5943         10.0           0.0   4.0     1.0  ...              NaN   \n",
       "17182         6.0           1.0   4.0     1.0  ...              NaN   \n",
       "30104         3.0           0.0   4.0     1.0  ...              NaN   \n",
       "20646         NaN           NaN   NaN     NaN  ...             21.0   \n",
       "17186         NaN           NaN   NaN     NaN  ...              3.0   \n",
       "...           ...           ...   ...     ...  ...              ...   \n",
       "21243         7.0           0.0   4.0     1.0  ...              NaN   \n",
       "45892         7.0           0.0   4.0     1.0  ...              NaN   \n",
       "42614        10.0           0.0   4.0     1.0  ...              NaN   \n",
       "43568        12.0           0.0   4.0     1.0  ...              NaN   \n",
       "19426         NaN           NaN   NaN     NaN  ...              0.0   \n",
       "\n",
       "       poly_featrue_28  poly_featrue_29  poly_featrue_30  poly_featrue_31  \\\n",
       "5943               NaN              NaN              NaN              NaN   \n",
       "17182              NaN              NaN              NaN              NaN   \n",
       "30104              NaN              NaN              NaN              NaN   \n",
       "20646             28.0              7.0            273.0             12.0   \n",
       "17186             12.0              0.0            117.0              4.0   \n",
       "...                ...              ...              ...              ...   \n",
       "21243              NaN              NaN              NaN              NaN   \n",
       "45892              NaN              NaN              NaN              NaN   \n",
       "42614              NaN              NaN              NaN              NaN   \n",
       "43568              NaN              NaN              NaN              NaN   \n",
       "19426              0.0              0.0              0.0              4.0   \n",
       "\n",
       "       poly_featrue_32  poly_featrue_33  poly_featrue_34  poly_featrue_35  \\\n",
       "5943               NaN              NaN              NaN              NaN   \n",
       "17182              NaN              NaN              NaN              NaN   \n",
       "30104              NaN              NaN              NaN              NaN   \n",
       "20646              3.0            117.0              4.0            156.0   \n",
       "17186              0.0             39.0              0.0            156.0   \n",
       "...                ...              ...              ...              ...   \n",
       "21243              NaN              NaN              NaN              NaN   \n",
       "45892              NaN              NaN              NaN              NaN   \n",
       "42614              NaN              NaN              NaN              NaN   \n",
       "43568              NaN              NaN              NaN              NaN   \n",
       "19426              0.0             39.0              0.0            156.0   \n",
       "\n",
       "       poly_featrue_36  \n",
       "5943               NaN  \n",
       "17182              NaN  \n",
       "30104              NaN  \n",
       "20646             39.0  \n",
       "17186              0.0  \n",
       "...                ...  \n",
       "21243              NaN  \n",
       "45892              NaN  \n",
       "42614              NaN  \n",
       "43568              NaN  \n",
       "19426              0.0  \n",
       "\n",
       "[78147 rows x 53 columns]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
