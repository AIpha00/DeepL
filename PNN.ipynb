{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##重新使用ml-1m\n",
    "##导入数据，并对数据进行简单处理\n",
    "def load_data():\n",
    "    ##读取user\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./data/ml-1m/users.dat', sep='::', header=None, names=users_title, engine='python')\n",
    "    users = users.drop(['Zip-code'], axis=1)\n",
    "    users_orig = users.values\n",
    "    \n",
    "    ##one_hot\n",
    "    gender_map = {'F': 0, 'M': 1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "    \n",
    "    ##年龄离散化\n",
    "    bins = [i for i in range(0, 61, 10)]\n",
    "    users['Age'] = pd.cut(users['Age'], bins=bins, labels=False)\n",
    "    \n",
    "    ##读取movie数据集\n",
    "    movies_title = [\"MovieID\", 'Title', 'Genres']\n",
    "    movies = pd.read_table('./data/ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine='python')\n",
    "    movies_orig = movies.values\n",
    "    \n",
    "    movies['Title'] = movies['Title'].str.extract(r'(.*)\\s*\\(\\d+\\)', expand=False)\n",
    "    #movies['year'] = movies['Title'].str.extract(r'(\\(\\d+\\))', expand=False)\n",
    "    \n",
    "    #电影类型转数字字典\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "\n",
    "    genres_set.add('<PAD>')\n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "    #将电影类型转成等长数字列表，长度是18\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "    \n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "    \n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "    \n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "    \n",
    "    ##将电影title专程等长数字列表，长度为15\n",
    "    title_count = 15\n",
    "    title_map = {val:[title2int[row] for row in val.split()] for ii, val in enumerate(set(movies['Title']))}\n",
    "    \n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(len(title_map[key]) + cnt, title2int['<PAD>'])\n",
    "    \n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "    \n",
    "    #读取评分数据集\n",
    "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./data/ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "\n",
    "    #合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    \n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    '''\n",
    "    title_count: title字段长度\n",
    "    title_set:title文本的集合\n",
    "        genres2int:电影类型转数字的字典\n",
    "        features：输入X\n",
    "        targets_values：是学习目标\n",
    "        ratings：评分数据集的pandas对象\n",
    "        users：用户数据集的pandas对象\n",
    "        movies：电影数据的pandas对象\n",
    "        data：三个数据集合并表\n",
    "        movies_orig：原始数据\n",
    "        users_orig：原属数据(用户)\n",
    "    '''\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_init():\n",
    "    \"\"\"\n",
    "    user:id,gender,age,job\n",
    "    movie:id, genres, title\n",
    "    rating: 目标输出\n",
    "    \"\"\"\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name='uid')\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name='user_gender')\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name='user_age')   ###这里是经过分桶过后的离散数据，数值型数据可以不同embedding层\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name='user_job')\n",
    "    \n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name='movie_id')\n",
    "    movie_genres = tf.placeholder(tf.int32, [None, 18], name='movie_genres')\n",
    "    movie_title = tf.placeholder(tf.int32, [None, 15], name='movie_title')\n",
    "    \n",
    "    rating = tf.placeholder(tf.int32, [None,1], name='rating')\n",
    "    \n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    return uid, user_age, user_gender, user_job, movie_id, movie_genres, movie_title, rating, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "###各个维度定义\n",
    "embed_dim = 32\n",
    "uid_dim = data['UserID'].unique().max() + 1\n",
    "user_gender_dim = data['Gender'].unique().max() + 1\n",
    "user_age_dim = data['Age'].unique().max() + 1\n",
    "user_job_dim = data['JobID'].unique().max() + 1\n",
    "\n",
    "movie_id_dim = data['MovieID'].unique().max() + 1\n",
    "movie_title_dim = len(title_set)\n",
    "movie_genres_dim = max(genres2int.values()) + 1\n",
    "\n",
    "sentences_size = 15\n",
    "\n",
    "#文本卷积滑动窗口\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "movieid2idx = {val[0]: i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##超参数\n",
    "num_epochs = 5\n",
    "batch_size =128\n",
    "\n",
    "dropout_keep = 0.5\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "##显示每n个批次的统计信息\n",
    "show_every_n_batches = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "###开始网络搭建\n",
    "\"\"\"\n",
    "第一部分:各个特征的embedding层\n",
    "\"\"\"\n",
    "def user_feature(**kwargs):\n",
    "    with tf.name_scope('user_embedding'):\n",
    "        uid = kwargs.get('uid', '')\n",
    "        uid_embed_matrix = tf.Variable(tf.random_normal([uid_dim, embed_dim], stddev=1.0, mean=0.0), name='uid_embed_matrix')\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name='uid_embed_layer')\n",
    "\n",
    "        user_gender = kwargs.get('user_gender', '')\n",
    "        gender_embed_matrix = tf.Variable(tf.random_normal([user_gender_dim, embed_dim], stddev=1.0, mean=0.0), name='gender_embed_matrix')\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name='gender_embed_layer')\n",
    "\n",
    "        user_age = kwargs.get('user_age', '')\n",
    "        age_embed_matrix = tf.Variable(tf.random_normal([user_age_dim, embed_dim], stddev=1.0, mean=0.0), name='age_embed_matrix')\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name='age_embed_layer')\n",
    "\n",
    "        user_job = kwargs.get('user_job', '')\n",
    "        job_embed_matrix = tf.Variable(tf.random_normal([user_job_dim, embed_dim], stddev=1.0, mean=0.0), name='job_embed_matrix')\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name='job_embed_layer')\n",
    "        \n",
    "        return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer\n",
    "\n",
    "def movie_feature(**kwargs):\n",
    "    with tf.name_scope('movie_embedding'):\n",
    "        movie_id = kwargs.get('movie_id')\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_normal([movie_id_dim, embed_dim], stddev=1.0, mean=0.0), name='movie_id_embed_matrix')\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name='movie_id_embed_layer')\n",
    "        \n",
    "        movie_genres = kwargs.get('movie_genres')\n",
    "        movie_genres_embed_matrix = tf.Variable(tf.random_normal([movie_genres_dim, embed_dim], stddev=1.0, mean=0.0), name='movie_genres_embed_matrix')\n",
    "        movie_genres_embed_layer = tf.nn.embedding_lookup(movie_genres_embed_matrix, movie_genres, name='movie_genres_embed_layer')\n",
    "        movie_genres_embed_layer = tf.reduce_sum(movie_genres_embed_layer, axis=1, keep_dims=True)\n",
    "        \n",
    "        return movie_id_embed_layer, movie_genres_embed_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "##对于电影名称这类的文本数据，这里采用文本卷积提取特征\n",
    "def get_movie_cnn_layer(movie_title):\n",
    "    ##电影名称的embedding\n",
    "    with tf.name_scope('title_embedding'):\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_normal([movie_title_dim, embed_dim], stddev=1.0), name='movie_title_embed_matrix')\n",
    "        ##shape(128, 32)\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_title, name='movie_title_embed_layer')\n",
    "        ##暂时不加这一个维度，不懂为什么要加这一个维度,   文本特征，缺少了像图像数据的channel(通道)维度\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    \n",
    "    pool_layer_lst = []\n",
    "    for windows in window_sizes:\n",
    "        with tf.name_scope('movie_txt_conv_maxpool_{}'.format(windows)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([windows, embed_dim, 1, filter_num], stddev=0.1), name='filter_weights')\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name='filter_bias')\n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding='VALID', name='conv_layer')\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer, filter_bias), name='relu_layer')\n",
    "\n",
    "            ##池化层，针对不同的窗口尺寸的卷积核进行计算\n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1, sentences_size - windows + 1, 1, 1], [1, 1, 1, 1], padding='VALID', name='maxpool_layer')\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "    pool_layer = tf.concat(pool_layer_lst, 3, name='pool_layer')\n",
    "    max_num = len(window_sizes) * filter_num\n",
    "    pool_layer_flat = tf.reshape(pool_layer, [-1, 1, max_num], name='pool_layer_flat')\n",
    "    \n",
    "    return pool_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_combine(movie_id_embed_layer, movie_genres_embed_layer, pool_layer_flat):\n",
    "    with tf.name_scope('movie_fc'):\n",
    "        ##第一层\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name='movie_id_fc_layer', activation=tf.nn.relu)\n",
    "        movie_genres_fc_layer = tf.layers.dense(movie_genres_embed_layer, embed_dim, name='movie_genres_fc_layer', activation=tf.nn.relu)\n",
    "        \n",
    "        ##第二层---特征组合？\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_genres_fc_layer, pool_layer_flat], axis=-1) ###axis在这里表示按照哪个维度进行拼接\n",
    "        movie_combine_layer = tf.layers.dense(movie_combine_layer, 200, activation=tf.nn.relu)\n",
    "        \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])  ##相当于转置\n",
    "    \n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "##将user的embedding向量一起全连接生成user的特征\n",
    "def user_combine(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope('user_fc'):\n",
    "        ##第一层\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name='uid_fc_layer', activation=tf.nn.relu)\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name='gender_fc_layer', activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name='age_fc_layer', activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name='job_fc_layer', activation=tf.nn.relu)\n",
    "        \n",
    "        ##第二层\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)\n",
    "        user_combine_layer = tf.layers.dense(user_combine_layer, 200, activation=tf.tanh, name='user_combine_layer')\n",
    "        \n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "###构建计算图\n",
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入\n",
    "    uid, user_age, user_gender, user_job, movie_id, movie_genres, movie_title, target, learning_rate = variable_init()\n",
    "    \n",
    "    #获取user特征的embedding向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = user_feature(uid=uid, user_age=user_age, user_gender=user_gender, user_job= user_job)\n",
    "    \n",
    "    movie_id_embed_layer, movie_genres_embed_layer = movie_feature(movie_id=movie_id, movie_genres=movie_genres)\n",
    "    \n",
    "    ##获取movie title的特征，embedding向量\n",
    "    pool_layer_flat = get_movie_cnn_layer(movie_title)\n",
    "    \n",
    "    ##获取用户特征---拼接层\n",
    "    user_combine_layer, user_combine_layer_flat = user_combine(uid_embed_layer=uid_embed_layer, gender_embed_layer=gender_embed_layer, age_embed_layer=age_embed_layer, job_embed_layer=job_embed_layer)\n",
    "    \n",
    "    ##获取电影特征拼接层\n",
    "    movie_combine_layer, movie_combine_layer_flat = movie_combine(movie_id_embed_layer=movie_id_embed_layer, movie_genres_embed_layer=movie_genres_embed_layer, pool_layer_flat=pool_layer_flat)\n",
    "    \n",
    "    with tf.name_scope('inference'):\n",
    "        ##将用户特征和电影特征作为输入， 全连接层，输出一个值\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        ##与rating的维度不对应\n",
    "        inference = tf.expand_dims(inference, axis=-1)\n",
    "    \n",
    "    cost = tf.losses.mean_squared_error(target, inference)\n",
    "    loss = tf.reduce_mean(cost)\n",
    "    \n",
    "    ##优化器\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inference/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size):\n",
    "    for start in range(0, len(X), batch_size):\n",
    "        end = min(start + batch_size, len(X))\n",
    "        yield X[start:end], y[start: end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:33:36.929154 Batch    0/6251 train_loss = 905.436\n",
      "2020-06-30T19:33:37.136268 Batch   20/6251 train_loss = 15.003\n",
      "2020-06-30T19:33:37.363791 Batch   40/6251 train_loss = 1.739\n",
      "2020-06-30T19:33:37.585963 Batch   60/6251 train_loss = 1.778\n",
      "2020-06-30T19:33:37.802719 Batch   80/6251 train_loss = 1.787\n",
      "2020-06-30T19:33:38.029167 Batch  100/6251 train_loss = 1.544\n",
      "2020-06-30T19:33:38.244463 Batch  120/6251 train_loss = 1.075\n",
      "2020-06-30T19:33:38.473562 Batch  140/6251 train_loss = 1.513\n",
      "2020-06-30T19:33:38.686011 Batch  160/6251 train_loss = 1.533\n",
      "2020-06-30T19:33:38.916806 Batch  180/6251 train_loss = 1.274\n",
      "2020-06-30T19:33:39.129098 Batch  200/6251 train_loss = 1.311\n",
      "2020-06-30T19:33:39.343748 Batch  220/6251 train_loss = 1.195\n",
      "2020-06-30T19:33:39.569535 Batch  240/6251 train_loss = 1.463\n",
      "2020-06-30T19:33:39.787758 Batch  260/6251 train_loss = 1.261\n",
      "2020-06-30T19:33:40.006312 Batch  280/6251 train_loss = 1.244\n",
      "2020-06-30T19:33:40.215934 Batch  300/6251 train_loss = 1.207\n",
      "2020-06-30T19:33:40.434575 Batch  320/6251 train_loss = 1.153\n",
      "2020-06-30T19:33:40.651634 Batch  340/6251 train_loss = 1.200\n",
      "2020-06-30T19:33:40.861051 Batch  360/6251 train_loss = 1.146\n",
      "2020-06-30T19:33:41.078836 Batch  380/6251 train_loss = 1.083\n",
      "2020-06-30T19:33:41.294381 Batch  400/6251 train_loss = 1.191\n",
      "2020-06-30T19:33:41.516611 Batch  420/6251 train_loss = 1.170\n",
      "2020-06-30T19:33:41.729061 Batch  440/6251 train_loss = 1.196\n",
      "2020-06-30T19:33:41.949420 Batch  460/6251 train_loss = 1.093\n",
      "2020-06-30T19:33:42.163680 Batch  480/6251 train_loss = 1.530\n",
      "2020-06-30T19:33:42.379009 Batch  500/6251 train_loss = 1.034\n",
      "2020-06-30T19:33:42.599360 Batch  520/6251 train_loss = 1.462\n",
      "2020-06-30T19:33:42.811649 Batch  540/6251 train_loss = 0.975\n",
      "2020-06-30T19:33:43.030344 Batch  560/6251 train_loss = 1.159\n",
      "2020-06-30T19:33:43.248011 Batch  580/6251 train_loss = 1.061\n",
      "2020-06-30T19:33:43.463541 Batch  600/6251 train_loss = 1.236\n",
      "2020-06-30T19:33:43.673329 Batch  620/6251 train_loss = 1.215\n",
      "2020-06-30T19:33:43.885669 Batch  640/6251 train_loss = 1.253\n",
      "2020-06-30T19:33:44.105544 Batch  660/6251 train_loss = 1.182\n",
      "2020-06-30T19:33:44.319720 Batch  680/6251 train_loss = 1.000\n",
      "2020-06-30T19:33:44.533983 Batch  700/6251 train_loss = 1.010\n",
      "2020-06-30T19:33:44.754291 Batch  720/6251 train_loss = 0.949\n",
      "2020-06-30T19:33:44.974207 Batch  740/6251 train_loss = 1.154\n",
      "2020-06-30T19:33:45.191982 Batch  760/6251 train_loss = 0.951\n",
      "2020-06-30T19:33:45.412655 Batch  780/6251 train_loss = 1.372\n",
      "2020-06-30T19:33:45.628879 Batch  800/6251 train_loss = 1.119\n",
      "2020-06-30T19:33:45.842361 Batch  820/6251 train_loss = 0.999\n",
      "2020-06-30T19:33:46.062323 Batch  840/6251 train_loss = 1.039\n",
      "2020-06-30T19:33:46.274144 Batch  860/6251 train_loss = 1.433\n",
      "2020-06-30T19:33:46.490169 Batch  880/6251 train_loss = 0.965\n",
      "2020-06-30T19:33:46.705834 Batch  900/6251 train_loss = 0.975\n",
      "2020-06-30T19:33:46.920290 Batch  920/6251 train_loss = 1.250\n",
      "2020-06-30T19:33:47.135991 Batch  940/6251 train_loss = 0.941\n",
      "2020-06-30T19:33:47.351742 Batch  960/6251 train_loss = 1.148\n",
      "2020-06-30T19:33:47.579219 Batch  980/6251 train_loss = 1.065\n",
      "2020-06-30T19:33:47.788441 Batch 1000/6251 train_loss = 0.947\n",
      "2020-06-30T19:33:48.013861 Batch 1020/6251 train_loss = 0.956\n",
      "2020-06-30T19:33:48.244359 Batch 1040/6251 train_loss = 1.430\n",
      "2020-06-30T19:33:48.472828 Batch 1060/6251 train_loss = 0.954\n",
      "2020-06-30T19:33:48.684425 Batch 1080/6251 train_loss = 0.983\n",
      "2020-06-30T19:33:48.897586 Batch 1100/6251 train_loss = 1.278\n",
      "2020-06-30T19:33:49.118499 Batch 1120/6251 train_loss = 1.074\n",
      "2020-06-30T19:33:49.376480 Batch 1140/6251 train_loss = 1.115\n",
      "2020-06-30T19:33:49.632627 Batch 1160/6251 train_loss = 1.092\n",
      "2020-06-30T19:33:49.878047 Batch 1180/6251 train_loss = 1.123\n",
      "2020-06-30T19:33:50.101963 Batch 1200/6251 train_loss = 1.170\n",
      "2020-06-30T19:33:50.353747 Batch 1220/6251 train_loss = 0.967\n",
      "2020-06-30T19:33:50.595733 Batch 1240/6251 train_loss = 0.962\n",
      "2020-06-30T19:33:50.811268 Batch 1260/6251 train_loss = 1.019\n",
      "2020-06-30T19:33:51.081566 Batch 1280/6251 train_loss = 0.896\n",
      "2020-06-30T19:33:51.317095 Batch 1300/6251 train_loss = 1.139\n",
      "2020-06-30T19:33:51.546522 Batch 1320/6251 train_loss = 1.152\n",
      "2020-06-30T19:33:51.788307 Batch 1340/6251 train_loss = 0.928\n",
      "2020-06-30T19:33:52.006868 Batch 1360/6251 train_loss = 1.094\n",
      "2020-06-30T19:33:52.229338 Batch 1380/6251 train_loss = 0.952\n",
      "2020-06-30T19:33:52.482877 Batch 1400/6251 train_loss = 1.032\n",
      "2020-06-30T19:33:52.699827 Batch 1420/6251 train_loss = 1.023\n",
      "2020-06-30T19:33:52.929223 Batch 1440/6251 train_loss = 0.807\n",
      "2020-06-30T19:33:53.187030 Batch 1460/6251 train_loss = 0.713\n",
      "2020-06-30T19:33:53.411961 Batch 1480/6251 train_loss = 1.219\n",
      "2020-06-30T19:33:53.628895 Batch 1500/6251 train_loss = 1.215\n",
      "2020-06-30T19:33:53.857693 Batch 1520/6251 train_loss = 1.065\n",
      "2020-06-30T19:33:54.068115 Batch 1540/6251 train_loss = 1.010\n",
      "2020-06-30T19:33:54.283632 Batch 1560/6251 train_loss = 1.226\n",
      "2020-06-30T19:33:54.499147 Batch 1580/6251 train_loss = 0.937\n",
      "2020-06-30T19:33:54.727575 Batch 1600/6251 train_loss = 0.975\n",
      "2020-06-30T19:33:54.977043 Batch 1620/6251 train_loss = 0.976\n",
      "2020-06-30T19:33:55.188478 Batch 1640/6251 train_loss = 0.959\n",
      "2020-06-30T19:33:55.420796 Batch 1660/6251 train_loss = 0.639\n",
      "2020-06-30T19:33:55.651352 Batch 1680/6251 train_loss = 1.027\n",
      "2020-06-30T19:33:55.873164 Batch 1700/6251 train_loss = 1.132\n",
      "2020-06-30T19:33:56.104327 Batch 1720/6251 train_loss = 0.854\n",
      "2020-06-30T19:33:56.350537 Batch 1740/6251 train_loss = 0.932\n",
      "2020-06-30T19:33:56.561718 Batch 1760/6251 train_loss = 0.983\n",
      "2020-06-30T19:33:56.775495 Batch 1780/6251 train_loss = 0.859\n",
      "2020-06-30T19:33:57.007375 Batch 1800/6251 train_loss = 0.986\n",
      "2020-06-30T19:33:57.220458 Batch 1820/6251 train_loss = 1.379\n",
      "2020-06-30T19:33:57.438519 Batch 1840/6251 train_loss = 1.174\n",
      "2020-06-30T19:33:57.655056 Batch 1860/6251 train_loss = 1.017\n",
      "2020-06-30T19:33:57.865822 Batch 1880/6251 train_loss = 1.006\n",
      "2020-06-30T19:33:58.124940 Batch 1900/6251 train_loss = 0.931\n",
      "2020-06-30T19:33:58.365009 Batch 1920/6251 train_loss = 1.117\n",
      "2020-06-30T19:33:58.581782 Batch 1940/6251 train_loss = 1.153\n",
      "2020-06-30T19:33:58.809469 Batch 1960/6251 train_loss = 1.061\n",
      "2020-06-30T19:33:59.035721 Batch 1980/6251 train_loss = 0.748\n",
      "2020-06-30T19:33:59.303493 Batch 2000/6251 train_loss = 1.050\n",
      "2020-06-30T19:33:59.552840 Batch 2020/6251 train_loss = 1.272\n",
      "2020-06-30T19:33:59.811724 Batch 2040/6251 train_loss = 1.016\n",
      "2020-06-30T19:34:00.030249 Batch 2060/6251 train_loss = 1.502\n",
      "2020-06-30T19:34:00.241963 Batch 2080/6251 train_loss = 0.871\n",
      "2020-06-30T19:34:00.465536 Batch 2100/6251 train_loss = 0.970\n",
      "2020-06-30T19:34:00.689693 Batch 2120/6251 train_loss = 1.111\n",
      "2020-06-30T19:34:00.900557 Batch 2140/6251 train_loss = 1.000\n",
      "2020-06-30T19:34:01.111887 Batch 2160/6251 train_loss = 0.988\n",
      "2020-06-30T19:34:01.331737 Batch 2180/6251 train_loss = 0.984\n",
      "2020-06-30T19:34:01.541592 Batch 2200/6251 train_loss = 1.092\n",
      "2020-06-30T19:34:01.752299 Batch 2220/6251 train_loss = 0.968\n",
      "2020-06-30T19:34:01.994080 Batch 2240/6251 train_loss = 0.809\n",
      "2020-06-30T19:34:02.224093 Batch 2260/6251 train_loss = 0.916\n",
      "2020-06-30T19:34:02.463853 Batch 2280/6251 train_loss = 1.031\n",
      "2020-06-30T19:34:02.689100 Batch 2300/6251 train_loss = 0.927\n",
      "2020-06-30T19:34:02.908173 Batch 2320/6251 train_loss = 0.704\n",
      "2020-06-30T19:34:03.160412 Batch 2340/6251 train_loss = 0.890\n",
      "2020-06-30T19:34:03.373530 Batch 2360/6251 train_loss = 0.868\n",
      "2020-06-30T19:34:03.619122 Batch 2380/6251 train_loss = 1.117\n",
      "2020-06-30T19:34:03.838918 Batch 2400/6251 train_loss = 1.178\n",
      "2020-06-30T19:34:04.045910 Batch 2420/6251 train_loss = 0.862\n",
      "2020-06-30T19:34:04.248549 Batch 2440/6251 train_loss = 0.929\n",
      "2020-06-30T19:34:04.455135 Batch 2460/6251 train_loss = 0.907\n",
      "2020-06-30T19:34:04.665388 Batch 2480/6251 train_loss = 0.802\n",
      "2020-06-30T19:34:04.868006 Batch 2500/6251 train_loss = 1.296\n",
      "2020-06-30T19:34:05.070931 Batch 2520/6251 train_loss = 1.190\n",
      "2020-06-30T19:34:05.269878 Batch 2540/6251 train_loss = 1.071\n",
      "2020-06-30T19:34:05.477945 Batch 2560/6251 train_loss = 1.157\n",
      "2020-06-30T19:34:05.675524 Batch 2580/6251 train_loss = 0.867\n",
      "2020-06-30T19:34:05.878784 Batch 2600/6251 train_loss = 0.797\n",
      "2020-06-30T19:34:06.076638 Batch 2620/6251 train_loss = 0.966\n",
      "2020-06-30T19:34:06.282010 Batch 2640/6251 train_loss = 1.080\n",
      "2020-06-30T19:34:06.474168 Batch 2660/6251 train_loss = 1.109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:34:06.673292 Batch 2680/6251 train_loss = 0.869\n",
      "2020-06-30T19:34:06.876965 Batch 2700/6251 train_loss = 0.954\n",
      "2020-06-30T19:34:07.097252 Batch 2720/6251 train_loss = 0.857\n",
      "2020-06-30T19:34:07.309306 Batch 2740/6251 train_loss = 1.028\n",
      "2020-06-30T19:34:07.520880 Batch 2760/6251 train_loss = 0.949\n",
      "2020-06-30T19:34:07.733196 Batch 2780/6251 train_loss = 0.840\n",
      "2020-06-30T19:34:07.948455 Batch 2800/6251 train_loss = 1.134\n",
      "2020-06-30T19:34:08.164824 Batch 2820/6251 train_loss = 0.911\n",
      "2020-06-30T19:34:08.376345 Batch 2840/6251 train_loss = 0.915\n",
      "2020-06-30T19:34:08.589678 Batch 2860/6251 train_loss = 1.205\n",
      "2020-06-30T19:34:08.804490 Batch 2880/6251 train_loss = 0.896\n",
      "2020-06-30T19:34:09.012155 Batch 2900/6251 train_loss = 0.937\n",
      "2020-06-30T19:34:09.230045 Batch 2920/6251 train_loss = 0.962\n",
      "2020-06-30T19:34:09.444323 Batch 2940/6251 train_loss = 0.916\n",
      "2020-06-30T19:34:09.658575 Batch 2960/6251 train_loss = 0.969\n",
      "2020-06-30T19:34:09.891143 Batch 2980/6251 train_loss = 1.019\n",
      "2020-06-30T19:34:10.123559 Batch 3000/6251 train_loss = 0.881\n",
      "2020-06-30T19:34:10.346937 Batch 3020/6251 train_loss = 1.054\n",
      "2020-06-30T19:34:10.604292 Batch 3040/6251 train_loss = 0.928\n",
      "2020-06-30T19:34:10.846815 Batch 3060/6251 train_loss = 1.176\n",
      "2020-06-30T19:34:11.101037 Batch 3080/6251 train_loss = 1.022\n",
      "2020-06-30T19:34:11.349138 Batch 3100/6251 train_loss = 0.900\n",
      "2020-06-30T19:34:11.584395 Batch 3120/6251 train_loss = 0.992\n",
      "2020-06-30T19:34:11.809875 Batch 3140/6251 train_loss = 0.796\n",
      "2020-06-30T19:34:12.034264 Batch 3160/6251 train_loss = 0.949\n",
      "2020-06-30T19:34:12.272644 Batch 3180/6251 train_loss = 1.174\n",
      "2020-06-30T19:34:12.486955 Batch 3200/6251 train_loss = 1.077\n",
      "2020-06-30T19:34:12.693814 Batch 3220/6251 train_loss = 0.805\n",
      "2020-06-30T19:34:12.902339 Batch 3240/6251 train_loss = 0.840\n",
      "2020-06-30T19:34:13.124659 Batch 3260/6251 train_loss = 1.150\n",
      "2020-06-30T19:34:13.339789 Batch 3280/6251 train_loss = 0.752\n",
      "2020-06-30T19:34:13.548628 Batch 3300/6251 train_loss = 0.859\n",
      "2020-06-30T19:34:13.774421 Batch 3320/6251 train_loss = 1.080\n",
      "2020-06-30T19:34:14.019010 Batch 3340/6251 train_loss = 0.830\n",
      "2020-06-30T19:34:14.232872 Batch 3360/6251 train_loss = 0.943\n",
      "2020-06-30T19:34:14.492161 Batch 3380/6251 train_loss = 1.015\n",
      "2020-06-30T19:34:14.719189 Batch 3400/6251 train_loss = 0.810\n",
      "2020-06-30T19:34:14.943424 Batch 3420/6251 train_loss = 1.038\n",
      "2020-06-30T19:34:15.168157 Batch 3440/6251 train_loss = 0.877\n",
      "2020-06-30T19:34:15.383000 Batch 3460/6251 train_loss = 1.022\n",
      "2020-06-30T19:34:15.596678 Batch 3480/6251 train_loss = 1.173\n",
      "2020-06-30T19:34:15.807307 Batch 3500/6251 train_loss = 0.790\n",
      "2020-06-30T19:34:16.020358 Batch 3520/6251 train_loss = 0.906\n",
      "2020-06-30T19:34:16.233072 Batch 3540/6251 train_loss = 1.218\n",
      "2020-06-30T19:34:16.436060 Batch 3560/6251 train_loss = 0.987\n",
      "2020-06-30T19:34:16.640886 Batch 3580/6251 train_loss = 0.997\n",
      "2020-06-30T19:34:16.842049 Batch 3600/6251 train_loss = 0.863\n",
      "2020-06-30T19:34:17.046177 Batch 3620/6251 train_loss = 1.257\n",
      "2020-06-30T19:34:17.244512 Batch 3640/6251 train_loss = 0.870\n",
      "2020-06-30T19:34:17.449088 Batch 3660/6251 train_loss = 1.065\n",
      "2020-06-30T19:34:17.682673 Batch 3680/6251 train_loss = 1.020\n",
      "2020-06-30T19:34:17.916333 Batch 3700/6251 train_loss = 0.811\n",
      "2020-06-30T19:34:18.160980 Batch 3720/6251 train_loss = 1.048\n",
      "2020-06-30T19:34:18.367978 Batch 3740/6251 train_loss = 0.928\n",
      "2020-06-30T19:34:18.608228 Batch 3760/6251 train_loss = 0.961\n",
      "2020-06-30T19:34:18.846919 Batch 3780/6251 train_loss = 1.042\n",
      "2020-06-30T19:34:19.087229 Batch 3800/6251 train_loss = 0.957\n",
      "2020-06-30T19:34:19.314217 Batch 3820/6251 train_loss = 0.770\n",
      "2020-06-30T19:34:19.542814 Batch 3840/6251 train_loss = 0.877\n",
      "2020-06-30T19:34:19.789496 Batch 3860/6251 train_loss = 1.051\n",
      "2020-06-30T19:34:20.005247 Batch 3880/6251 train_loss = 0.697\n",
      "2020-06-30T19:34:20.244849 Batch 3900/6251 train_loss = 0.955\n",
      "2020-06-30T19:34:20.472350 Batch 3920/6251 train_loss = 0.833\n",
      "2020-06-30T19:34:20.690171 Batch 3940/6251 train_loss = 0.990\n",
      "2020-06-30T19:34:20.936916 Batch 3960/6251 train_loss = 0.883\n",
      "2020-06-30T19:34:21.149204 Batch 3980/6251 train_loss = 0.839\n",
      "2020-06-30T19:34:21.361408 Batch 4000/6251 train_loss = 1.069\n",
      "2020-06-30T19:34:21.609432 Batch 4020/6251 train_loss = 0.853\n",
      "2020-06-30T19:34:21.818771 Batch 4040/6251 train_loss = 1.048\n",
      "2020-06-30T19:34:22.032450 Batch 4060/6251 train_loss = 0.932\n",
      "2020-06-30T19:34:22.249921 Batch 4080/6251 train_loss = 0.881\n",
      "2020-06-30T19:34:22.459271 Batch 4100/6251 train_loss = 0.949\n",
      "2020-06-30T19:34:22.665088 Batch 4120/6251 train_loss = 1.048\n",
      "2020-06-30T19:34:22.897040 Batch 4140/6251 train_loss = 1.051\n",
      "2020-06-30T19:34:23.116751 Batch 4160/6251 train_loss = 1.147\n",
      "2020-06-30T19:34:23.321706 Batch 4180/6251 train_loss = 0.823\n",
      "2020-06-30T19:34:23.538450 Batch 4200/6251 train_loss = 0.693\n",
      "2020-06-30T19:34:23.750126 Batch 4220/6251 train_loss = 0.894\n",
      "2020-06-30T19:34:23.988712 Batch 4240/6251 train_loss = 0.898\n",
      "2020-06-30T19:34:24.190020 Batch 4260/6251 train_loss = 1.089\n",
      "2020-06-30T19:34:24.402755 Batch 4280/6251 train_loss = 0.766\n",
      "2020-06-30T19:34:24.623243 Batch 4300/6251 train_loss = 0.897\n",
      "2020-06-30T19:34:24.837765 Batch 4320/6251 train_loss = 0.917\n",
      "2020-06-30T19:34:25.041091 Batch 4340/6251 train_loss = 0.845\n",
      "2020-06-30T19:34:25.246592 Batch 4360/6251 train_loss = 1.128\n",
      "2020-06-30T19:34:25.448573 Batch 4380/6251 train_loss = 0.958\n",
      "2020-06-30T19:34:25.664494 Batch 4400/6251 train_loss = 0.976\n",
      "2020-06-30T19:34:25.870183 Batch 4420/6251 train_loss = 0.949\n",
      "2020-06-30T19:34:26.084768 Batch 4440/6251 train_loss = 0.888\n",
      "2020-06-30T19:34:26.292765 Batch 4460/6251 train_loss = 0.695\n",
      "2020-06-30T19:34:26.500304 Batch 4480/6251 train_loss = 0.908\n",
      "2020-06-30T19:34:26.711734 Batch 4500/6251 train_loss = 1.071\n",
      "2020-06-30T19:34:26.920298 Batch 4520/6251 train_loss = 0.768\n",
      "2020-06-30T19:34:27.131062 Batch 4540/6251 train_loss = 0.994\n",
      "2020-06-30T19:34:27.337538 Batch 4560/6251 train_loss = 0.896\n",
      "2020-06-30T19:34:27.542474 Batch 4580/6251 train_loss = 0.838\n",
      "2020-06-30T19:34:27.749589 Batch 4600/6251 train_loss = 0.796\n",
      "2020-06-30T19:34:27.953225 Batch 4620/6251 train_loss = 0.967\n",
      "2020-06-30T19:34:28.156540 Batch 4640/6251 train_loss = 0.944\n",
      "2020-06-30T19:34:28.360871 Batch 4660/6251 train_loss = 1.016\n",
      "2020-06-30T19:34:28.562894 Batch 4680/6251 train_loss = 0.947\n",
      "2020-06-30T19:34:28.761731 Batch 4700/6251 train_loss = 1.001\n",
      "2020-06-30T19:34:28.959672 Batch 4720/6251 train_loss = 1.010\n",
      "2020-06-30T19:34:29.167979 Batch 4740/6251 train_loss = 1.015\n",
      "2020-06-30T19:34:29.365935 Batch 4760/6251 train_loss = 1.084\n",
      "2020-06-30T19:34:29.567379 Batch 4780/6251 train_loss = 0.960\n",
      "2020-06-30T19:34:29.771364 Batch 4800/6251 train_loss = 0.888\n",
      "2020-06-30T19:34:29.973891 Batch 4820/6251 train_loss = 0.943\n",
      "2020-06-30T19:34:30.177423 Batch 4840/6251 train_loss = 0.768\n",
      "2020-06-30T19:34:30.385664 Batch 4860/6251 train_loss = 0.885\n",
      "2020-06-30T19:34:30.586535 Batch 4880/6251 train_loss = 0.800\n",
      "2020-06-30T19:34:30.782107 Batch 4900/6251 train_loss = 1.215\n",
      "2020-06-30T19:34:30.983458 Batch 4920/6251 train_loss = 0.977\n",
      "2020-06-30T19:34:31.176885 Batch 4940/6251 train_loss = 0.951\n",
      "2020-06-30T19:34:31.379884 Batch 4960/6251 train_loss = 1.076\n",
      "2020-06-30T19:34:31.579425 Batch 4980/6251 train_loss = 0.799\n",
      "2020-06-30T19:34:31.778977 Batch 5000/6251 train_loss = 0.942\n",
      "2020-06-30T19:34:31.978359 Batch 5020/6251 train_loss = 0.962\n",
      "2020-06-30T19:34:32.178594 Batch 5040/6251 train_loss = 0.850\n",
      "2020-06-30T19:34:32.377114 Batch 5060/6251 train_loss = 0.891\n",
      "2020-06-30T19:34:32.578914 Batch 5080/6251 train_loss = 0.864\n",
      "2020-06-30T19:34:32.814327 Batch 5100/6251 train_loss = 0.944\n",
      "2020-06-30T19:34:33.022562 Batch 5120/6251 train_loss = 0.771\n",
      "2020-06-30T19:34:33.223983 Batch 5140/6251 train_loss = 0.879\n",
      "2020-06-30T19:34:33.422965 Batch 5160/6251 train_loss = 1.040\n",
      "2020-06-30T19:34:33.628922 Batch 5180/6251 train_loss = 0.862\n",
      "2020-06-30T19:34:33.826688 Batch 5200/6251 train_loss = 0.704\n",
      "2020-06-30T19:34:34.020790 Batch 5220/6251 train_loss = 1.064\n",
      "2020-06-30T19:34:34.225038 Batch 5240/6251 train_loss = 0.792\n",
      "2020-06-30T19:34:34.435582 Batch 5260/6251 train_loss = 0.651\n",
      "2020-06-30T19:34:34.640630 Batch 5280/6251 train_loss = 0.933\n",
      "2020-06-30T19:34:34.846752 Batch 5300/6251 train_loss = 0.778\n",
      "2020-06-30T19:34:35.055847 Batch 5320/6251 train_loss = 1.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:34:35.263681 Batch 5340/6251 train_loss = 0.860\n",
      "2020-06-30T19:34:35.465012 Batch 5360/6251 train_loss = 0.940\n",
      "2020-06-30T19:34:35.674798 Batch 5380/6251 train_loss = 0.978\n",
      "2020-06-30T19:34:35.878876 Batch 5400/6251 train_loss = 1.036\n",
      "2020-06-30T19:34:36.090586 Batch 5420/6251 train_loss = 0.950\n",
      "2020-06-30T19:34:36.303890 Batch 5440/6251 train_loss = 0.805\n",
      "2020-06-30T19:34:36.514980 Batch 5460/6251 train_loss = 1.010\n",
      "2020-06-30T19:34:36.723176 Batch 5480/6251 train_loss = 0.708\n",
      "2020-06-30T19:34:36.926104 Batch 5500/6251 train_loss = 1.351\n",
      "2020-06-30T19:34:37.130392 Batch 5520/6251 train_loss = 0.900\n",
      "2020-06-30T19:34:37.341160 Batch 5540/6251 train_loss = 0.845\n",
      "2020-06-30T19:34:37.542702 Batch 5560/6251 train_loss = 0.878\n",
      "2020-06-30T19:34:37.742256 Batch 5580/6251 train_loss = 0.838\n",
      "2020-06-30T19:34:37.935787 Batch 5600/6251 train_loss = 1.118\n",
      "2020-06-30T19:34:38.133788 Batch 5620/6251 train_loss = 0.858\n",
      "2020-06-30T19:34:38.330726 Batch 5640/6251 train_loss = 1.173\n",
      "2020-06-30T19:34:38.534762 Batch 5660/6251 train_loss = 0.789\n",
      "2020-06-30T19:34:38.732304 Batch 5680/6251 train_loss = 0.728\n",
      "2020-06-30T19:34:38.931408 Batch 5700/6251 train_loss = 1.005\n",
      "2020-06-30T19:34:39.128818 Batch 5720/6251 train_loss = 0.980\n",
      "2020-06-30T19:34:39.328276 Batch 5740/6251 train_loss = 0.834\n",
      "2020-06-30T19:34:39.527550 Batch 5760/6251 train_loss = 0.917\n",
      "2020-06-30T19:34:39.729341 Batch 5780/6251 train_loss = 0.777\n",
      "2020-06-30T19:34:39.935047 Batch 5800/6251 train_loss = 0.749\n",
      "2020-06-30T19:34:40.140477 Batch 5820/6251 train_loss = 0.896\n",
      "2020-06-30T19:34:40.345258 Batch 5840/6251 train_loss = 0.716\n",
      "2020-06-30T19:34:40.539863 Batch 5860/6251 train_loss = 0.854\n",
      "2020-06-30T19:34:40.738309 Batch 5880/6251 train_loss = 1.042\n",
      "2020-06-30T19:34:40.930613 Batch 5900/6251 train_loss = 0.843\n",
      "2020-06-30T19:34:41.139179 Batch 5920/6251 train_loss = 0.867\n",
      "2020-06-30T19:34:41.331541 Batch 5940/6251 train_loss = 0.918\n",
      "2020-06-30T19:34:41.533342 Batch 5960/6251 train_loss = 0.949\n",
      "2020-06-30T19:34:41.726344 Batch 5980/6251 train_loss = 0.774\n",
      "2020-06-30T19:34:41.922967 Batch 6000/6251 train_loss = 1.061\n",
      "2020-06-30T19:34:42.121253 Batch 6020/6251 train_loss = 0.905\n",
      "2020-06-30T19:34:42.314748 Batch 6040/6251 train_loss = 1.045\n",
      "2020-06-30T19:34:42.508742 Batch 6060/6251 train_loss = 0.894\n",
      "2020-06-30T19:34:42.707075 Batch 6080/6251 train_loss = 0.982\n",
      "2020-06-30T19:34:42.903756 Batch 6100/6251 train_loss = 0.995\n",
      "2020-06-30T19:34:43.103092 Batch 6120/6251 train_loss = 0.737\n",
      "2020-06-30T19:34:43.296711 Batch 6140/6251 train_loss = 0.976\n",
      "2020-06-30T19:34:43.492384 Batch 6160/6251 train_loss = 0.955\n",
      "2020-06-30T19:34:43.688837 Batch 6180/6251 train_loss = 0.677\n",
      "2020-06-30T19:34:43.889835 Batch 6200/6251 train_loss = 1.221\n",
      "2020-06-30T19:34:44.087775 Batch 6220/6251 train_loss = 0.826\n",
      "2020-06-30T19:34:44.287107 Batch 6240/6251 train_loss = 0.816\n",
      "2020-06-30T19:34:44.396883:Epoch   0 Batch    0/6251 test_loss = 0.765\n",
      "2020-06-30T19:34:44.594526:Epoch   0 Batch   20/6251 test_loss = 0.845\n",
      "2020-06-30T19:34:44.797213:Epoch   0 Batch   40/6251 test_loss = 0.919\n",
      "2020-06-30T19:34:44.995017:Epoch   0 Batch   60/6251 test_loss = 0.857\n",
      "2020-06-30T19:34:45.194504:Epoch   0 Batch   80/6251 test_loss = 0.823\n",
      "2020-06-30T19:34:45.391703:Epoch   0 Batch  100/6251 test_loss = 0.914\n",
      "2020-06-30T19:34:45.589995:Epoch   0 Batch  120/6251 test_loss = 0.903\n",
      "2020-06-30T19:34:45.788516:Epoch   0 Batch  140/6251 test_loss = 0.812\n",
      "2020-06-30T19:34:45.984023:Epoch   0 Batch  160/6251 test_loss = 0.882\n",
      "2020-06-30T19:34:46.183164:Epoch   0 Batch  180/6251 test_loss = 0.827\n",
      "2020-06-30T19:34:46.386388:Epoch   0 Batch  200/6251 test_loss = 1.106\n",
      "2020-06-30T19:34:46.590505:Epoch   0 Batch  220/6251 test_loss = 0.931\n",
      "2020-06-30T19:34:46.790644:Epoch   0 Batch  240/6251 test_loss = 1.009\n",
      "2020-06-30T19:34:46.990862:Epoch   0 Batch  260/6251 test_loss = 0.705\n",
      "2020-06-30T19:34:47.189979:Epoch   0 Batch  280/6251 test_loss = 0.804\n",
      "2020-06-30T19:34:47.387742:Epoch   0 Batch  300/6251 test_loss = 0.959\n",
      "2020-06-30T19:34:47.584210:Epoch   0 Batch  320/6251 test_loss = 1.177\n",
      "2020-06-30T19:34:47.784484:Epoch   0 Batch  340/6251 test_loss = 0.770\n",
      "2020-06-30T19:34:47.984646:Epoch   0 Batch  360/6251 test_loss = 0.956\n",
      "2020-06-30T19:34:48.184485:Epoch   0 Batch  380/6251 test_loss = 0.862\n",
      "2020-06-30T19:34:48.386487:Epoch   0 Batch  400/6251 test_loss = 1.080\n",
      "2020-06-30T19:34:48.586739:Epoch   0 Batch  420/6251 test_loss = 0.895\n",
      "2020-06-30T19:34:48.789364:Epoch   0 Batch  440/6251 test_loss = 0.753\n",
      "2020-06-30T19:34:48.992836:Epoch   0 Batch  460/6251 test_loss = 0.865\n",
      "2020-06-30T19:34:49.196032:Epoch   0 Batch  480/6251 test_loss = 0.848\n",
      "2020-06-30T19:34:49.402759:Epoch   0 Batch  500/6251 test_loss = 0.866\n",
      "2020-06-30T19:34:49.599557:Epoch   0 Batch  520/6251 test_loss = 1.031\n",
      "2020-06-30T19:34:49.804064:Epoch   0 Batch  540/6251 test_loss = 0.955\n",
      "2020-06-30T19:34:50.009957:Epoch   0 Batch  560/6251 test_loss = 1.001\n",
      "2020-06-30T19:34:50.222090:Epoch   0 Batch  580/6251 test_loss = 0.804\n",
      "2020-06-30T19:34:50.422919:Epoch   0 Batch  600/6251 test_loss = 0.798\n",
      "2020-06-30T19:34:50.631217:Epoch   0 Batch  620/6251 test_loss = 1.081\n",
      "2020-06-30T19:34:50.826330:Epoch   0 Batch  640/6251 test_loss = 0.899\n",
      "2020-06-30T19:34:51.024318:Epoch   0 Batch  660/6251 test_loss = 0.767\n",
      "2020-06-30T19:34:51.224056:Epoch   0 Batch  680/6251 test_loss = 0.764\n",
      "2020-06-30T19:34:51.429634:Epoch   0 Batch  700/6251 test_loss = 0.932\n",
      "2020-06-30T19:34:51.627690:Epoch   0 Batch  720/6251 test_loss = 1.053\n",
      "2020-06-30T19:34:51.829170:Epoch   0 Batch  740/6251 test_loss = 0.952\n",
      "2020-06-30T19:34:52.032396:Epoch   0 Batch  760/6251 test_loss = 1.090\n",
      "2020-06-30T19:34:52.228864:Epoch   0 Batch  780/6251 test_loss = 0.842\n",
      "2020-06-30T19:34:52.434429:Epoch   0 Batch  800/6251 test_loss = 0.962\n",
      "2020-06-30T19:34:52.634362:Epoch   0 Batch  820/6251 test_loss = 0.847\n",
      "2020-06-30T19:34:52.837208:Epoch   0 Batch  840/6251 test_loss = 0.522\n",
      "2020-06-30T19:34:53.029001:Epoch   0 Batch  860/6251 test_loss = 0.743\n",
      "2020-06-30T19:34:53.231928:Epoch   0 Batch  880/6251 test_loss = 0.997\n",
      "2020-06-30T19:34:53.424082:Epoch   0 Batch  900/6251 test_loss = 0.900\n",
      "2020-06-30T19:34:53.626780:Epoch   0 Batch  920/6251 test_loss = 1.056\n",
      "2020-06-30T19:34:53.824510:Epoch   0 Batch  940/6251 test_loss = 0.882\n",
      "2020-06-30T19:34:54.026597:Epoch   0 Batch  960/6251 test_loss = 0.848\n",
      "2020-06-30T19:34:54.230986:Epoch   0 Batch  980/6251 test_loss = 0.908\n",
      "2020-06-30T19:34:54.436098:Epoch   0 Batch 1000/6251 test_loss = 0.736\n",
      "2020-06-30T19:34:54.636452:Epoch   0 Batch 1020/6251 test_loss = 1.340\n",
      "2020-06-30T19:34:54.837164:Epoch   0 Batch 1040/6251 test_loss = 0.758\n",
      "2020-06-30T19:34:55.032616:Epoch   0 Batch 1060/6251 test_loss = 0.609\n",
      "2020-06-30T19:34:55.236009:Epoch   0 Batch 1080/6251 test_loss = 0.822\n",
      "2020-06-30T19:34:55.427082:Epoch   0 Batch 1100/6251 test_loss = 0.821\n",
      "2020-06-30T19:34:55.626666:Epoch   0 Batch 1120/6251 test_loss = 0.925\n",
      "2020-06-30T19:34:55.824224:Epoch   0 Batch 1140/6251 test_loss = 0.856\n",
      "2020-06-30T19:34:56.026450:Epoch   0 Batch 1160/6251 test_loss = 0.926\n",
      "2020-06-30T19:34:56.222857:Epoch   0 Batch 1180/6251 test_loss = 0.781\n",
      "2020-06-30T19:34:56.419985:Epoch   0 Batch 1200/6251 test_loss = 1.091\n",
      "2020-06-30T19:34:56.622987:Epoch   0 Batch 1220/6251 test_loss = 0.862\n",
      "2020-06-30T19:34:56.823155:Epoch   0 Batch 1240/6251 test_loss = 0.898\n",
      "2020-06-30T19:34:57.021657:Epoch   0 Batch 1260/6251 test_loss = 0.766\n",
      "2020-06-30T19:34:57.222789:Epoch   0 Batch 1280/6251 test_loss = 0.825\n",
      "2020-06-30T19:34:57.427924:Epoch   0 Batch 1300/6251 test_loss = 0.659\n",
      "2020-06-30T19:34:57.628813:Epoch   0 Batch 1320/6251 test_loss = 0.882\n",
      "2020-06-30T19:34:57.827771:Epoch   0 Batch 1340/6251 test_loss = 0.883\n",
      "2020-06-30T19:34:58.024769:Epoch   0 Batch 1360/6251 test_loss = 1.119\n",
      "2020-06-30T19:34:58.222414:Epoch   0 Batch 1380/6251 test_loss = 0.754\n",
      "2020-06-30T19:34:58.425563:Epoch   0 Batch 1400/6251 test_loss = 0.900\n",
      "2020-06-30T19:34:58.623167:Epoch   0 Batch 1420/6251 test_loss = 0.596\n",
      "2020-06-30T19:34:58.826936:Epoch   0 Batch 1440/6251 test_loss = 1.147\n",
      "2020-06-30T19:34:59.024959:Epoch   0 Batch 1460/6251 test_loss = 0.993\n",
      "2020-06-30T19:34:59.227860:Epoch   0 Batch 1480/6251 test_loss = 0.893\n",
      "2020-06-30T19:34:59.431422:Epoch   0 Batch 1500/6251 test_loss = 0.869\n",
      "2020-06-30T19:34:59.632116:Epoch   0 Batch 1520/6251 test_loss = 0.774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:34:59.838395:Epoch   0 Batch 1540/6251 test_loss = 0.882\n",
      "2020-06-30T19:35:00.043516:Epoch   0 Batch 1560/6251 test_loss = 0.905\n",
      "2020-06-30T19:35:01.171033 Batch    9/6251 train_loss = 0.858\n",
      "2020-06-30T19:35:01.387108 Batch   29/6251 train_loss = 0.880\n",
      "2020-06-30T19:35:01.598531 Batch   49/6251 train_loss = 0.806\n",
      "2020-06-30T19:35:01.816342 Batch   69/6251 train_loss = 0.919\n",
      "2020-06-30T19:35:02.031095 Batch   89/6251 train_loss = 0.705\n",
      "2020-06-30T19:35:02.245161 Batch  109/6251 train_loss = 0.705\n",
      "2020-06-30T19:35:02.460995 Batch  129/6251 train_loss = 0.945\n",
      "2020-06-30T19:35:02.676032 Batch  149/6251 train_loss = 0.952\n",
      "2020-06-30T19:35:02.888811 Batch  169/6251 train_loss = 0.918\n",
      "2020-06-30T19:35:03.101844 Batch  189/6251 train_loss = 0.838\n",
      "2020-06-30T19:35:03.319752 Batch  209/6251 train_loss = 0.865\n",
      "2020-06-30T19:35:03.540752 Batch  229/6251 train_loss = 0.970\n",
      "2020-06-30T19:35:03.755539 Batch  249/6251 train_loss = 0.758\n",
      "2020-06-30T19:35:03.962925 Batch  269/6251 train_loss = 0.689\n",
      "2020-06-30T19:35:04.172119 Batch  289/6251 train_loss = 0.941\n",
      "2020-06-30T19:35:04.392092 Batch  309/6251 train_loss = 0.891\n",
      "2020-06-30T19:35:04.607527 Batch  329/6251 train_loss = 0.977\n",
      "2020-06-30T19:35:04.821887 Batch  349/6251 train_loss = 0.834\n",
      "2020-06-30T19:35:05.039520 Batch  369/6251 train_loss = 1.027\n",
      "2020-06-30T19:35:05.256894 Batch  389/6251 train_loss = 0.865\n",
      "2020-06-30T19:35:05.479496 Batch  409/6251 train_loss = 0.752\n",
      "2020-06-30T19:35:05.706630 Batch  429/6251 train_loss = 0.939\n",
      "2020-06-30T19:35:05.928445 Batch  449/6251 train_loss = 0.758\n",
      "2020-06-30T19:35:06.143518 Batch  469/6251 train_loss = 0.917\n",
      "2020-06-30T19:35:06.365955 Batch  489/6251 train_loss = 0.818\n",
      "2020-06-30T19:35:06.582399 Batch  509/6251 train_loss = 0.852\n",
      "2020-06-30T19:35:06.790853 Batch  529/6251 train_loss = 0.945\n",
      "2020-06-30T19:35:07.009042 Batch  549/6251 train_loss = 0.939\n",
      "2020-06-30T19:35:07.228706 Batch  569/6251 train_loss = 0.920\n",
      "2020-06-30T19:35:07.442120 Batch  589/6251 train_loss = 0.856\n",
      "2020-06-30T19:35:07.663425 Batch  609/6251 train_loss = 0.956\n",
      "2020-06-30T19:35:07.877534 Batch  629/6251 train_loss = 1.015\n",
      "2020-06-30T19:35:08.095896 Batch  649/6251 train_loss = 0.943\n",
      "2020-06-30T19:35:08.310036 Batch  669/6251 train_loss = 1.004\n",
      "2020-06-30T19:35:08.528819 Batch  689/6251 train_loss = 1.166\n",
      "2020-06-30T19:35:08.741233 Batch  709/6251 train_loss = 0.887\n",
      "2020-06-30T19:35:08.963009 Batch  729/6251 train_loss = 0.805\n",
      "2020-06-30T19:35:09.177249 Batch  749/6251 train_loss = 0.947\n",
      "2020-06-30T19:35:09.397974 Batch  769/6251 train_loss = 0.787\n",
      "2020-06-30T19:35:09.621457 Batch  789/6251 train_loss = 0.976\n",
      "2020-06-30T19:35:09.838846 Batch  809/6251 train_loss = 0.864\n",
      "2020-06-30T19:35:10.059626 Batch  829/6251 train_loss = 0.558\n",
      "2020-06-30T19:35:10.269474 Batch  849/6251 train_loss = 0.949\n",
      "2020-06-30T19:35:10.487911 Batch  869/6251 train_loss = 0.846\n",
      "2020-06-30T19:35:10.705615 Batch  889/6251 train_loss = 0.945\n",
      "2020-06-30T19:35:10.918880 Batch  909/6251 train_loss = 0.974\n",
      "2020-06-30T19:35:11.140462 Batch  929/6251 train_loss = 0.822\n",
      "2020-06-30T19:35:11.357328 Batch  949/6251 train_loss = 0.734\n",
      "2020-06-30T19:35:11.573210 Batch  969/6251 train_loss = 0.888\n",
      "2020-06-30T19:35:11.790529 Batch  989/6251 train_loss = 0.827\n",
      "2020-06-30T19:35:12.011091 Batch 1009/6251 train_loss = 0.862\n",
      "2020-06-30T19:35:12.229200 Batch 1029/6251 train_loss = 0.751\n",
      "2020-06-30T19:35:12.443832 Batch 1049/6251 train_loss = 0.837\n",
      "2020-06-30T19:35:12.661439 Batch 1069/6251 train_loss = 0.935\n",
      "2020-06-30T19:35:12.880468 Batch 1089/6251 train_loss = 0.826\n",
      "2020-06-30T19:35:13.096761 Batch 1109/6251 train_loss = 0.757\n",
      "2020-06-30T19:35:13.315363 Batch 1129/6251 train_loss = 1.020\n",
      "2020-06-30T19:35:13.527221 Batch 1149/6251 train_loss = 0.787\n",
      "2020-06-30T19:35:13.745187 Batch 1169/6251 train_loss = 0.889\n",
      "2020-06-30T19:35:13.953445 Batch 1189/6251 train_loss = 0.793\n",
      "2020-06-30T19:35:14.172924 Batch 1209/6251 train_loss = 1.046\n",
      "2020-06-30T19:35:14.395142 Batch 1229/6251 train_loss = 1.056\n",
      "2020-06-30T19:35:14.605263 Batch 1249/6251 train_loss = 0.810\n",
      "2020-06-30T19:35:14.819967 Batch 1269/6251 train_loss = 0.715\n",
      "2020-06-30T19:35:15.032258 Batch 1289/6251 train_loss = 0.845\n",
      "2020-06-30T19:35:15.252389 Batch 1309/6251 train_loss = 0.649\n",
      "2020-06-30T19:35:15.470829 Batch 1329/6251 train_loss = 0.905\n",
      "2020-06-30T19:35:15.689349 Batch 1349/6251 train_loss = 0.879\n",
      "2020-06-30T19:35:15.908041 Batch 1369/6251 train_loss = 1.013\n",
      "2020-06-30T19:35:16.128189 Batch 1389/6251 train_loss = 0.821\n",
      "2020-06-30T19:35:16.345649 Batch 1409/6251 train_loss = 0.887\n",
      "2020-06-30T19:35:16.575287 Batch 1429/6251 train_loss = 0.800\n",
      "2020-06-30T19:35:16.789792 Batch 1449/6251 train_loss = 0.829\n",
      "2020-06-30T19:35:17.005081 Batch 1469/6251 train_loss = 0.883\n",
      "2020-06-30T19:35:17.221408 Batch 1489/6251 train_loss = 0.866\n",
      "2020-06-30T19:35:17.438983 Batch 1509/6251 train_loss = 0.835\n",
      "2020-06-30T19:35:17.649404 Batch 1529/6251 train_loss = 0.898\n",
      "2020-06-30T19:35:17.863752 Batch 1549/6251 train_loss = 0.856\n",
      "2020-06-30T19:35:18.083744 Batch 1569/6251 train_loss = 1.050\n",
      "2020-06-30T19:35:18.303709 Batch 1589/6251 train_loss = 0.853\n",
      "2020-06-30T19:35:18.518543 Batch 1609/6251 train_loss = 0.987\n",
      "2020-06-30T19:35:18.732805 Batch 1629/6251 train_loss = 0.872\n",
      "2020-06-30T19:35:18.949165 Batch 1649/6251 train_loss = 1.025\n",
      "2020-06-30T19:35:19.177183 Batch 1669/6251 train_loss = 0.936\n",
      "2020-06-30T19:35:19.394277 Batch 1689/6251 train_loss = 0.918\n",
      "2020-06-30T19:35:19.616292 Batch 1709/6251 train_loss = 0.944\n",
      "2020-06-30T19:35:19.833008 Batch 1729/6251 train_loss = 0.854\n",
      "2020-06-30T19:35:20.048828 Batch 1749/6251 train_loss = 0.808\n",
      "2020-06-30T19:35:20.267984 Batch 1769/6251 train_loss = 1.199\n",
      "2020-06-30T19:35:20.480848 Batch 1789/6251 train_loss = 1.213\n",
      "2020-06-30T19:35:20.699521 Batch 1809/6251 train_loss = 1.005\n",
      "2020-06-30T19:35:20.916977 Batch 1829/6251 train_loss = 1.085\n",
      "2020-06-30T19:35:21.136867 Batch 1849/6251 train_loss = 0.921\n",
      "2020-06-30T19:35:21.350811 Batch 1869/6251 train_loss = 0.788\n",
      "2020-06-30T19:35:21.566235 Batch 1889/6251 train_loss = 0.873\n",
      "2020-06-30T19:35:21.786448 Batch 1909/6251 train_loss = 0.888\n",
      "2020-06-30T19:35:22.010126 Batch 1929/6251 train_loss = 0.917\n",
      "2020-06-30T19:35:22.228172 Batch 1949/6251 train_loss = 0.996\n",
      "2020-06-30T19:35:22.446180 Batch 1969/6251 train_loss = 0.828\n",
      "2020-06-30T19:35:22.661534 Batch 1989/6251 train_loss = 0.785\n",
      "2020-06-30T19:35:22.876803 Batch 2009/6251 train_loss = 0.816\n",
      "2020-06-30T19:35:23.085429 Batch 2029/6251 train_loss = 1.139\n",
      "2020-06-30T19:35:23.303389 Batch 2049/6251 train_loss = 0.917\n",
      "2020-06-30T19:35:23.520062 Batch 2069/6251 train_loss = 0.882\n",
      "2020-06-30T19:35:23.730668 Batch 2089/6251 train_loss = 0.995\n",
      "2020-06-30T19:35:23.952334 Batch 2109/6251 train_loss = 0.926\n",
      "2020-06-30T19:35:24.171763 Batch 2129/6251 train_loss = 0.649\n",
      "2020-06-30T19:35:24.382365 Batch 2149/6251 train_loss = 0.858\n",
      "2020-06-30T19:35:24.590424 Batch 2169/6251 train_loss = 0.937\n",
      "2020-06-30T19:35:24.806451 Batch 2189/6251 train_loss = 0.863\n",
      "2020-06-30T19:35:25.021036 Batch 2209/6251 train_loss = 1.029\n",
      "2020-06-30T19:35:25.229243 Batch 2229/6251 train_loss = 0.732\n",
      "2020-06-30T19:35:25.446558 Batch 2249/6251 train_loss = 1.001\n",
      "2020-06-30T19:35:25.658110 Batch 2269/6251 train_loss = 0.805\n",
      "2020-06-30T19:35:25.872131 Batch 2289/6251 train_loss = 0.976\n",
      "2020-06-30T19:35:26.089158 Batch 2309/6251 train_loss = 0.977\n",
      "2020-06-30T19:35:26.305340 Batch 2329/6251 train_loss = 0.984\n",
      "2020-06-30T19:35:26.521649 Batch 2349/6251 train_loss = 0.816\n",
      "2020-06-30T19:35:26.726747 Batch 2369/6251 train_loss = 0.935\n",
      "2020-06-30T19:35:26.939146 Batch 2389/6251 train_loss = 0.933\n",
      "2020-06-30T19:35:27.153051 Batch 2409/6251 train_loss = 0.903\n",
      "2020-06-30T19:35:27.372572 Batch 2429/6251 train_loss = 0.772\n",
      "2020-06-30T19:35:27.583621 Batch 2449/6251 train_loss = 0.819\n",
      "2020-06-30T19:35:27.799349 Batch 2469/6251 train_loss = 0.806\n",
      "2020-06-30T19:35:28.012124 Batch 2489/6251 train_loss = 1.003\n",
      "2020-06-30T19:35:28.231317 Batch 2509/6251 train_loss = 0.950\n",
      "2020-06-30T19:35:28.444029 Batch 2529/6251 train_loss = 0.770\n",
      "2020-06-30T19:35:28.656971 Batch 2549/6251 train_loss = 1.051\n",
      "2020-06-30T19:35:28.866492 Batch 2569/6251 train_loss = 1.060\n",
      "2020-06-30T19:35:29.083442 Batch 2589/6251 train_loss = 0.918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:35:29.305198 Batch 2609/6251 train_loss = 0.725\n",
      "2020-06-30T19:35:29.527916 Batch 2629/6251 train_loss = 0.883\n",
      "2020-06-30T19:35:29.740724 Batch 2649/6251 train_loss = 0.871\n",
      "2020-06-30T19:35:29.954921 Batch 2669/6251 train_loss = 0.665\n",
      "2020-06-30T19:35:30.171346 Batch 2689/6251 train_loss = 0.839\n",
      "2020-06-30T19:35:30.384687 Batch 2709/6251 train_loss = 0.765\n",
      "2020-06-30T19:35:30.588904 Batch 2729/6251 train_loss = 0.792\n",
      "2020-06-30T19:35:30.803465 Batch 2749/6251 train_loss = 0.775\n",
      "2020-06-30T19:35:31.020021 Batch 2769/6251 train_loss = 1.068\n",
      "2020-06-30T19:35:31.236744 Batch 2789/6251 train_loss = 0.747\n",
      "2020-06-30T19:35:31.448621 Batch 2809/6251 train_loss = 0.937\n",
      "2020-06-30T19:35:31.663175 Batch 2829/6251 train_loss = 0.809\n",
      "2020-06-30T19:35:31.881271 Batch 2849/6251 train_loss = 0.876\n",
      "2020-06-30T19:35:32.095102 Batch 2869/6251 train_loss = 0.690\n",
      "2020-06-30T19:35:32.311484 Batch 2889/6251 train_loss = 0.792\n",
      "2020-06-30T19:35:32.525169 Batch 2909/6251 train_loss = 0.937\n",
      "2020-06-30T19:35:32.736864 Batch 2929/6251 train_loss = 0.833\n",
      "2020-06-30T19:35:32.957221 Batch 2949/6251 train_loss = 0.685\n",
      "2020-06-30T19:35:33.175962 Batch 2969/6251 train_loss = 0.959\n",
      "2020-06-30T19:35:33.390192 Batch 2989/6251 train_loss = 0.874\n",
      "2020-06-30T19:35:33.605247 Batch 3009/6251 train_loss = 0.938\n",
      "2020-06-30T19:35:33.819471 Batch 3029/6251 train_loss = 0.689\n",
      "2020-06-30T19:35:34.032419 Batch 3049/6251 train_loss = 0.830\n",
      "2020-06-30T19:35:34.246779 Batch 3069/6251 train_loss = 1.034\n",
      "2020-06-30T19:35:34.463206 Batch 3089/6251 train_loss = 0.903\n",
      "2020-06-30T19:35:34.678424 Batch 3109/6251 train_loss = 0.696\n",
      "2020-06-30T19:35:34.890988 Batch 3129/6251 train_loss = 0.697\n",
      "2020-06-30T19:35:35.112152 Batch 3149/6251 train_loss = 0.892\n",
      "2020-06-30T19:35:35.325492 Batch 3169/6251 train_loss = 0.652\n",
      "2020-06-30T19:35:35.543615 Batch 3189/6251 train_loss = 0.728\n",
      "2020-06-30T19:35:35.755661 Batch 3209/6251 train_loss = 0.988\n",
      "2020-06-30T19:35:35.976690 Batch 3229/6251 train_loss = 0.919\n",
      "2020-06-30T19:35:36.191954 Batch 3249/6251 train_loss = 0.893\n",
      "2020-06-30T19:35:36.400666 Batch 3269/6251 train_loss = 1.308\n",
      "2020-06-30T19:35:36.619212 Batch 3289/6251 train_loss = 0.713\n",
      "2020-06-30T19:35:36.835032 Batch 3309/6251 train_loss = 0.765\n",
      "2020-06-30T19:35:37.047318 Batch 3329/6251 train_loss = 0.920\n",
      "2020-06-30T19:35:37.261222 Batch 3349/6251 train_loss = 0.759\n",
      "2020-06-30T19:35:37.476494 Batch 3369/6251 train_loss = 0.757\n",
      "2020-06-30T19:35:37.698433 Batch 3389/6251 train_loss = 0.829\n",
      "2020-06-30T19:35:37.915261 Batch 3409/6251 train_loss = 0.759\n",
      "2020-06-30T19:35:38.130299 Batch 3429/6251 train_loss = 1.023\n",
      "2020-06-30T19:35:38.345744 Batch 3449/6251 train_loss = 1.125\n",
      "2020-06-30T19:35:38.558952 Batch 3469/6251 train_loss = 0.845\n",
      "2020-06-30T19:35:38.773437 Batch 3489/6251 train_loss = 0.918\n",
      "2020-06-30T19:35:38.987046 Batch 3509/6251 train_loss = 1.004\n",
      "2020-06-30T19:35:39.197215 Batch 3529/6251 train_loss = 1.152\n",
      "2020-06-30T19:35:39.416110 Batch 3549/6251 train_loss = 0.866\n",
      "2020-06-30T19:35:39.629906 Batch 3569/6251 train_loss = 1.057\n",
      "2020-06-30T19:35:39.849031 Batch 3589/6251 train_loss = 0.997\n",
      "2020-06-30T19:35:40.076413 Batch 3609/6251 train_loss = 0.887\n",
      "2020-06-30T19:35:40.291585 Batch 3629/6251 train_loss = 0.897\n",
      "2020-06-30T19:35:40.497158 Batch 3649/6251 train_loss = 0.730\n",
      "2020-06-30T19:35:40.711870 Batch 3669/6251 train_loss = 0.954\n",
      "2020-06-30T19:35:40.926129 Batch 3689/6251 train_loss = 1.020\n",
      "2020-06-30T19:35:41.137233 Batch 3709/6251 train_loss = 1.098\n",
      "2020-06-30T19:35:41.354114 Batch 3729/6251 train_loss = 0.710\n",
      "2020-06-30T19:35:41.568799 Batch 3749/6251 train_loss = 0.952\n",
      "2020-06-30T19:35:41.775711 Batch 3769/6251 train_loss = 0.972\n",
      "2020-06-30T19:35:41.993310 Batch 3789/6251 train_loss = 0.699\n",
      "2020-06-30T19:35:42.200654 Batch 3809/6251 train_loss = 0.960\n",
      "2020-06-30T19:35:42.412999 Batch 3829/6251 train_loss = 1.087\n",
      "2020-06-30T19:35:42.626928 Batch 3849/6251 train_loss = 1.141\n",
      "2020-06-30T19:35:42.841647 Batch 3869/6251 train_loss = 0.795\n",
      "2020-06-30T19:35:43.062386 Batch 3889/6251 train_loss = 0.831\n",
      "2020-06-30T19:35:43.276838 Batch 3909/6251 train_loss = 0.764\n",
      "2020-06-30T19:35:43.491386 Batch 3929/6251 train_loss = 0.945\n",
      "2020-06-30T19:35:43.704654 Batch 3949/6251 train_loss = 0.902\n",
      "2020-06-30T19:35:43.921811 Batch 3969/6251 train_loss = 0.767\n",
      "2020-06-30T19:35:44.135214 Batch 3989/6251 train_loss = 0.844\n",
      "2020-06-30T19:35:44.346401 Batch 4009/6251 train_loss = 0.956\n",
      "2020-06-30T19:35:44.565794 Batch 4029/6251 train_loss = 0.941\n",
      "2020-06-30T19:35:44.787640 Batch 4049/6251 train_loss = 0.850\n",
      "2020-06-30T19:35:45.010683 Batch 4069/6251 train_loss = 0.921\n",
      "2020-06-30T19:35:45.224666 Batch 4089/6251 train_loss = 1.101\n",
      "2020-06-30T19:35:45.442330 Batch 4109/6251 train_loss = 0.688\n",
      "2020-06-30T19:35:45.663639 Batch 4129/6251 train_loss = 0.668\n",
      "2020-06-30T19:35:45.877182 Batch 4149/6251 train_loss = 0.881\n",
      "2020-06-30T19:35:46.097520 Batch 4169/6251 train_loss = 0.886\n",
      "2020-06-30T19:35:46.316073 Batch 4189/6251 train_loss = 0.917\n",
      "2020-06-30T19:35:46.537555 Batch 4209/6251 train_loss = 0.696\n",
      "2020-06-30T19:35:46.757921 Batch 4229/6251 train_loss = 0.925\n",
      "2020-06-30T19:35:46.972004 Batch 4249/6251 train_loss = 0.733\n",
      "2020-06-30T19:35:47.191621 Batch 4269/6251 train_loss = 0.931\n",
      "2020-06-30T19:35:47.405684 Batch 4289/6251 train_loss = 0.693\n",
      "2020-06-30T19:35:47.617837 Batch 4309/6251 train_loss = 0.796\n",
      "2020-06-30T19:35:47.832099 Batch 4329/6251 train_loss = 0.770\n",
      "2020-06-30T19:35:48.052795 Batch 4349/6251 train_loss = 0.813\n",
      "2020-06-30T19:35:48.278584 Batch 4369/6251 train_loss = 0.910\n",
      "2020-06-30T19:35:48.495795 Batch 4389/6251 train_loss = 0.847\n",
      "2020-06-30T19:35:48.724571 Batch 4409/6251 train_loss = 0.739\n",
      "2020-06-30T19:35:48.947788 Batch 4429/6251 train_loss = 1.001\n",
      "2020-06-30T19:35:49.182745 Batch 4449/6251 train_loss = 0.662\n",
      "2020-06-30T19:35:49.405324 Batch 4469/6251 train_loss = 0.818\n",
      "2020-06-30T19:35:49.618588 Batch 4489/6251 train_loss = 0.837\n",
      "2020-06-30T19:35:49.838510 Batch 4509/6251 train_loss = 0.974\n",
      "2020-06-30T19:35:50.048805 Batch 4529/6251 train_loss = 0.828\n",
      "2020-06-30T19:35:50.262064 Batch 4549/6251 train_loss = 1.082\n",
      "2020-06-30T19:35:50.476787 Batch 4569/6251 train_loss = 0.980\n",
      "2020-06-30T19:35:50.704204 Batch 4589/6251 train_loss = 0.735\n",
      "2020-06-30T19:35:50.930763 Batch 4609/6251 train_loss = 0.878\n",
      "2020-06-30T19:35:51.152712 Batch 4629/6251 train_loss = 1.036\n",
      "2020-06-30T19:35:51.390423 Batch 4649/6251 train_loss = 0.777\n",
      "2020-06-30T19:35:51.620076 Batch 4669/6251 train_loss = 0.860\n",
      "2020-06-30T19:35:51.839854 Batch 4689/6251 train_loss = 0.942\n",
      "2020-06-30T19:35:52.063477 Batch 4709/6251 train_loss = 1.047\n",
      "2020-06-30T19:35:52.306714 Batch 4729/6251 train_loss = 0.898\n",
      "2020-06-30T19:35:52.532407 Batch 4749/6251 train_loss = 0.842\n",
      "2020-06-30T19:35:52.762972 Batch 4769/6251 train_loss = 1.005\n",
      "2020-06-30T19:35:52.979843 Batch 4789/6251 train_loss = 0.788\n",
      "2020-06-30T19:35:53.198567 Batch 4809/6251 train_loss = 0.980\n",
      "2020-06-30T19:35:53.419628 Batch 4829/6251 train_loss = 0.845\n",
      "2020-06-30T19:35:53.626249 Batch 4849/6251 train_loss = 0.827\n",
      "2020-06-30T19:35:53.845687 Batch 4869/6251 train_loss = 1.194\n",
      "2020-06-30T19:35:54.064798 Batch 4889/6251 train_loss = 0.706\n",
      "2020-06-30T19:35:54.288390 Batch 4909/6251 train_loss = 0.865\n",
      "2020-06-30T19:35:54.518090 Batch 4929/6251 train_loss = 0.861\n",
      "2020-06-30T19:35:54.739525 Batch 4949/6251 train_loss = 0.862\n",
      "2020-06-30T19:35:54.971019 Batch 4969/6251 train_loss = 0.915\n",
      "2020-06-30T19:35:55.196460 Batch 4989/6251 train_loss = 0.790\n",
      "2020-06-30T19:35:55.413951 Batch 5009/6251 train_loss = 0.806\n",
      "2020-06-30T19:35:55.639644 Batch 5029/6251 train_loss = 1.035\n",
      "2020-06-30T19:35:55.860812 Batch 5049/6251 train_loss = 0.847\n",
      "2020-06-30T19:35:56.153021 Batch 5069/6251 train_loss = 0.880\n",
      "2020-06-30T19:35:56.417163 Batch 5089/6251 train_loss = 1.137\n",
      "2020-06-30T19:35:56.654437 Batch 5109/6251 train_loss = 0.943\n",
      "2020-06-30T19:35:56.878803 Batch 5129/6251 train_loss = 0.788\n",
      "2020-06-30T19:35:57.102937 Batch 5149/6251 train_loss = 0.819\n",
      "2020-06-30T19:35:57.305639 Batch 5169/6251 train_loss = 0.937\n",
      "2020-06-30T19:35:57.515804 Batch 5189/6251 train_loss = 0.764\n",
      "2020-06-30T19:35:57.722547 Batch 5209/6251 train_loss = 0.911\n",
      "2020-06-30T19:35:57.937157 Batch 5229/6251 train_loss = 0.885\n",
      "2020-06-30T19:35:58.139445 Batch 5249/6251 train_loss = 0.988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:35:58.343519 Batch 5269/6251 train_loss = 0.829\n",
      "2020-06-30T19:35:58.547421 Batch 5289/6251 train_loss = 0.906\n",
      "2020-06-30T19:35:58.758228 Batch 5309/6251 train_loss = 0.856\n",
      "2020-06-30T19:35:58.964420 Batch 5329/6251 train_loss = 0.760\n",
      "2020-06-30T19:35:59.170317 Batch 5349/6251 train_loss = 0.883\n",
      "2020-06-30T19:35:59.376492 Batch 5369/6251 train_loss = 1.097\n",
      "2020-06-30T19:35:59.586791 Batch 5389/6251 train_loss = 0.881\n",
      "2020-06-30T19:35:59.794283 Batch 5409/6251 train_loss = 0.681\n",
      "2020-06-30T19:36:00.000932 Batch 5429/6251 train_loss = 1.155\n",
      "2020-06-30T19:36:00.211959 Batch 5449/6251 train_loss = 0.937\n",
      "2020-06-30T19:36:00.419800 Batch 5469/6251 train_loss = 0.690\n",
      "2020-06-30T19:36:00.632241 Batch 5489/6251 train_loss = 0.955\n",
      "2020-06-30T19:36:00.845405 Batch 5509/6251 train_loss = 0.826\n",
      "2020-06-30T19:36:01.065224 Batch 5529/6251 train_loss = 0.928\n",
      "2020-06-30T19:36:01.302196 Batch 5549/6251 train_loss = 0.697\n",
      "2020-06-30T19:36:01.706388 Batch 5569/6251 train_loss = 0.945\n",
      "2020-06-30T19:36:02.090230 Batch 5589/6251 train_loss = 0.776\n",
      "2020-06-30T19:36:02.372098 Batch 5609/6251 train_loss = 0.637\n",
      "2020-06-30T19:36:02.652676 Batch 5629/6251 train_loss = 0.746\n",
      "2020-06-30T19:36:02.971450 Batch 5649/6251 train_loss = 0.798\n",
      "2020-06-30T19:36:03.269177 Batch 5669/6251 train_loss = 0.840\n",
      "2020-06-30T19:36:03.539270 Batch 5689/6251 train_loss = 0.738\n",
      "2020-06-30T19:36:03.793668 Batch 5709/6251 train_loss = 0.883\n",
      "2020-06-30T19:36:04.035361 Batch 5729/6251 train_loss = 0.979\n",
      "2020-06-30T19:36:04.284786 Batch 5749/6251 train_loss = 0.951\n",
      "2020-06-30T19:36:04.529943 Batch 5769/6251 train_loss = 0.812\n",
      "2020-06-30T19:36:04.772727 Batch 5789/6251 train_loss = 0.830\n",
      "2020-06-30T19:36:05.000074 Batch 5809/6251 train_loss = 0.879\n",
      "2020-06-30T19:36:05.222961 Batch 5829/6251 train_loss = 0.893\n",
      "2020-06-30T19:36:05.435038 Batch 5849/6251 train_loss = 0.810\n",
      "2020-06-30T19:36:05.650548 Batch 5869/6251 train_loss = 0.664\n",
      "2020-06-30T19:36:05.866139 Batch 5889/6251 train_loss = 0.979\n",
      "2020-06-30T19:36:06.073401 Batch 5909/6251 train_loss = 0.945\n",
      "2020-06-30T19:36:06.296484 Batch 5929/6251 train_loss = 1.148\n",
      "2020-06-30T19:36:06.513077 Batch 5949/6251 train_loss = 0.854\n",
      "2020-06-30T19:36:06.721713 Batch 5969/6251 train_loss = 0.805\n",
      "2020-06-30T19:36:06.924044 Batch 5989/6251 train_loss = 0.743\n",
      "2020-06-30T19:36:07.125015 Batch 6009/6251 train_loss = 0.834\n",
      "2020-06-30T19:36:07.327040 Batch 6029/6251 train_loss = 1.048\n",
      "2020-06-30T19:36:07.542257 Batch 6049/6251 train_loss = 0.828\n",
      "2020-06-30T19:36:07.744434 Batch 6069/6251 train_loss = 0.788\n",
      "2020-06-30T19:36:07.951639 Batch 6089/6251 train_loss = 0.872\n",
      "2020-06-30T19:36:08.166051 Batch 6109/6251 train_loss = 0.751\n",
      "2020-06-30T19:36:08.375972 Batch 6129/6251 train_loss = 1.051\n",
      "2020-06-30T19:36:08.581508 Batch 6149/6251 train_loss = 0.948\n",
      "2020-06-30T19:36:08.787577 Batch 6169/6251 train_loss = 0.888\n",
      "2020-06-30T19:36:08.993842 Batch 6189/6251 train_loss = 0.760\n",
      "2020-06-30T19:36:09.201789 Batch 6209/6251 train_loss = 0.960\n",
      "2020-06-30T19:36:09.405250 Batch 6229/6251 train_loss = 0.821\n",
      "2020-06-30T19:36:09.626478 Batch 6249/6251 train_loss = 0.955\n",
      "2020-06-30T19:36:09.735306:Epoch   1 Batch    9/6251 test_loss = 0.878\n",
      "2020-06-30T19:36:09.929595:Epoch   1 Batch   29/6251 test_loss = 0.889\n",
      "2020-06-30T19:36:10.130686:Epoch   1 Batch   49/6251 test_loss = 0.978\n",
      "2020-06-30T19:36:10.330916:Epoch   1 Batch   69/6251 test_loss = 0.725\n",
      "2020-06-30T19:36:10.531540:Epoch   1 Batch   89/6251 test_loss = 0.915\n",
      "2020-06-30T19:36:10.732516:Epoch   1 Batch  109/6251 test_loss = 0.719\n",
      "2020-06-30T19:36:10.928803:Epoch   1 Batch  129/6251 test_loss = 1.031\n",
      "2020-06-30T19:36:11.128355:Epoch   1 Batch  149/6251 test_loss = 0.927\n",
      "2020-06-30T19:36:11.347401:Epoch   1 Batch  169/6251 test_loss = 0.957\n",
      "2020-06-30T19:36:11.556700:Epoch   1 Batch  189/6251 test_loss = 0.987\n",
      "2020-06-30T19:36:11.760970:Epoch   1 Batch  209/6251 test_loss = 0.959\n",
      "2020-06-30T19:36:11.960843:Epoch   1 Batch  229/6251 test_loss = 0.893\n",
      "2020-06-30T19:36:12.173860:Epoch   1 Batch  249/6251 test_loss = 0.814\n",
      "2020-06-30T19:36:12.379759:Epoch   1 Batch  269/6251 test_loss = 0.888\n",
      "2020-06-30T19:36:12.582955:Epoch   1 Batch  289/6251 test_loss = 0.879\n",
      "2020-06-30T19:36:12.797791:Epoch   1 Batch  309/6251 test_loss = 0.742\n",
      "2020-06-30T19:36:13.002681:Epoch   1 Batch  329/6251 test_loss = 0.831\n",
      "2020-06-30T19:36:13.219895:Epoch   1 Batch  349/6251 test_loss = 0.916\n",
      "2020-06-30T19:36:13.431945:Epoch   1 Batch  369/6251 test_loss = 0.883\n",
      "2020-06-30T19:36:13.629840:Epoch   1 Batch  389/6251 test_loss = 1.018\n",
      "2020-06-30T19:36:13.832778:Epoch   1 Batch  409/6251 test_loss = 0.916\n",
      "2020-06-30T19:36:14.029680:Epoch   1 Batch  429/6251 test_loss = 0.889\n",
      "2020-06-30T19:36:14.264234:Epoch   1 Batch  449/6251 test_loss = 0.840\n",
      "2020-06-30T19:36:14.544603:Epoch   1 Batch  469/6251 test_loss = 0.903\n",
      "2020-06-30T19:36:14.761152:Epoch   1 Batch  489/6251 test_loss = 0.938\n",
      "2020-06-30T19:36:14.971541:Epoch   1 Batch  509/6251 test_loss = 1.001\n",
      "2020-06-30T19:36:15.203348:Epoch   1 Batch  529/6251 test_loss = 0.931\n",
      "2020-06-30T19:36:15.412798:Epoch   1 Batch  549/6251 test_loss = 0.820\n",
      "2020-06-30T19:36:15.614364:Epoch   1 Batch  569/6251 test_loss = 0.734\n",
      "2020-06-30T19:36:15.824315:Epoch   1 Batch  589/6251 test_loss = 0.647\n",
      "2020-06-30T19:36:16.044813:Epoch   1 Batch  609/6251 test_loss = 0.954\n",
      "2020-06-30T19:36:16.301598:Epoch   1 Batch  629/6251 test_loss = 0.876\n",
      "2020-06-30T19:36:16.582435:Epoch   1 Batch  649/6251 test_loss = 0.845\n",
      "2020-06-30T19:36:16.794698:Epoch   1 Batch  669/6251 test_loss = 0.781\n",
      "2020-06-30T19:36:17.012294:Epoch   1 Batch  689/6251 test_loss = 1.028\n",
      "2020-06-30T19:36:17.219743:Epoch   1 Batch  709/6251 test_loss = 0.717\n",
      "2020-06-30T19:36:17.442417:Epoch   1 Batch  729/6251 test_loss = 0.889\n",
      "2020-06-30T19:36:17.662698:Epoch   1 Batch  749/6251 test_loss = 0.940\n",
      "2020-06-30T19:36:17.879465:Epoch   1 Batch  769/6251 test_loss = 0.942\n",
      "2020-06-30T19:36:18.095579:Epoch   1 Batch  789/6251 test_loss = 0.787\n",
      "2020-06-30T19:36:18.305003:Epoch   1 Batch  809/6251 test_loss = 0.705\n",
      "2020-06-30T19:36:18.518470:Epoch   1 Batch  829/6251 test_loss = 0.705\n",
      "2020-06-30T19:36:18.733463:Epoch   1 Batch  849/6251 test_loss = 0.851\n",
      "2020-06-30T19:36:18.953983:Epoch   1 Batch  869/6251 test_loss = 0.923\n",
      "2020-06-30T19:36:19.163958:Epoch   1 Batch  889/6251 test_loss = 0.852\n",
      "2020-06-30T19:36:19.376218:Epoch   1 Batch  909/6251 test_loss = 0.896\n",
      "2020-06-30T19:36:19.595786:Epoch   1 Batch  929/6251 test_loss = 0.884\n",
      "2020-06-30T19:36:19.801243:Epoch   1 Batch  949/6251 test_loss = 1.034\n",
      "2020-06-30T19:36:20.009747:Epoch   1 Batch  969/6251 test_loss = 0.883\n",
      "2020-06-30T19:36:20.215021:Epoch   1 Batch  989/6251 test_loss = 0.865\n",
      "2020-06-30T19:36:20.430653:Epoch   1 Batch 1009/6251 test_loss = 0.895\n",
      "2020-06-30T19:36:20.645549:Epoch   1 Batch 1029/6251 test_loss = 1.019\n",
      "2020-06-30T19:36:20.847036:Epoch   1 Batch 1049/6251 test_loss = 0.816\n",
      "2020-06-30T19:36:21.053932:Epoch   1 Batch 1069/6251 test_loss = 0.970\n",
      "2020-06-30T19:36:21.258139:Epoch   1 Batch 1089/6251 test_loss = 0.950\n",
      "2020-06-30T19:36:21.489394:Epoch   1 Batch 1109/6251 test_loss = 0.681\n",
      "2020-06-30T19:36:21.712327:Epoch   1 Batch 1129/6251 test_loss = 1.129\n",
      "2020-06-30T19:36:21.932830:Epoch   1 Batch 1149/6251 test_loss = 0.941\n",
      "2020-06-30T19:36:22.142379:Epoch   1 Batch 1169/6251 test_loss = 0.794\n",
      "2020-06-30T19:36:22.358618:Epoch   1 Batch 1189/6251 test_loss = 0.938\n",
      "2020-06-30T19:36:22.565977:Epoch   1 Batch 1209/6251 test_loss = 0.899\n",
      "2020-06-30T19:36:22.804166:Epoch   1 Batch 1229/6251 test_loss = 0.674\n",
      "2020-06-30T19:36:23.012472:Epoch   1 Batch 1249/6251 test_loss = 0.858\n",
      "2020-06-30T19:36:23.232830:Epoch   1 Batch 1269/6251 test_loss = 0.745\n",
      "2020-06-30T19:36:23.443072:Epoch   1 Batch 1289/6251 test_loss = 0.802\n",
      "2020-06-30T19:36:23.658279:Epoch   1 Batch 1309/6251 test_loss = 0.760\n",
      "2020-06-30T19:36:23.871980:Epoch   1 Batch 1329/6251 test_loss = 0.804\n",
      "2020-06-30T19:36:24.079028:Epoch   1 Batch 1349/6251 test_loss = 0.947\n",
      "2020-06-30T19:36:24.289788:Epoch   1 Batch 1369/6251 test_loss = 0.898\n",
      "2020-06-30T19:36:24.507920:Epoch   1 Batch 1389/6251 test_loss = 0.844\n",
      "2020-06-30T19:36:24.713388:Epoch   1 Batch 1409/6251 test_loss = 0.807\n",
      "2020-06-30T19:36:24.926959:Epoch   1 Batch 1429/6251 test_loss = 0.943\n",
      "2020-06-30T19:36:25.126931:Epoch   1 Batch 1449/6251 test_loss = 1.031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:36:25.338625:Epoch   1 Batch 1469/6251 test_loss = 0.779\n",
      "2020-06-30T19:36:25.542301:Epoch   1 Batch 1489/6251 test_loss = 0.850\n",
      "2020-06-30T19:36:25.759678:Epoch   1 Batch 1509/6251 test_loss = 1.253\n",
      "2020-06-30T19:36:25.985227:Epoch   1 Batch 1529/6251 test_loss = 1.081\n",
      "2020-06-30T19:36:26.206079:Epoch   1 Batch 1549/6251 test_loss = 0.775\n",
      "2020-06-30T19:36:27.716714 Batch   18/6251 train_loss = 0.741\n",
      "2020-06-30T19:36:27.937310 Batch   38/6251 train_loss = 0.923\n",
      "2020-06-30T19:36:28.160703 Batch   58/6251 train_loss = 0.832\n",
      "2020-06-30T19:36:28.382159 Batch   78/6251 train_loss = 0.839\n",
      "2020-06-30T19:36:28.607833 Batch   98/6251 train_loss = 0.927\n",
      "2020-06-30T19:36:28.829796 Batch  118/6251 train_loss = 0.730\n",
      "2020-06-30T19:36:29.074057 Batch  138/6251 train_loss = 0.951\n",
      "2020-06-30T19:36:29.310664 Batch  158/6251 train_loss = 0.782\n",
      "2020-06-30T19:36:29.535636 Batch  178/6251 train_loss = 0.960\n",
      "2020-06-30T19:36:29.751256 Batch  198/6251 train_loss = 1.187\n",
      "2020-06-30T19:36:29.970673 Batch  218/6251 train_loss = 0.695\n",
      "2020-06-30T19:36:30.187541 Batch  238/6251 train_loss = 0.810\n",
      "2020-06-30T19:36:30.408025 Batch  258/6251 train_loss = 1.043\n",
      "2020-06-30T19:36:30.633413 Batch  278/6251 train_loss = 1.050\n",
      "2020-06-30T19:36:30.855503 Batch  298/6251 train_loss = 0.819\n",
      "2020-06-30T19:36:31.086966 Batch  318/6251 train_loss = 0.658\n",
      "2020-06-30T19:36:31.318292 Batch  338/6251 train_loss = 0.879\n",
      "2020-06-30T19:36:31.548102 Batch  358/6251 train_loss = 0.936\n",
      "2020-06-30T19:36:31.775552 Batch  378/6251 train_loss = 0.804\n",
      "2020-06-30T19:36:32.000968 Batch  398/6251 train_loss = 0.911\n",
      "2020-06-30T19:36:32.214980 Batch  418/6251 train_loss = 0.660\n",
      "2020-06-30T19:36:32.428224 Batch  438/6251 train_loss = 0.981\n",
      "2020-06-30T19:36:32.639651 Batch  458/6251 train_loss = 0.857\n",
      "2020-06-30T19:36:32.856632 Batch  478/6251 train_loss = 0.854\n",
      "2020-06-30T19:36:33.071668 Batch  498/6251 train_loss = 0.777\n",
      "2020-06-30T19:36:33.295516 Batch  518/6251 train_loss = 0.749\n",
      "2020-06-30T19:36:33.519382 Batch  538/6251 train_loss = 1.008\n",
      "2020-06-30T19:36:33.733429 Batch  558/6251 train_loss = 0.881\n",
      "2020-06-30T19:36:33.949698 Batch  578/6251 train_loss = 0.817\n",
      "2020-06-30T19:36:34.168673 Batch  598/6251 train_loss = 1.024\n",
      "2020-06-30T19:36:34.389071 Batch  618/6251 train_loss = 0.752\n",
      "2020-06-30T19:36:34.611616 Batch  638/6251 train_loss = 0.771\n",
      "2020-06-30T19:36:34.841717 Batch  658/6251 train_loss = 0.944\n",
      "2020-06-30T19:36:35.058071 Batch  678/6251 train_loss = 0.736\n",
      "2020-06-30T19:36:35.272625 Batch  698/6251 train_loss = 1.000\n",
      "2020-06-30T19:36:35.489384 Batch  718/6251 train_loss = 0.761\n",
      "2020-06-30T19:36:35.706930 Batch  738/6251 train_loss = 0.906\n",
      "2020-06-30T19:36:35.925754 Batch  758/6251 train_loss = 1.004\n",
      "2020-06-30T19:36:36.142269 Batch  778/6251 train_loss = 0.626\n",
      "2020-06-30T19:36:36.367413 Batch  798/6251 train_loss = 0.984\n",
      "2020-06-30T19:36:36.586342 Batch  818/6251 train_loss = 0.838\n",
      "2020-06-30T19:36:36.813058 Batch  838/6251 train_loss = 0.895\n",
      "2020-06-30T19:36:37.042746 Batch  858/6251 train_loss = 0.836\n",
      "2020-06-30T19:36:37.266784 Batch  878/6251 train_loss = 1.063\n",
      "2020-06-30T19:36:37.492645 Batch  898/6251 train_loss = 0.674\n",
      "2020-06-30T19:36:37.724524 Batch  918/6251 train_loss = 0.957\n",
      "2020-06-30T19:36:37.947778 Batch  938/6251 train_loss = 0.724\n",
      "2020-06-30T19:36:38.168231 Batch  958/6251 train_loss = 0.876\n",
      "2020-06-30T19:36:38.393996 Batch  978/6251 train_loss = 0.797\n",
      "2020-06-30T19:36:38.612158 Batch  998/6251 train_loss = 0.729\n",
      "2020-06-30T19:36:38.830895 Batch 1018/6251 train_loss = 0.627\n",
      "2020-06-30T19:36:39.047145 Batch 1038/6251 train_loss = 0.800\n",
      "2020-06-30T19:36:39.266122 Batch 1058/6251 train_loss = 0.755\n",
      "2020-06-30T19:36:39.502360 Batch 1078/6251 train_loss = 0.964\n",
      "2020-06-30T19:36:39.713804 Batch 1098/6251 train_loss = 0.795\n",
      "2020-06-30T19:36:39.948520 Batch 1118/6251 train_loss = 0.993\n",
      "2020-06-30T19:36:40.166151 Batch 1138/6251 train_loss = 0.941\n",
      "2020-06-30T19:36:40.409228 Batch 1158/6251 train_loss = 0.733\n",
      "2020-06-30T19:36:40.631522 Batch 1178/6251 train_loss = 0.745\n",
      "2020-06-30T19:36:40.854022 Batch 1198/6251 train_loss = 0.724\n",
      "2020-06-30T19:36:41.069215 Batch 1218/6251 train_loss = 0.774\n",
      "2020-06-30T19:36:41.294502 Batch 1238/6251 train_loss = 0.642\n",
      "2020-06-30T19:36:41.519890 Batch 1258/6251 train_loss = 0.999\n",
      "2020-06-30T19:36:41.763664 Batch 1278/6251 train_loss = 1.073\n",
      "2020-06-30T19:36:41.984614 Batch 1298/6251 train_loss = 0.900\n",
      "2020-06-30T19:36:42.201921 Batch 1318/6251 train_loss = 1.047\n",
      "2020-06-30T19:36:42.426520 Batch 1338/6251 train_loss = 0.776\n",
      "2020-06-30T19:36:42.659240 Batch 1358/6251 train_loss = 1.104\n",
      "2020-06-30T19:36:42.873842 Batch 1378/6251 train_loss = 0.952\n",
      "2020-06-30T19:36:43.095326 Batch 1398/6251 train_loss = 0.808\n",
      "2020-06-30T19:36:43.329300 Batch 1418/6251 train_loss = 0.798\n",
      "2020-06-30T19:36:43.547429 Batch 1438/6251 train_loss = 0.930\n",
      "2020-06-30T19:36:43.765124 Batch 1458/6251 train_loss = 0.866\n",
      "2020-06-30T19:36:44.000862 Batch 1478/6251 train_loss = 0.740\n",
      "2020-06-30T19:36:44.220085 Batch 1498/6251 train_loss = 0.941\n",
      "2020-06-30T19:36:44.458484 Batch 1518/6251 train_loss = 0.805\n",
      "2020-06-30T19:36:44.676390 Batch 1538/6251 train_loss = 0.913\n",
      "2020-06-30T19:36:44.911963 Batch 1558/6251 train_loss = 0.854\n",
      "2020-06-30T19:36:45.142795 Batch 1578/6251 train_loss = 0.731\n",
      "2020-06-30T19:36:45.374032 Batch 1598/6251 train_loss = 0.935\n",
      "2020-06-30T19:36:45.603583 Batch 1618/6251 train_loss = 0.854\n",
      "2020-06-30T19:36:45.816368 Batch 1638/6251 train_loss = 0.788\n",
      "2020-06-30T19:36:46.042619 Batch 1658/6251 train_loss = 1.135\n",
      "2020-06-30T19:36:46.262904 Batch 1678/6251 train_loss = 0.856\n",
      "2020-06-30T19:36:46.495604 Batch 1698/6251 train_loss = 0.908\n",
      "2020-06-30T19:36:46.735457 Batch 1718/6251 train_loss = 0.878\n",
      "2020-06-30T19:36:46.974795 Batch 1738/6251 train_loss = 1.074\n",
      "2020-06-30T19:36:47.302726 Batch 1758/6251 train_loss = 0.592\n",
      "2020-06-30T19:36:47.683658 Batch 1778/6251 train_loss = 1.007\n",
      "2020-06-30T19:36:47.913788 Batch 1798/6251 train_loss = 0.932\n",
      "2020-06-30T19:36:48.283719 Batch 1818/6251 train_loss = 0.689\n",
      "2020-06-30T19:36:48.617422 Batch 1838/6251 train_loss = 0.813\n",
      "2020-06-30T19:36:48.864896 Batch 1858/6251 train_loss = 0.666\n",
      "2020-06-30T19:36:49.140958 Batch 1878/6251 train_loss = 0.844\n",
      "2020-06-30T19:36:49.350249 Batch 1898/6251 train_loss = 0.766\n",
      "2020-06-30T19:36:49.568188 Batch 1918/6251 train_loss = 1.145\n",
      "2020-06-30T19:36:49.785547 Batch 1938/6251 train_loss = 0.764\n",
      "2020-06-30T19:36:50.006156 Batch 1958/6251 train_loss = 0.942\n",
      "2020-06-30T19:36:50.230083 Batch 1978/6251 train_loss = 0.850\n",
      "2020-06-30T19:36:50.438731 Batch 1998/6251 train_loss = 0.847\n",
      "2020-06-30T19:36:50.672084 Batch 2018/6251 train_loss = 1.072\n",
      "2020-06-30T19:36:50.888018 Batch 2038/6251 train_loss = 0.956\n",
      "2020-06-30T19:36:51.099052 Batch 2058/6251 train_loss = 0.870\n",
      "2020-06-30T19:36:51.316813 Batch 2078/6251 train_loss = 1.000\n",
      "2020-06-30T19:36:51.536472 Batch 2098/6251 train_loss = 0.925\n",
      "2020-06-30T19:36:51.742501 Batch 2118/6251 train_loss = 0.863\n",
      "2020-06-30T19:36:51.954443 Batch 2138/6251 train_loss = 0.951\n",
      "2020-06-30T19:36:52.156365 Batch 2158/6251 train_loss = 0.890\n",
      "2020-06-30T19:36:52.360832 Batch 2178/6251 train_loss = 0.873\n",
      "2020-06-30T19:36:52.590867 Batch 2198/6251 train_loss = 0.754\n",
      "2020-06-30T19:36:52.794330 Batch 2218/6251 train_loss = 0.706\n",
      "2020-06-30T19:36:53.011755 Batch 2238/6251 train_loss = 0.841\n",
      "2020-06-30T19:36:53.218166 Batch 2258/6251 train_loss = 0.765\n",
      "2020-06-30T19:36:53.419181 Batch 2278/6251 train_loss = 0.772\n",
      "2020-06-30T19:36:53.632383 Batch 2298/6251 train_loss = 0.781\n",
      "2020-06-30T19:36:53.841068 Batch 2318/6251 train_loss = 1.042\n",
      "2020-06-30T19:36:54.043224 Batch 2338/6251 train_loss = 0.721\n",
      "2020-06-30T19:36:54.242885 Batch 2358/6251 train_loss = 0.806\n",
      "2020-06-30T19:36:54.444821 Batch 2378/6251 train_loss = 0.824\n",
      "2020-06-30T19:36:54.645171 Batch 2398/6251 train_loss = 0.925\n",
      "2020-06-30T19:36:54.840242 Batch 2418/6251 train_loss = 0.732\n",
      "2020-06-30T19:36:55.053493 Batch 2438/6251 train_loss = 1.025\n",
      "2020-06-30T19:36:55.252937 Batch 2458/6251 train_loss = 0.697\n",
      "2020-06-30T19:36:55.451320 Batch 2478/6251 train_loss = 0.878\n",
      "2020-06-30T19:36:55.652550 Batch 2498/6251 train_loss = 0.984\n",
      "2020-06-30T19:36:55.852746 Batch 2518/6251 train_loss = 0.883\n",
      "2020-06-30T19:36:56.050376 Batch 2538/6251 train_loss = 0.822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:36:56.250268 Batch 2558/6251 train_loss = 1.149\n",
      "2020-06-30T19:36:56.446985 Batch 2578/6251 train_loss = 1.009\n",
      "2020-06-30T19:36:56.649641 Batch 2598/6251 train_loss = 0.770\n",
      "2020-06-30T19:36:56.848393 Batch 2618/6251 train_loss = 0.881\n",
      "2020-06-30T19:36:57.052058 Batch 2638/6251 train_loss = 0.866\n",
      "2020-06-30T19:36:57.254183 Batch 2658/6251 train_loss = 0.677\n",
      "2020-06-30T19:36:57.455474 Batch 2678/6251 train_loss = 0.728\n",
      "2020-06-30T19:36:57.659716 Batch 2698/6251 train_loss = 0.954\n",
      "2020-06-30T19:36:57.868991 Batch 2718/6251 train_loss = 0.918\n",
      "2020-06-30T19:36:58.068454 Batch 2738/6251 train_loss = 0.802\n",
      "2020-06-30T19:36:58.277993 Batch 2758/6251 train_loss = 0.922\n",
      "2020-06-30T19:36:58.473920 Batch 2778/6251 train_loss = 0.777\n",
      "2020-06-30T19:36:58.687526 Batch 2798/6251 train_loss = 0.676\n",
      "2020-06-30T19:36:58.896500 Batch 2818/6251 train_loss = 0.964\n",
      "2020-06-30T19:36:59.102888 Batch 2838/6251 train_loss = 0.788\n",
      "2020-06-30T19:36:59.447673 Batch 2858/6251 train_loss = 0.938\n",
      "2020-06-30T19:36:59.775425 Batch 2878/6251 train_loss = 1.040\n",
      "2020-06-30T19:36:59.990957 Batch 2898/6251 train_loss = 0.931\n",
      "2020-06-30T19:37:00.373217 Batch 2918/6251 train_loss = 0.677\n",
      "2020-06-30T19:37:00.680301 Batch 2938/6251 train_loss = 0.936\n",
      "2020-06-30T19:37:00.918807 Batch 2958/6251 train_loss = 0.794\n",
      "2020-06-30T19:37:01.258837 Batch 2978/6251 train_loss = 0.878\n",
      "2020-06-30T19:37:01.549641 Batch 2998/6251 train_loss = 0.788\n",
      "2020-06-30T19:37:01.826527 Batch 3018/6251 train_loss = 0.522\n",
      "2020-06-30T19:37:02.071051 Batch 3038/6251 train_loss = 0.657\n",
      "2020-06-30T19:37:02.315304 Batch 3058/6251 train_loss = 0.892\n",
      "2020-06-30T19:37:02.592083 Batch 3078/6251 train_loss = 0.714\n",
      "2020-06-30T19:37:02.828722 Batch 3098/6251 train_loss = 1.038\n",
      "2020-06-30T19:37:03.051858 Batch 3118/6251 train_loss = 0.775\n",
      "2020-06-30T19:37:03.276751 Batch 3138/6251 train_loss = 0.946\n",
      "2020-06-30T19:37:03.499023 Batch 3158/6251 train_loss = 0.727\n",
      "2020-06-30T19:37:03.727676 Batch 3178/6251 train_loss = 0.649\n",
      "2020-06-30T19:37:03.958805 Batch 3198/6251 train_loss = 0.666\n",
      "2020-06-30T19:37:04.187819 Batch 3218/6251 train_loss = 0.959\n",
      "2020-06-30T19:37:04.409524 Batch 3238/6251 train_loss = 0.824\n",
      "2020-06-30T19:37:04.638933 Batch 3258/6251 train_loss = 0.727\n",
      "2020-06-30T19:37:04.865024 Batch 3278/6251 train_loss = 0.733\n",
      "2020-06-30T19:37:05.086671 Batch 3298/6251 train_loss = 0.871\n",
      "2020-06-30T19:37:05.305830 Batch 3318/6251 train_loss = 0.778\n",
      "2020-06-30T19:37:05.525806 Batch 3338/6251 train_loss = 0.891\n",
      "2020-06-30T19:37:05.752927 Batch 3358/6251 train_loss = 0.721\n",
      "2020-06-30T19:37:05.985233 Batch 3378/6251 train_loss = 0.896\n",
      "2020-06-30T19:37:06.210349 Batch 3398/6251 train_loss = 0.907\n",
      "2020-06-30T19:37:06.445792 Batch 3418/6251 train_loss = 0.637\n",
      "2020-06-30T19:37:06.666335 Batch 3438/6251 train_loss = 0.835\n",
      "2020-06-30T19:37:06.887465 Batch 3458/6251 train_loss = 0.753\n",
      "2020-06-30T19:37:07.112486 Batch 3478/6251 train_loss = 0.768\n",
      "2020-06-30T19:37:07.335349 Batch 3498/6251 train_loss = 0.884\n",
      "2020-06-30T19:37:07.569530 Batch 3518/6251 train_loss = 1.071\n",
      "2020-06-30T19:37:07.785758 Batch 3538/6251 train_loss = 0.782\n",
      "2020-06-30T19:37:08.004566 Batch 3558/6251 train_loss = 0.893\n",
      "2020-06-30T19:37:08.230183 Batch 3578/6251 train_loss = 0.809\n",
      "2020-06-30T19:37:08.451846 Batch 3598/6251 train_loss = 0.900\n",
      "2020-06-30T19:37:08.669705 Batch 3618/6251 train_loss = 0.810\n",
      "2020-06-30T19:37:08.878632 Batch 3638/6251 train_loss = 0.624\n",
      "2020-06-30T19:37:09.091802 Batch 3658/6251 train_loss = 1.017\n",
      "2020-06-30T19:37:09.304662 Batch 3678/6251 train_loss = 0.821\n",
      "2020-06-30T19:37:09.509453 Batch 3698/6251 train_loss = 0.719\n",
      "2020-06-30T19:37:09.716388 Batch 3718/6251 train_loss = 1.000\n",
      "2020-06-30T19:37:09.924593 Batch 3738/6251 train_loss = 0.937\n",
      "2020-06-30T19:37:10.139212 Batch 3758/6251 train_loss = 0.747\n",
      "2020-06-30T19:37:10.351283 Batch 3778/6251 train_loss = 0.869\n",
      "2020-06-30T19:37:10.558578 Batch 3798/6251 train_loss = 0.910\n",
      "2020-06-30T19:37:10.758537 Batch 3818/6251 train_loss = 0.805\n",
      "2020-06-30T19:37:10.963351 Batch 3838/6251 train_loss = 0.789\n",
      "2020-06-30T19:37:11.168279 Batch 3858/6251 train_loss = 0.681\n",
      "2020-06-30T19:37:11.387409 Batch 3878/6251 train_loss = 0.968\n",
      "2020-06-30T19:37:11.590795 Batch 3898/6251 train_loss = 0.836\n",
      "2020-06-30T19:37:11.791390 Batch 3918/6251 train_loss = 0.919\n",
      "2020-06-30T19:37:11.993496 Batch 3938/6251 train_loss = 0.748\n",
      "2020-06-30T19:37:12.200576 Batch 3958/6251 train_loss = 0.754\n",
      "2020-06-30T19:37:12.399852 Batch 3978/6251 train_loss = 0.728\n",
      "2020-06-30T19:37:12.613716 Batch 3998/6251 train_loss = 0.912\n",
      "2020-06-30T19:37:12.814311 Batch 4018/6251 train_loss = 0.890\n",
      "2020-06-30T19:37:13.025928 Batch 4038/6251 train_loss = 1.914\n",
      "2020-06-30T19:37:13.249232 Batch 4058/6251 train_loss = 0.999\n",
      "2020-06-30T19:37:13.474371 Batch 4078/6251 train_loss = 0.994\n",
      "2020-06-30T19:37:13.691997 Batch 4098/6251 train_loss = 0.719\n",
      "2020-06-30T19:37:13.905038 Batch 4118/6251 train_loss = 0.913\n",
      "2020-06-30T19:37:14.120554 Batch 4138/6251 train_loss = 0.797\n",
      "2020-06-30T19:37:14.360154 Batch 4158/6251 train_loss = 0.902\n",
      "2020-06-30T19:37:14.590634 Batch 4178/6251 train_loss = 0.672\n",
      "2020-06-30T19:37:14.804577 Batch 4198/6251 train_loss = 0.806\n",
      "2020-06-30T19:37:15.019656 Batch 4218/6251 train_loss = 0.827\n",
      "2020-06-30T19:37:15.235330 Batch 4238/6251 train_loss = 0.931\n",
      "2020-06-30T19:37:15.461194 Batch 4258/6251 train_loss = 0.623\n",
      "2020-06-30T19:37:15.686996 Batch 4278/6251 train_loss = 1.055\n",
      "2020-06-30T19:37:15.903086 Batch 4298/6251 train_loss = 0.829\n",
      "2020-06-30T19:37:16.122919 Batch 4318/6251 train_loss = 0.688\n",
      "2020-06-30T19:37:16.337257 Batch 4338/6251 train_loss = 0.686\n",
      "2020-06-30T19:37:16.558313 Batch 4358/6251 train_loss = 0.845\n",
      "2020-06-30T19:37:16.779464 Batch 4378/6251 train_loss = 0.959\n",
      "2020-06-30T19:37:17.000152 Batch 4398/6251 train_loss = 0.657\n",
      "2020-06-30T19:37:17.220073 Batch 4418/6251 train_loss = 0.870\n",
      "2020-06-30T19:37:17.438697 Batch 4438/6251 train_loss = 1.096\n",
      "2020-06-30T19:37:17.752512 Batch 4458/6251 train_loss = 0.617\n",
      "2020-06-30T19:37:17.977145 Batch 4478/6251 train_loss = 0.555\n",
      "2020-06-30T19:37:18.214513 Batch 4498/6251 train_loss = 0.792\n",
      "2020-06-30T19:37:18.432323 Batch 4518/6251 train_loss = 0.856\n",
      "2020-06-30T19:37:18.649031 Batch 4538/6251 train_loss = 0.900\n",
      "2020-06-30T19:37:18.859500 Batch 4558/6251 train_loss = 0.778\n",
      "2020-06-30T19:37:19.078605 Batch 4578/6251 train_loss = 0.992\n",
      "2020-06-30T19:37:19.286124 Batch 4598/6251 train_loss = 0.909\n",
      "2020-06-30T19:37:19.492271 Batch 4618/6251 train_loss = 1.003\n",
      "2020-06-30T19:37:19.708981 Batch 4638/6251 train_loss = 0.833\n",
      "2020-06-30T19:37:19.911676 Batch 4658/6251 train_loss = 0.963\n",
      "2020-06-30T19:37:20.123286 Batch 4678/6251 train_loss = 0.709\n",
      "2020-06-30T19:37:20.330747 Batch 4698/6251 train_loss = 0.882\n",
      "2020-06-30T19:37:20.541267 Batch 4718/6251 train_loss = 0.924\n",
      "2020-06-30T19:37:20.755545 Batch 4738/6251 train_loss = 0.852\n",
      "2020-06-30T19:37:20.965592 Batch 4758/6251 train_loss = 0.993\n",
      "2020-06-30T19:37:21.193658 Batch 4778/6251 train_loss = 0.968\n",
      "2020-06-30T19:37:21.402860 Batch 4798/6251 train_loss = 0.705\n",
      "2020-06-30T19:37:21.617809 Batch 4818/6251 train_loss = 0.783\n",
      "2020-06-30T19:37:21.844833 Batch 4838/6251 train_loss = 0.767\n",
      "2020-06-30T19:37:22.071786 Batch 4858/6251 train_loss = 0.905\n",
      "2020-06-30T19:37:22.296207 Batch 4878/6251 train_loss = 0.985\n",
      "2020-06-30T19:37:22.519942 Batch 4898/6251 train_loss = 0.773\n",
      "2020-06-30T19:37:22.744175 Batch 4918/6251 train_loss = 0.833\n",
      "2020-06-30T19:37:22.960339 Batch 4938/6251 train_loss = 0.948\n",
      "2020-06-30T19:37:23.176587 Batch 4958/6251 train_loss = 1.004\n",
      "2020-06-30T19:37:23.397218 Batch 4978/6251 train_loss = 0.868\n",
      "2020-06-30T19:37:23.608334 Batch 4998/6251 train_loss = 0.947\n",
      "2020-06-30T19:37:23.822483 Batch 5018/6251 train_loss = 0.777\n",
      "2020-06-30T19:37:24.035643 Batch 5038/6251 train_loss = 0.771\n",
      "2020-06-30T19:37:24.246645 Batch 5058/6251 train_loss = 0.990\n",
      "2020-06-30T19:37:24.458645 Batch 5078/6251 train_loss = 0.882\n",
      "2020-06-30T19:37:24.702617 Batch 5098/6251 train_loss = 0.662\n",
      "2020-06-30T19:37:24.940110 Batch 5118/6251 train_loss = 0.974\n",
      "2020-06-30T19:37:25.302654 Batch 5138/6251 train_loss = 0.906\n",
      "2020-06-30T19:37:25.595521 Batch 5158/6251 train_loss = 0.766\n",
      "2020-06-30T19:37:25.882321 Batch 5178/6251 train_loss = 0.817\n",
      "2020-06-30T19:37:26.156192 Batch 5198/6251 train_loss = 0.769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:37:26.409422 Batch 5218/6251 train_loss = 0.921\n",
      "2020-06-30T19:37:26.666380 Batch 5238/6251 train_loss = 0.887\n",
      "2020-06-30T19:37:26.982262 Batch 5258/6251 train_loss = 0.794\n",
      "2020-06-30T19:37:27.317709 Batch 5278/6251 train_loss = 0.739\n",
      "2020-06-30T19:37:27.590267 Batch 5298/6251 train_loss = 0.968\n",
      "2020-06-30T19:37:27.841709 Batch 5318/6251 train_loss = 0.664\n",
      "2020-06-30T19:37:28.080470 Batch 5338/6251 train_loss = 0.732\n",
      "2020-06-30T19:37:28.313199 Batch 5358/6251 train_loss = 0.733\n",
      "2020-06-30T19:37:28.546895 Batch 5378/6251 train_loss = 0.813\n",
      "2020-06-30T19:37:28.777138 Batch 5398/6251 train_loss = 0.704\n",
      "2020-06-30T19:37:28.994083 Batch 5418/6251 train_loss = 0.762\n",
      "2020-06-30T19:37:29.215961 Batch 5438/6251 train_loss = 0.905\n",
      "2020-06-30T19:37:29.454113 Batch 5458/6251 train_loss = 1.080\n",
      "2020-06-30T19:37:29.676806 Batch 5478/6251 train_loss = 0.914\n",
      "2020-06-30T19:37:29.894620 Batch 5498/6251 train_loss = 1.004\n",
      "2020-06-30T19:37:30.106172 Batch 5518/6251 train_loss = 0.979\n",
      "2020-06-30T19:37:30.329326 Batch 5538/6251 train_loss = 0.776\n",
      "2020-06-30T19:37:30.543086 Batch 5558/6251 train_loss = 0.824\n",
      "2020-06-30T19:37:30.768788 Batch 5578/6251 train_loss = 0.829\n",
      "2020-06-30T19:37:30.992162 Batch 5598/6251 train_loss = 0.971\n",
      "2020-06-30T19:37:31.203373 Batch 5618/6251 train_loss = 0.909\n",
      "2020-06-30T19:37:31.422103 Batch 5638/6251 train_loss = 0.969\n",
      "2020-06-30T19:37:31.628490 Batch 5658/6251 train_loss = 0.610\n",
      "2020-06-30T19:37:31.848645 Batch 5678/6251 train_loss = 0.875\n",
      "2020-06-30T19:37:32.056446 Batch 5698/6251 train_loss = 0.895\n",
      "2020-06-30T19:37:32.283864 Batch 5718/6251 train_loss = 0.759\n",
      "2020-06-30T19:37:32.560085 Batch 5738/6251 train_loss = 0.792\n",
      "2020-06-30T19:37:32.783779 Batch 5758/6251 train_loss = 0.655\n",
      "2020-06-30T19:37:32.997411 Batch 5778/6251 train_loss = 0.783\n",
      "2020-06-30T19:37:33.200797 Batch 5798/6251 train_loss = 0.884\n",
      "2020-06-30T19:37:33.410884 Batch 5818/6251 train_loss = 0.957\n",
      "2020-06-30T19:37:33.635647 Batch 5838/6251 train_loss = 0.830\n",
      "2020-06-30T19:37:33.846037 Batch 5858/6251 train_loss = 0.678\n",
      "2020-06-30T19:37:34.056073 Batch 5878/6251 train_loss = 0.800\n",
      "2020-06-30T19:37:34.288796 Batch 5898/6251 train_loss = 0.922\n",
      "2020-06-30T19:37:34.514024 Batch 5918/6251 train_loss = 0.725\n",
      "2020-06-30T19:37:34.745237 Batch 5938/6251 train_loss = 0.785\n",
      "2020-06-30T19:37:34.978486 Batch 5958/6251 train_loss = 1.075\n",
      "2020-06-30T19:37:35.197927 Batch 5978/6251 train_loss = 0.930\n",
      "2020-06-30T19:37:35.423247 Batch 5998/6251 train_loss = 0.837\n",
      "2020-06-30T19:37:35.663260 Batch 6018/6251 train_loss = 0.818\n",
      "2020-06-30T19:37:35.901212 Batch 6038/6251 train_loss = 0.892\n",
      "2020-06-30T19:37:36.120132 Batch 6058/6251 train_loss = 0.955\n",
      "2020-06-30T19:37:36.339116 Batch 6078/6251 train_loss = 0.800\n",
      "2020-06-30T19:37:36.569343 Batch 6098/6251 train_loss = 0.844\n",
      "2020-06-30T19:37:36.789232 Batch 6118/6251 train_loss = 0.876\n",
      "2020-06-30T19:37:37.023929 Batch 6138/6251 train_loss = 0.639\n",
      "2020-06-30T19:37:37.255461 Batch 6158/6251 train_loss = 0.787\n",
      "2020-06-30T19:37:37.512609 Batch 6178/6251 train_loss = 0.732\n",
      "2020-06-30T19:37:37.752591 Batch 6198/6251 train_loss = 0.962\n",
      "2020-06-30T19:37:37.977287 Batch 6218/6251 train_loss = 0.772\n",
      "2020-06-30T19:37:38.198235 Batch 6238/6251 train_loss = 0.871\n",
      "2020-06-30T19:37:38.537239:Epoch   2 Batch   18/6251 test_loss = 0.806\n",
      "2020-06-30T19:37:38.792456:Epoch   2 Batch   38/6251 test_loss = 0.936\n",
      "2020-06-30T19:37:39.010959:Epoch   2 Batch   58/6251 test_loss = 0.887\n",
      "2020-06-30T19:37:39.233719:Epoch   2 Batch   78/6251 test_loss = 0.717\n",
      "2020-06-30T19:37:39.457010:Epoch   2 Batch   98/6251 test_loss = 0.801\n",
      "2020-06-30T19:37:39.678445:Epoch   2 Batch  118/6251 test_loss = 0.849\n",
      "2020-06-30T19:37:39.923627:Epoch   2 Batch  138/6251 test_loss = 0.974\n",
      "2020-06-30T19:37:40.149148:Epoch   2 Batch  158/6251 test_loss = 0.845\n",
      "2020-06-30T19:37:40.367417:Epoch   2 Batch  178/6251 test_loss = 0.930\n",
      "2020-06-30T19:37:40.577720:Epoch   2 Batch  198/6251 test_loss = 0.770\n",
      "2020-06-30T19:37:40.814906:Epoch   2 Batch  218/6251 test_loss = 0.673\n",
      "2020-06-30T19:37:41.060199:Epoch   2 Batch  238/6251 test_loss = 0.823\n",
      "2020-06-30T19:37:41.270872:Epoch   2 Batch  258/6251 test_loss = 0.747\n",
      "2020-06-30T19:37:41.532879:Epoch   2 Batch  278/6251 test_loss = 0.893\n",
      "2020-06-30T19:37:41.770227:Epoch   2 Batch  298/6251 test_loss = 0.801\n",
      "2020-06-30T19:37:41.995517:Epoch   2 Batch  318/6251 test_loss = 0.848\n",
      "2020-06-30T19:37:42.217052:Epoch   2 Batch  338/6251 test_loss = 1.019\n",
      "2020-06-30T19:37:42.436381:Epoch   2 Batch  358/6251 test_loss = 0.902\n",
      "2020-06-30T19:37:42.652627:Epoch   2 Batch  378/6251 test_loss = 0.964\n",
      "2020-06-30T19:37:42.867941:Epoch   2 Batch  398/6251 test_loss = 0.821\n",
      "2020-06-30T19:37:43.077732:Epoch   2 Batch  418/6251 test_loss = 0.735\n",
      "2020-06-30T19:37:43.292456:Epoch   2 Batch  438/6251 test_loss = 0.810\n",
      "2020-06-30T19:37:43.505416:Epoch   2 Batch  458/6251 test_loss = 0.855\n",
      "2020-06-30T19:37:43.742071:Epoch   2 Batch  478/6251 test_loss = 1.086\n",
      "2020-06-30T19:37:43.949156:Epoch   2 Batch  498/6251 test_loss = 0.816\n",
      "2020-06-30T19:37:44.181139:Epoch   2 Batch  518/6251 test_loss = 0.934\n",
      "2020-06-30T19:37:44.401232:Epoch   2 Batch  538/6251 test_loss = 0.796\n",
      "2020-06-30T19:37:44.613705:Epoch   2 Batch  558/6251 test_loss = 0.874\n",
      "2020-06-30T19:37:44.846765:Epoch   2 Batch  578/6251 test_loss = 0.856\n",
      "2020-06-30T19:37:45.080296:Epoch   2 Batch  598/6251 test_loss = 1.082\n",
      "2020-06-30T19:37:45.290593:Epoch   2 Batch  618/6251 test_loss = 1.239\n",
      "2020-06-30T19:37:45.503203:Epoch   2 Batch  638/6251 test_loss = 0.868\n",
      "2020-06-30T19:37:45.718111:Epoch   2 Batch  658/6251 test_loss = 0.768\n",
      "2020-06-30T19:37:45.945614:Epoch   2 Batch  678/6251 test_loss = 0.712\n",
      "2020-06-30T19:37:46.165887:Epoch   2 Batch  698/6251 test_loss = 0.915\n",
      "2020-06-30T19:37:46.386094:Epoch   2 Batch  718/6251 test_loss = 0.932\n",
      "2020-06-30T19:37:46.607355:Epoch   2 Batch  738/6251 test_loss = 0.633\n",
      "2020-06-30T19:37:46.827229:Epoch   2 Batch  758/6251 test_loss = 0.862\n",
      "2020-06-30T19:37:47.040536:Epoch   2 Batch  778/6251 test_loss = 1.006\n",
      "2020-06-30T19:37:47.255149:Epoch   2 Batch  798/6251 test_loss = 0.548\n",
      "2020-06-30T19:37:47.470655:Epoch   2 Batch  818/6251 test_loss = 0.913\n",
      "2020-06-30T19:37:47.678892:Epoch   2 Batch  838/6251 test_loss = 0.836\n",
      "2020-06-30T19:37:47.894533:Epoch   2 Batch  858/6251 test_loss = 0.797\n",
      "2020-06-30T19:37:48.103990:Epoch   2 Batch  878/6251 test_loss = 0.885\n",
      "2020-06-30T19:37:48.325436:Epoch   2 Batch  898/6251 test_loss = 0.721\n",
      "2020-06-30T19:37:48.577487:Epoch   2 Batch  918/6251 test_loss = 1.033\n",
      "2020-06-30T19:37:48.807638:Epoch   2 Batch  938/6251 test_loss = 0.878\n",
      "2020-06-30T19:37:49.039488:Epoch   2 Batch  958/6251 test_loss = 0.822\n",
      "2020-06-30T19:37:49.261514:Epoch   2 Batch  978/6251 test_loss = 0.803\n",
      "2020-06-30T19:37:49.465627:Epoch   2 Batch  998/6251 test_loss = 0.928\n",
      "2020-06-30T19:37:49.684218:Epoch   2 Batch 1018/6251 test_loss = 0.772\n",
      "2020-06-30T19:37:49.894738:Epoch   2 Batch 1038/6251 test_loss = 0.962\n",
      "2020-06-30T19:37:50.138715:Epoch   2 Batch 1058/6251 test_loss = 0.942\n",
      "2020-06-30T19:37:50.342893:Epoch   2 Batch 1078/6251 test_loss = 0.782\n",
      "2020-06-30T19:37:50.563513:Epoch   2 Batch 1098/6251 test_loss = 1.025\n",
      "2020-06-30T19:37:50.782625:Epoch   2 Batch 1118/6251 test_loss = 1.040\n",
      "2020-06-30T19:37:50.996605:Epoch   2 Batch 1138/6251 test_loss = 0.863\n",
      "2020-06-30T19:37:51.217954:Epoch   2 Batch 1158/6251 test_loss = 0.818\n",
      "2020-06-30T19:37:51.435392:Epoch   2 Batch 1178/6251 test_loss = 0.813\n",
      "2020-06-30T19:37:51.669145:Epoch   2 Batch 1198/6251 test_loss = 0.754\n",
      "2020-06-30T19:37:51.928378:Epoch   2 Batch 1218/6251 test_loss = 0.774\n",
      "2020-06-30T19:37:52.154552:Epoch   2 Batch 1238/6251 test_loss = 1.024\n",
      "2020-06-30T19:37:52.376547:Epoch   2 Batch 1258/6251 test_loss = 0.667\n",
      "2020-06-30T19:37:52.596533:Epoch   2 Batch 1278/6251 test_loss = 0.751\n",
      "2020-06-30T19:37:52.847358:Epoch   2 Batch 1298/6251 test_loss = 0.763\n",
      "2020-06-30T19:37:53.107976:Epoch   2 Batch 1318/6251 test_loss = 0.887\n",
      "2020-06-30T19:37:53.353085:Epoch   2 Batch 1338/6251 test_loss = 0.856\n",
      "2020-06-30T19:37:53.603954:Epoch   2 Batch 1358/6251 test_loss = 0.927\n",
      "2020-06-30T19:37:53.828908:Epoch   2 Batch 1378/6251 test_loss = 0.986\n",
      "2020-06-30T19:37:54.052580:Epoch   2 Batch 1398/6251 test_loss = 0.891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:37:54.278383:Epoch   2 Batch 1418/6251 test_loss = 0.850\n",
      "2020-06-30T19:37:54.518663:Epoch   2 Batch 1438/6251 test_loss = 0.764\n",
      "2020-06-30T19:37:54.745813:Epoch   2 Batch 1458/6251 test_loss = 0.699\n",
      "2020-06-30T19:37:54.973226:Epoch   2 Batch 1478/6251 test_loss = 0.950\n",
      "2020-06-30T19:37:55.188791:Epoch   2 Batch 1498/6251 test_loss = 0.989\n",
      "2020-06-30T19:37:55.416075:Epoch   2 Batch 1518/6251 test_loss = 0.731\n",
      "2020-06-30T19:37:55.643421:Epoch   2 Batch 1538/6251 test_loss = 0.608\n",
      "2020-06-30T19:37:55.865609:Epoch   2 Batch 1558/6251 test_loss = 0.745\n",
      "2020-06-30T19:37:57.117436 Batch    7/6251 train_loss = 0.855\n",
      "2020-06-30T19:37:57.349544 Batch   27/6251 train_loss = 0.843\n",
      "2020-06-30T19:37:57.573377 Batch   47/6251 train_loss = 0.809\n",
      "2020-06-30T19:37:57.793992 Batch   67/6251 train_loss = 1.001\n",
      "2020-06-30T19:37:58.008642 Batch   87/6251 train_loss = 0.899\n",
      "2020-06-30T19:37:58.230160 Batch  107/6251 train_loss = 0.741\n",
      "2020-06-30T19:37:58.454472 Batch  127/6251 train_loss = 0.593\n",
      "2020-06-30T19:37:58.700199 Batch  147/6251 train_loss = 0.719\n",
      "2020-06-30T19:37:58.960389 Batch  167/6251 train_loss = 0.685\n",
      "2020-06-30T19:37:59.290982 Batch  187/6251 train_loss = 0.948\n",
      "2020-06-30T19:37:59.541426 Batch  207/6251 train_loss = 0.684\n",
      "2020-06-30T19:37:59.766122 Batch  227/6251 train_loss = 0.843\n",
      "2020-06-30T19:37:59.990680 Batch  247/6251 train_loss = 0.891\n",
      "2020-06-30T19:38:00.234896 Batch  267/6251 train_loss = 0.775\n",
      "2020-06-30T19:38:00.470902 Batch  287/6251 train_loss = 0.806\n",
      "2020-06-30T19:38:00.697395 Batch  307/6251 train_loss = 0.946\n",
      "2020-06-30T19:38:00.923598 Batch  327/6251 train_loss = 0.912\n",
      "2020-06-30T19:38:01.151724 Batch  347/6251 train_loss = 0.782\n",
      "2020-06-30T19:38:01.375140 Batch  367/6251 train_loss = 0.798\n",
      "2020-06-30T19:38:01.652688 Batch  387/6251 train_loss = 0.815\n",
      "2020-06-30T19:38:01.943219 Batch  407/6251 train_loss = 0.959\n",
      "2020-06-30T19:38:02.174316 Batch  427/6251 train_loss = 0.825\n",
      "2020-06-30T19:38:02.441821 Batch  447/6251 train_loss = 0.880\n",
      "2020-06-30T19:38:02.688897 Batch  467/6251 train_loss = 0.805\n",
      "2020-06-30T19:38:02.932035 Batch  487/6251 train_loss = 1.027\n",
      "2020-06-30T19:38:03.154524 Batch  507/6251 train_loss = 1.002\n",
      "2020-06-30T19:38:03.378726 Batch  527/6251 train_loss = 0.846\n",
      "2020-06-30T19:38:03.600914 Batch  547/6251 train_loss = 0.817\n",
      "2020-06-30T19:38:03.838150 Batch  567/6251 train_loss = 0.879\n",
      "2020-06-30T19:38:04.054090 Batch  587/6251 train_loss = 0.953\n",
      "2020-06-30T19:38:04.267445 Batch  607/6251 train_loss = 0.700\n",
      "2020-06-30T19:38:04.495047 Batch  627/6251 train_loss = 0.953\n",
      "2020-06-30T19:38:04.760676 Batch  647/6251 train_loss = 0.877\n",
      "2020-06-30T19:38:05.031277 Batch  667/6251 train_loss = 0.797\n",
      "2020-06-30T19:38:05.260137 Batch  687/6251 train_loss = 0.889\n",
      "2020-06-30T19:38:05.493755 Batch  707/6251 train_loss = 0.810\n",
      "2020-06-30T19:38:05.752032 Batch  727/6251 train_loss = 0.938\n",
      "2020-06-30T19:38:05.972690 Batch  747/6251 train_loss = 0.888\n",
      "2020-06-30T19:38:06.195285 Batch  767/6251 train_loss = 0.845\n",
      "2020-06-30T19:38:06.419450 Batch  787/6251 train_loss = 0.955\n",
      "2020-06-30T19:38:06.661112 Batch  807/6251 train_loss = 0.854\n",
      "2020-06-30T19:38:06.885827 Batch  827/6251 train_loss = 0.783\n",
      "2020-06-30T19:38:07.106176 Batch  847/6251 train_loss = 0.804\n",
      "2020-06-30T19:38:07.319761 Batch  867/6251 train_loss = 0.835\n",
      "2020-06-30T19:38:07.577036 Batch  887/6251 train_loss = 0.952\n",
      "2020-06-30T19:38:07.822681 Batch  907/6251 train_loss = 0.846\n",
      "2020-06-30T19:38:08.030348 Batch  927/6251 train_loss = 0.934\n",
      "2020-06-30T19:38:08.250891 Batch  947/6251 train_loss = 0.839\n",
      "2020-06-30T19:38:08.461840 Batch  967/6251 train_loss = 0.846\n",
      "2020-06-30T19:38:08.689290 Batch  987/6251 train_loss = 0.706\n",
      "2020-06-30T19:38:08.913758 Batch 1007/6251 train_loss = 0.723\n",
      "2020-06-30T19:38:09.132140 Batch 1027/6251 train_loss = 0.809\n",
      "2020-06-30T19:38:09.359089 Batch 1047/6251 train_loss = 0.829\n",
      "2020-06-30T19:38:09.587531 Batch 1067/6251 train_loss = 0.832\n",
      "2020-06-30T19:38:09.818620 Batch 1087/6251 train_loss = 1.046\n",
      "2020-06-30T19:38:10.046906 Batch 1107/6251 train_loss = 0.925\n",
      "2020-06-30T19:38:10.269825 Batch 1127/6251 train_loss = 0.848\n",
      "2020-06-30T19:38:10.493615 Batch 1147/6251 train_loss = 0.667\n",
      "2020-06-30T19:38:10.718966 Batch 1167/6251 train_loss = 1.042\n",
      "2020-06-30T19:38:10.939617 Batch 1187/6251 train_loss = 0.841\n",
      "2020-06-30T19:38:11.155919 Batch 1207/6251 train_loss = 0.800\n",
      "2020-06-30T19:38:11.376539 Batch 1227/6251 train_loss = 0.912\n",
      "2020-06-30T19:38:11.601770 Batch 1247/6251 train_loss = 0.716\n",
      "2020-06-30T19:38:11.812009 Batch 1267/6251 train_loss = 0.731\n",
      "2020-06-30T19:38:12.026353 Batch 1287/6251 train_loss = 0.759\n",
      "2020-06-30T19:38:12.243026 Batch 1307/6251 train_loss = 0.782\n",
      "2020-06-30T19:38:12.500317 Batch 1327/6251 train_loss = 0.656\n",
      "2020-06-30T19:38:12.756032 Batch 1347/6251 train_loss = 0.901\n",
      "2020-06-30T19:38:12.973222 Batch 1367/6251 train_loss = 0.953\n",
      "2020-06-30T19:38:13.188692 Batch 1387/6251 train_loss = 1.076\n",
      "2020-06-30T19:38:13.403501 Batch 1407/6251 train_loss = 0.710\n",
      "2020-06-30T19:38:13.608070 Batch 1427/6251 train_loss = 0.887\n",
      "2020-06-30T19:38:13.850716 Batch 1447/6251 train_loss = 1.196\n",
      "2020-06-30T19:38:14.118221 Batch 1467/6251 train_loss = 0.921\n",
      "2020-06-30T19:38:14.327641 Batch 1487/6251 train_loss = 0.816\n",
      "2020-06-30T19:38:14.540593 Batch 1507/6251 train_loss = 0.808\n",
      "2020-06-30T19:38:14.754118 Batch 1527/6251 train_loss = 1.018\n",
      "2020-06-30T19:38:14.963577 Batch 1547/6251 train_loss = 0.770\n",
      "2020-06-30T19:38:15.171982 Batch 1567/6251 train_loss = 0.782\n",
      "2020-06-30T19:38:15.381421 Batch 1587/6251 train_loss = 0.776\n",
      "2020-06-30T19:38:15.587300 Batch 1607/6251 train_loss = 0.885\n",
      "2020-06-30T19:38:15.800843 Batch 1627/6251 train_loss = 1.046\n",
      "2020-06-30T19:38:16.009617 Batch 1647/6251 train_loss = 0.821\n",
      "2020-06-30T19:38:16.219194 Batch 1667/6251 train_loss = 1.015\n",
      "2020-06-30T19:38:16.424528 Batch 1687/6251 train_loss = 0.756\n",
      "2020-06-30T19:38:16.630152 Batch 1707/6251 train_loss = 0.735\n",
      "2020-06-30T19:38:16.844287 Batch 1727/6251 train_loss = 0.586\n",
      "2020-06-30T19:38:17.058514 Batch 1747/6251 train_loss = 0.884\n",
      "2020-06-30T19:38:17.276654 Batch 1767/6251 train_loss = 1.100\n",
      "2020-06-30T19:38:17.486208 Batch 1787/6251 train_loss = 0.809\n",
      "2020-06-30T19:38:17.701168 Batch 1807/6251 train_loss = 0.957\n",
      "2020-06-30T19:38:17.926373 Batch 1827/6251 train_loss = 0.913\n",
      "2020-06-30T19:38:18.134188 Batch 1847/6251 train_loss = 0.853\n",
      "2020-06-30T19:38:18.345919 Batch 1867/6251 train_loss = 0.699\n",
      "2020-06-30T19:38:18.557865 Batch 1887/6251 train_loss = 0.847\n",
      "2020-06-30T19:38:18.765770 Batch 1907/6251 train_loss = 0.759\n",
      "2020-06-30T19:38:18.981258 Batch 1927/6251 train_loss = 0.809\n",
      "2020-06-30T19:38:19.202640 Batch 1947/6251 train_loss = 1.065\n",
      "2020-06-30T19:38:19.411812 Batch 1967/6251 train_loss = 0.934\n",
      "2020-06-30T19:38:19.650922 Batch 1987/6251 train_loss = 0.939\n",
      "2020-06-30T19:38:19.864174 Batch 2007/6251 train_loss = 1.006\n",
      "2020-06-30T19:38:20.076906 Batch 2027/6251 train_loss = 0.797\n",
      "2020-06-30T19:38:20.336981 Batch 2047/6251 train_loss = 0.749\n",
      "2020-06-30T19:38:20.573999 Batch 2067/6251 train_loss = 0.817\n",
      "2020-06-30T19:38:20.801241 Batch 2087/6251 train_loss = 0.813\n",
      "2020-06-30T19:38:21.045154 Batch 2107/6251 train_loss = 0.817\n",
      "2020-06-30T19:38:21.282031 Batch 2127/6251 train_loss = 0.722\n",
      "2020-06-30T19:38:21.506740 Batch 2147/6251 train_loss = 0.561\n",
      "2020-06-30T19:38:21.740768 Batch 2167/6251 train_loss = 0.767\n",
      "2020-06-30T19:38:21.962706 Batch 2187/6251 train_loss = 0.923\n",
      "2020-06-30T19:38:22.183584 Batch 2207/6251 train_loss = 0.954\n",
      "2020-06-30T19:38:22.417739 Batch 2227/6251 train_loss = 0.936\n",
      "2020-06-30T19:38:22.654881 Batch 2247/6251 train_loss = 0.799\n",
      "2020-06-30T19:38:22.895065 Batch 2267/6251 train_loss = 1.131\n",
      "2020-06-30T19:38:23.145500 Batch 2287/6251 train_loss = 1.057\n",
      "2020-06-30T19:38:23.393453 Batch 2307/6251 train_loss = 0.729\n",
      "2020-06-30T19:38:23.620728 Batch 2327/6251 train_loss = 0.821\n",
      "2020-06-30T19:38:23.865118 Batch 2347/6251 train_loss = 0.784\n",
      "2020-06-30T19:38:24.097346 Batch 2367/6251 train_loss = 0.635\n",
      "2020-06-30T19:38:24.347041 Batch 2387/6251 train_loss = 0.692\n",
      "2020-06-30T19:38:24.590101 Batch 2407/6251 train_loss = 0.811\n",
      "2020-06-30T19:38:24.817490 Batch 2427/6251 train_loss = 0.855\n",
      "2020-06-30T19:38:25.047296 Batch 2447/6251 train_loss = 0.576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:38:25.277812 Batch 2467/6251 train_loss = 0.919\n",
      "2020-06-30T19:38:25.508501 Batch 2487/6251 train_loss = 1.015\n",
      "2020-06-30T19:38:25.823344 Batch 2507/6251 train_loss = 1.055\n",
      "2020-06-30T19:38:26.057611 Batch 2527/6251 train_loss = 0.823\n",
      "2020-06-30T19:38:26.285459 Batch 2547/6251 train_loss = 0.683\n",
      "2020-06-30T19:38:26.520724 Batch 2567/6251 train_loss = 0.674\n",
      "2020-06-30T19:38:26.831652 Batch 2587/6251 train_loss = 0.737\n",
      "2020-06-30T19:38:27.150907 Batch 2607/6251 train_loss = 0.646\n",
      "2020-06-30T19:38:27.448435 Batch 2627/6251 train_loss = 0.740\n",
      "2020-06-30T19:38:27.802787 Batch 2647/6251 train_loss = 0.668\n",
      "2020-06-30T19:38:28.140505 Batch 2667/6251 train_loss = 0.756\n",
      "2020-06-30T19:38:28.453480 Batch 2687/6251 train_loss = 0.831\n",
      "2020-06-30T19:38:28.754583 Batch 2707/6251 train_loss = 0.723\n",
      "2020-06-30T19:38:29.008223 Batch 2727/6251 train_loss = 1.090\n",
      "2020-06-30T19:38:29.249105 Batch 2747/6251 train_loss = 0.796\n",
      "2020-06-30T19:38:29.504125 Batch 2767/6251 train_loss = 0.933\n",
      "2020-06-30T19:38:29.745462 Batch 2787/6251 train_loss = 0.871\n",
      "2020-06-30T19:38:30.005660 Batch 2807/6251 train_loss = 0.742\n",
      "2020-06-30T19:38:30.236687 Batch 2827/6251 train_loss = 0.751\n",
      "2020-06-30T19:38:30.469815 Batch 2847/6251 train_loss = 0.602\n",
      "2020-06-30T19:38:30.720352 Batch 2867/6251 train_loss = 0.787\n",
      "2020-06-30T19:38:30.939114 Batch 2887/6251 train_loss = 0.760\n",
      "2020-06-30T19:38:31.158956 Batch 2907/6251 train_loss = 0.828\n",
      "2020-06-30T19:38:31.376676 Batch 2927/6251 train_loss = 0.808\n",
      "2020-06-30T19:38:31.612006 Batch 2947/6251 train_loss = 0.953\n",
      "2020-06-30T19:38:31.829259 Batch 2967/6251 train_loss = 0.830\n",
      "2020-06-30T19:38:32.041009 Batch 2987/6251 train_loss = 0.882\n",
      "2020-06-30T19:38:32.252966 Batch 3007/6251 train_loss = 1.044\n",
      "2020-06-30T19:38:32.475389 Batch 3027/6251 train_loss = 0.866\n",
      "2020-06-30T19:38:32.701387 Batch 3047/6251 train_loss = 0.731\n",
      "2020-06-30T19:38:32.918739 Batch 3067/6251 train_loss = 1.019\n",
      "2020-06-30T19:38:33.129490 Batch 3087/6251 train_loss = 0.782\n",
      "2020-06-30T19:38:33.341485 Batch 3107/6251 train_loss = 1.055\n",
      "2020-06-30T19:38:33.569655 Batch 3127/6251 train_loss = 0.968\n",
      "2020-06-30T19:38:33.788302 Batch 3147/6251 train_loss = 0.738\n",
      "2020-06-30T19:38:33.992662 Batch 3167/6251 train_loss = 0.541\n",
      "2020-06-30T19:38:34.208254 Batch 3187/6251 train_loss = 0.876\n",
      "2020-06-30T19:38:34.466659 Batch 3207/6251 train_loss = 0.653\n",
      "2020-06-30T19:38:34.819100 Batch 3227/6251 train_loss = 0.770\n",
      "2020-06-30T19:38:35.032527 Batch 3247/6251 train_loss = 0.842\n",
      "2020-06-30T19:38:35.237813 Batch 3267/6251 train_loss = 0.950\n",
      "2020-06-30T19:38:35.473186 Batch 3287/6251 train_loss = 0.979\n",
      "2020-06-30T19:38:35.745994 Batch 3307/6251 train_loss = 0.954\n",
      "2020-06-30T19:38:36.033304 Batch 3327/6251 train_loss = 0.814\n",
      "2020-06-30T19:38:36.247951 Batch 3347/6251 train_loss = 0.737\n",
      "2020-06-30T19:38:36.525861 Batch 3367/6251 train_loss = 0.933\n",
      "2020-06-30T19:38:36.841270 Batch 3387/6251 train_loss = 0.970\n",
      "2020-06-30T19:38:37.052723 Batch 3407/6251 train_loss = 0.833\n",
      "2020-06-30T19:38:37.315952 Batch 3427/6251 train_loss = 0.890\n",
      "2020-06-30T19:38:37.588669 Batch 3447/6251 train_loss = 0.868\n",
      "2020-06-30T19:38:37.824896 Batch 3467/6251 train_loss = 0.833\n",
      "2020-06-30T19:38:38.035292 Batch 3487/6251 train_loss = 0.737\n",
      "2020-06-30T19:38:38.250687 Batch 3507/6251 train_loss = 0.736\n",
      "2020-06-30T19:38:38.470259 Batch 3527/6251 train_loss = 0.786\n",
      "2020-06-30T19:38:38.699084 Batch 3547/6251 train_loss = 0.830\n",
      "2020-06-30T19:38:38.908764 Batch 3567/6251 train_loss = 1.119\n",
      "2020-06-30T19:38:39.125533 Batch 3587/6251 train_loss = 0.780\n",
      "2020-06-30T19:38:39.354620 Batch 3607/6251 train_loss = 0.788\n",
      "2020-06-30T19:38:39.592633 Batch 3627/6251 train_loss = 0.899\n",
      "2020-06-30T19:38:39.804407 Batch 3647/6251 train_loss = 0.861\n",
      "2020-06-30T19:38:40.018037 Batch 3667/6251 train_loss = 0.831\n",
      "2020-06-30T19:38:40.231424 Batch 3687/6251 train_loss = 0.598\n",
      "2020-06-30T19:38:40.452813 Batch 3707/6251 train_loss = 0.761\n",
      "2020-06-30T19:38:40.665775 Batch 3727/6251 train_loss = 0.848\n",
      "2020-06-30T19:38:40.881951 Batch 3747/6251 train_loss = 0.841\n",
      "2020-06-30T19:38:41.100792 Batch 3767/6251 train_loss = 0.802\n",
      "2020-06-30T19:38:41.346580 Batch 3787/6251 train_loss = 0.977\n",
      "2020-06-30T19:38:41.645719 Batch 3807/6251 train_loss = 0.742\n",
      "2020-06-30T19:38:41.944031 Batch 3827/6251 train_loss = 0.889\n",
      "2020-06-30T19:38:42.224788 Batch 3847/6251 train_loss = 0.829\n",
      "2020-06-30T19:38:42.439867 Batch 3867/6251 train_loss = 0.741\n",
      "2020-06-30T19:38:42.647548 Batch 3887/6251 train_loss = 0.712\n",
      "2020-06-30T19:38:42.853082 Batch 3907/6251 train_loss = 1.141\n",
      "2020-06-30T19:38:43.077841 Batch 3927/6251 train_loss = 0.750\n",
      "2020-06-30T19:38:43.294610 Batch 3947/6251 train_loss = 0.836\n",
      "2020-06-30T19:38:43.501217 Batch 3967/6251 train_loss = 0.728\n",
      "2020-06-30T19:38:43.708451 Batch 3987/6251 train_loss = 0.788\n",
      "2020-06-30T19:38:43.923179 Batch 4007/6251 train_loss = 0.778\n",
      "2020-06-30T19:38:44.156257 Batch 4027/6251 train_loss = 0.711\n",
      "2020-06-30T19:38:44.368000 Batch 4047/6251 train_loss = 1.048\n",
      "2020-06-30T19:38:44.574360 Batch 4067/6251 train_loss = 0.878\n",
      "2020-06-30T19:38:44.792483 Batch 4087/6251 train_loss = 0.729\n",
      "2020-06-30T19:38:45.028537 Batch 4107/6251 train_loss = 0.953\n",
      "2020-06-30T19:38:45.262436 Batch 4127/6251 train_loss = 0.802\n",
      "2020-06-30T19:38:45.473985 Batch 4147/6251 train_loss = 0.703\n",
      "2020-06-30T19:38:45.713227 Batch 4167/6251 train_loss = 0.804\n",
      "2020-06-30T19:38:45.934010 Batch 4187/6251 train_loss = 0.988\n",
      "2020-06-30T19:38:46.154240 Batch 4207/6251 train_loss = 0.972\n",
      "2020-06-30T19:38:46.379563 Batch 4227/6251 train_loss = 0.749\n",
      "2020-06-30T19:38:46.611409 Batch 4247/6251 train_loss = 0.652\n",
      "2020-06-30T19:38:46.835121 Batch 4267/6251 train_loss = 0.792\n",
      "2020-06-30T19:38:47.062842 Batch 4287/6251 train_loss = 0.800\n",
      "2020-06-30T19:38:47.285342 Batch 4307/6251 train_loss = 0.979\n",
      "2020-06-30T19:38:47.507078 Batch 4327/6251 train_loss = 0.806\n",
      "2020-06-30T19:38:47.731963 Batch 4347/6251 train_loss = 0.604\n",
      "2020-06-30T19:38:47.958638 Batch 4367/6251 train_loss = 0.758\n",
      "2020-06-30T19:38:48.181780 Batch 4387/6251 train_loss = 1.067\n",
      "2020-06-30T19:38:48.405207 Batch 4407/6251 train_loss = 0.749\n",
      "2020-06-30T19:38:48.631719 Batch 4427/6251 train_loss = 0.842\n",
      "2020-06-30T19:38:48.855970 Batch 4447/6251 train_loss = 0.737\n",
      "2020-06-30T19:38:49.107699 Batch 4467/6251 train_loss = 0.925\n",
      "2020-06-30T19:38:49.346863 Batch 4487/6251 train_loss = 0.726\n",
      "2020-06-30T19:38:49.574549 Batch 4507/6251 train_loss = 0.808\n",
      "2020-06-30T19:38:49.798607 Batch 4527/6251 train_loss = 0.768\n",
      "2020-06-30T19:38:50.031101 Batch 4547/6251 train_loss = 0.832\n",
      "2020-06-30T19:38:50.264666 Batch 4567/6251 train_loss = 0.781\n",
      "2020-06-30T19:38:50.490534 Batch 4587/6251 train_loss = 0.718\n",
      "2020-06-30T19:38:50.717954 Batch 4607/6251 train_loss = 0.847\n",
      "2020-06-30T19:38:50.946050 Batch 4627/6251 train_loss = 0.734\n",
      "2020-06-30T19:38:51.168833 Batch 4647/6251 train_loss = 0.716\n",
      "2020-06-30T19:38:51.415359 Batch 4667/6251 train_loss = 0.750\n",
      "2020-06-30T19:38:51.649504 Batch 4687/6251 train_loss = 0.831\n",
      "2020-06-30T19:38:51.877944 Batch 4707/6251 train_loss = 0.881\n",
      "2020-06-30T19:38:52.104044 Batch 4727/6251 train_loss = 0.698\n",
      "2020-06-30T19:38:52.348750 Batch 4747/6251 train_loss = 0.651\n",
      "2020-06-30T19:38:52.577705 Batch 4767/6251 train_loss = 0.734\n",
      "2020-06-30T19:38:52.804314 Batch 4787/6251 train_loss = 0.869\n",
      "2020-06-30T19:38:53.040641 Batch 4807/6251 train_loss = 0.756\n",
      "2020-06-30T19:38:53.287295 Batch 4827/6251 train_loss = 0.790\n",
      "2020-06-30T19:38:53.515928 Batch 4847/6251 train_loss = 0.708\n",
      "2020-06-30T19:38:53.759380 Batch 4867/6251 train_loss = 0.759\n",
      "2020-06-30T19:38:53.983239 Batch 4887/6251 train_loss = 0.754\n",
      "2020-06-30T19:38:54.206052 Batch 4907/6251 train_loss = 0.992\n",
      "2020-06-30T19:38:54.447338 Batch 4927/6251 train_loss = 0.695\n",
      "2020-06-30T19:38:54.675266 Batch 4947/6251 train_loss = 0.864\n",
      "2020-06-30T19:38:54.900339 Batch 4967/6251 train_loss = 0.948\n",
      "2020-06-30T19:38:55.129086 Batch 4987/6251 train_loss = 1.016\n",
      "2020-06-30T19:38:55.367235 Batch 5007/6251 train_loss = 0.583\n",
      "2020-06-30T19:38:55.592502 Batch 5027/6251 train_loss = 0.892\n",
      "2020-06-30T19:38:55.810793 Batch 5047/6251 train_loss = 1.052\n",
      "2020-06-30T19:38:56.034862 Batch 5067/6251 train_loss = 0.840\n",
      "2020-06-30T19:38:56.255860 Batch 5087/6251 train_loss = 1.146\n",
      "2020-06-30T19:38:56.482142 Batch 5107/6251 train_loss = 1.052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:38:56.700197 Batch 5127/6251 train_loss = 0.822\n",
      "2020-06-30T19:38:56.931376 Batch 5147/6251 train_loss = 0.736\n",
      "2020-06-30T19:38:57.151523 Batch 5167/6251 train_loss = 0.903\n",
      "2020-06-30T19:38:57.375735 Batch 5187/6251 train_loss = 0.905\n",
      "2020-06-30T19:38:57.600829 Batch 5207/6251 train_loss = 0.943\n",
      "2020-06-30T19:38:57.820825 Batch 5227/6251 train_loss = 1.021\n",
      "2020-06-30T19:38:58.040157 Batch 5247/6251 train_loss = 0.713\n",
      "2020-06-30T19:38:58.260681 Batch 5267/6251 train_loss = 0.593\n",
      "2020-06-30T19:38:58.489886 Batch 5287/6251 train_loss = 0.856\n",
      "2020-06-30T19:38:58.729952 Batch 5307/6251 train_loss = 0.772\n",
      "2020-06-30T19:38:58.957759 Batch 5327/6251 train_loss = 0.982\n",
      "2020-06-30T19:38:59.178465 Batch 5347/6251 train_loss = 0.768\n",
      "2020-06-30T19:38:59.417233 Batch 5367/6251 train_loss = 0.784\n",
      "2020-06-30T19:38:59.639932 Batch 5387/6251 train_loss = 0.721\n",
      "2020-06-30T19:38:59.854958 Batch 5407/6251 train_loss = 0.733\n",
      "2020-06-30T19:39:00.077902 Batch 5427/6251 train_loss = 0.825\n",
      "2020-06-30T19:39:00.322021 Batch 5447/6251 train_loss = 0.950\n",
      "2020-06-30T19:39:00.545038 Batch 5467/6251 train_loss = 0.983\n",
      "2020-06-30T19:39:00.761359 Batch 5487/6251 train_loss = 0.754\n",
      "2020-06-30T19:39:00.987675 Batch 5507/6251 train_loss = 0.819\n",
      "2020-06-30T19:39:01.212843 Batch 5527/6251 train_loss = 0.726\n",
      "2020-06-30T19:39:01.433345 Batch 5547/6251 train_loss = 0.821\n",
      "2020-06-30T19:39:01.656044 Batch 5567/6251 train_loss = 0.782\n",
      "2020-06-30T19:39:01.888546 Batch 5587/6251 train_loss = 0.941\n",
      "2020-06-30T19:39:02.106481 Batch 5607/6251 train_loss = 0.844\n",
      "2020-06-30T19:39:02.322800 Batch 5627/6251 train_loss = 0.874\n",
      "2020-06-30T19:39:02.542786 Batch 5647/6251 train_loss = 0.900\n",
      "2020-06-30T19:39:02.757970 Batch 5667/6251 train_loss = 0.875\n",
      "2020-06-30T19:39:02.965383 Batch 5687/6251 train_loss = 0.873\n",
      "2020-06-30T19:39:03.179328 Batch 5707/6251 train_loss = 0.812\n",
      "2020-06-30T19:39:03.387454 Batch 5727/6251 train_loss = 0.791\n",
      "2020-06-30T19:39:03.601154 Batch 5747/6251 train_loss = 0.838\n",
      "2020-06-30T19:39:03.812628 Batch 5767/6251 train_loss = 0.834\n",
      "2020-06-30T19:39:04.036753 Batch 5787/6251 train_loss = 0.593\n",
      "2020-06-30T19:39:04.246809 Batch 5807/6251 train_loss = 0.646\n",
      "2020-06-30T19:39:04.463012 Batch 5827/6251 train_loss = 0.642\n",
      "2020-06-30T19:39:04.669914 Batch 5847/6251 train_loss = 0.690\n",
      "2020-06-30T19:39:04.883387 Batch 5867/6251 train_loss = 0.897\n",
      "2020-06-30T19:39:05.099592 Batch 5887/6251 train_loss = 1.346\n",
      "2020-06-30T19:39:05.311574 Batch 5907/6251 train_loss = 0.946\n",
      "2020-06-30T19:39:05.530063 Batch 5927/6251 train_loss = 0.799\n",
      "2020-06-30T19:39:05.739201 Batch 5947/6251 train_loss = 0.805\n",
      "2020-06-30T19:39:05.951475 Batch 5967/6251 train_loss = 0.709\n",
      "2020-06-30T19:39:06.195705 Batch 5987/6251 train_loss = 0.792\n",
      "2020-06-30T19:39:06.415397 Batch 6007/6251 train_loss = 0.670\n",
      "2020-06-30T19:39:06.642578 Batch 6027/6251 train_loss = 1.185\n",
      "2020-06-30T19:39:06.852812 Batch 6047/6251 train_loss = 0.895\n",
      "2020-06-30T19:39:07.063462 Batch 6067/6251 train_loss = 0.831\n",
      "2020-06-30T19:39:07.276017 Batch 6087/6251 train_loss = 0.859\n",
      "2020-06-30T19:39:07.492048 Batch 6107/6251 train_loss = 0.929\n",
      "2020-06-30T19:39:07.700845 Batch 6127/6251 train_loss = 0.915\n",
      "2020-06-30T19:39:07.913522 Batch 6147/6251 train_loss = 1.185\n",
      "2020-06-30T19:39:08.167511 Batch 6167/6251 train_loss = 0.918\n",
      "2020-06-30T19:39:08.396550 Batch 6187/6251 train_loss = 0.878\n",
      "2020-06-30T19:39:08.611975 Batch 6207/6251 train_loss = 0.941\n",
      "2020-06-30T19:39:08.821758 Batch 6227/6251 train_loss = 1.103\n",
      "2020-06-30T19:39:09.021140 Batch 6247/6251 train_loss = 0.774\n",
      "2020-06-30T19:39:09.130935:Epoch   3 Batch    7/6251 test_loss = 0.993\n",
      "2020-06-30T19:39:09.337849:Epoch   3 Batch   27/6251 test_loss = 0.976\n",
      "2020-06-30T19:39:09.555417:Epoch   3 Batch   47/6251 test_loss = 0.915\n",
      "2020-06-30T19:39:09.764396:Epoch   3 Batch   67/6251 test_loss = 0.826\n",
      "2020-06-30T19:39:09.971571:Epoch   3 Batch   87/6251 test_loss = 0.923\n",
      "2020-06-30T19:39:10.183882:Epoch   3 Batch  107/6251 test_loss = 0.912\n",
      "2020-06-30T19:39:10.393694:Epoch   3 Batch  127/6251 test_loss = 0.888\n",
      "2020-06-30T19:39:10.614980:Epoch   3 Batch  147/6251 test_loss = 0.975\n",
      "2020-06-30T19:39:10.823145:Epoch   3 Batch  167/6251 test_loss = 0.951\n",
      "2020-06-30T19:39:11.040453:Epoch   3 Batch  187/6251 test_loss = 0.737\n",
      "2020-06-30T19:39:11.260207:Epoch   3 Batch  207/6251 test_loss = 0.753\n",
      "2020-06-30T19:39:11.470967:Epoch   3 Batch  227/6251 test_loss = 0.925\n",
      "2020-06-30T19:39:11.682835:Epoch   3 Batch  247/6251 test_loss = 0.874\n",
      "2020-06-30T19:39:11.906438:Epoch   3 Batch  267/6251 test_loss = 0.768\n",
      "2020-06-30T19:39:12.139349:Epoch   3 Batch  287/6251 test_loss = 0.848\n",
      "2020-06-30T19:39:12.355462:Epoch   3 Batch  307/6251 test_loss = 0.969\n",
      "2020-06-30T19:39:12.598732:Epoch   3 Batch  327/6251 test_loss = 0.701\n",
      "2020-06-30T19:39:12.813933:Epoch   3 Batch  347/6251 test_loss = 0.669\n",
      "2020-06-30T19:39:13.023365:Epoch   3 Batch  367/6251 test_loss = 0.812\n",
      "2020-06-30T19:39:13.234695:Epoch   3 Batch  387/6251 test_loss = 0.944\n",
      "2020-06-30T19:39:13.447223:Epoch   3 Batch  407/6251 test_loss = 0.985\n",
      "2020-06-30T19:39:13.653736:Epoch   3 Batch  427/6251 test_loss = 0.979\n",
      "2020-06-30T19:39:13.867778:Epoch   3 Batch  447/6251 test_loss = 0.822\n",
      "2020-06-30T19:39:14.075620:Epoch   3 Batch  467/6251 test_loss = 0.963\n",
      "2020-06-30T19:39:14.286412:Epoch   3 Batch  487/6251 test_loss = 0.819\n",
      "2020-06-30T19:39:14.498895:Epoch   3 Batch  507/6251 test_loss = 0.613\n",
      "2020-06-30T19:39:14.716532:Epoch   3 Batch  527/6251 test_loss = 0.766\n",
      "2020-06-30T19:39:14.935198:Epoch   3 Batch  547/6251 test_loss = 0.797\n",
      "2020-06-30T19:39:15.141829:Epoch   3 Batch  567/6251 test_loss = 0.760\n",
      "2020-06-30T19:39:15.362236:Epoch   3 Batch  587/6251 test_loss = 0.848\n",
      "2020-06-30T19:39:15.579857:Epoch   3 Batch  607/6251 test_loss = 0.827\n",
      "2020-06-30T19:39:15.806702:Epoch   3 Batch  627/6251 test_loss = 1.079\n",
      "2020-06-30T19:39:16.007525:Epoch   3 Batch  647/6251 test_loss = 0.882\n",
      "2020-06-30T19:39:16.214006:Epoch   3 Batch  667/6251 test_loss = 0.641\n",
      "2020-06-30T19:39:16.427477:Epoch   3 Batch  687/6251 test_loss = 0.789\n",
      "2020-06-30T19:39:16.644515:Epoch   3 Batch  707/6251 test_loss = 0.871\n",
      "2020-06-30T19:39:16.864405:Epoch   3 Batch  727/6251 test_loss = 0.794\n",
      "2020-06-30T19:39:17.084571:Epoch   3 Batch  747/6251 test_loss = 0.834\n",
      "2020-06-30T19:39:17.298599:Epoch   3 Batch  767/6251 test_loss = 0.747\n",
      "2020-06-30T19:39:17.506984:Epoch   3 Batch  787/6251 test_loss = 0.689\n",
      "2020-06-30T19:39:17.727188:Epoch   3 Batch  807/6251 test_loss = 0.881\n",
      "2020-06-30T19:39:17.943871:Epoch   3 Batch  827/6251 test_loss = 0.788\n",
      "2020-06-30T19:39:18.150959:Epoch   3 Batch  847/6251 test_loss = 0.881\n",
      "2020-06-30T19:39:18.358909:Epoch   3 Batch  867/6251 test_loss = 0.906\n",
      "2020-06-30T19:39:18.569646:Epoch   3 Batch  887/6251 test_loss = 0.575\n",
      "2020-06-30T19:39:18.780086:Epoch   3 Batch  907/6251 test_loss = 0.850\n",
      "2020-06-30T19:39:18.992095:Epoch   3 Batch  927/6251 test_loss = 0.894\n",
      "2020-06-30T19:39:19.205772:Epoch   3 Batch  947/6251 test_loss = 0.796\n",
      "2020-06-30T19:39:19.413522:Epoch   3 Batch  967/6251 test_loss = 0.720\n",
      "2020-06-30T19:39:19.627346:Epoch   3 Batch  987/6251 test_loss = 0.933\n",
      "2020-06-30T19:39:19.834850:Epoch   3 Batch 1007/6251 test_loss = 0.723\n",
      "2020-06-30T19:39:20.049128:Epoch   3 Batch 1027/6251 test_loss = 0.784\n",
      "2020-06-30T19:39:20.256804:Epoch   3 Batch 1047/6251 test_loss = 0.762\n",
      "2020-06-30T19:39:20.465487:Epoch   3 Batch 1067/6251 test_loss = 0.878\n",
      "2020-06-30T19:39:20.672282:Epoch   3 Batch 1087/6251 test_loss = 0.841\n",
      "2020-06-30T19:39:20.888200:Epoch   3 Batch 1107/6251 test_loss = 0.797\n",
      "2020-06-30T19:39:21.116781:Epoch   3 Batch 1127/6251 test_loss = 0.872\n",
      "2020-06-30T19:39:21.331626:Epoch   3 Batch 1147/6251 test_loss = 0.754\n",
      "2020-06-30T19:39:21.556177:Epoch   3 Batch 1167/6251 test_loss = 0.741\n",
      "2020-06-30T19:39:21.785180:Epoch   3 Batch 1187/6251 test_loss = 0.725\n",
      "2020-06-30T19:39:21.994869:Epoch   3 Batch 1207/6251 test_loss = 0.663\n",
      "2020-06-30T19:39:22.208600:Epoch   3 Batch 1227/6251 test_loss = 0.709\n",
      "2020-06-30T19:39:22.413298:Epoch   3 Batch 1247/6251 test_loss = 0.922\n",
      "2020-06-30T19:39:22.635269:Epoch   3 Batch 1267/6251 test_loss = 0.842\n",
      "2020-06-30T19:39:22.851073:Epoch   3 Batch 1287/6251 test_loss = 0.770\n",
      "2020-06-30T19:39:23.061617:Epoch   3 Batch 1307/6251 test_loss = 0.759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:39:23.275275:Epoch   3 Batch 1327/6251 test_loss = 1.006\n",
      "2020-06-30T19:39:23.483722:Epoch   3 Batch 1347/6251 test_loss = 0.937\n",
      "2020-06-30T19:39:23.697050:Epoch   3 Batch 1367/6251 test_loss = 0.874\n",
      "2020-06-30T19:39:23.902885:Epoch   3 Batch 1387/6251 test_loss = 1.008\n",
      "2020-06-30T19:39:24.117358:Epoch   3 Batch 1407/6251 test_loss = 0.722\n",
      "2020-06-30T19:39:24.324807:Epoch   3 Batch 1427/6251 test_loss = 0.767\n",
      "2020-06-30T19:39:24.537390:Epoch   3 Batch 1447/6251 test_loss = 0.809\n",
      "2020-06-30T19:39:24.752700:Epoch   3 Batch 1467/6251 test_loss = 0.621\n",
      "2020-06-30T19:39:24.966938:Epoch   3 Batch 1487/6251 test_loss = 0.886\n",
      "2020-06-30T19:39:25.178122:Epoch   3 Batch 1507/6251 test_loss = 0.794\n",
      "2020-06-30T19:39:25.392088:Epoch   3 Batch 1527/6251 test_loss = 0.781\n",
      "2020-06-30T19:39:25.594720:Epoch   3 Batch 1547/6251 test_loss = 0.969\n",
      "2020-06-30T19:39:27.107921 Batch   16/6251 train_loss = 0.827\n",
      "2020-06-30T19:39:27.341998 Batch   36/6251 train_loss = 0.746\n",
      "2020-06-30T19:39:27.606710 Batch   56/6251 train_loss = 1.020\n",
      "2020-06-30T19:39:27.833612 Batch   76/6251 train_loss = 1.010\n",
      "2020-06-30T19:39:28.059968 Batch   96/6251 train_loss = 1.031\n",
      "2020-06-30T19:39:28.290683 Batch  116/6251 train_loss = 0.673\n",
      "2020-06-30T19:39:28.519334 Batch  136/6251 train_loss = 0.708\n",
      "2020-06-30T19:39:28.750407 Batch  156/6251 train_loss = 0.626\n",
      "2020-06-30T19:39:28.987335 Batch  176/6251 train_loss = 1.061\n",
      "2020-06-30T19:39:29.217497 Batch  196/6251 train_loss = 0.697\n",
      "2020-06-30T19:39:29.448755 Batch  216/6251 train_loss = 0.862\n",
      "2020-06-30T19:39:29.676138 Batch  236/6251 train_loss = 0.793\n",
      "2020-06-30T19:39:29.913907 Batch  256/6251 train_loss = 0.743\n",
      "2020-06-30T19:39:30.155972 Batch  276/6251 train_loss = 0.688\n",
      "2020-06-30T19:39:30.385443 Batch  296/6251 train_loss = 0.735\n",
      "2020-06-30T19:39:30.610901 Batch  316/6251 train_loss = 0.812\n",
      "2020-06-30T19:39:30.848455 Batch  336/6251 train_loss = 0.905\n",
      "2020-06-30T19:39:31.087551 Batch  356/6251 train_loss = 0.686\n",
      "2020-06-30T19:39:31.331306 Batch  376/6251 train_loss = 0.625\n",
      "2020-06-30T19:39:31.611274 Batch  396/6251 train_loss = 0.946\n",
      "2020-06-30T19:39:31.921880 Batch  416/6251 train_loss = 0.809\n",
      "2020-06-30T19:39:32.194377 Batch  436/6251 train_loss = 0.842\n",
      "2020-06-30T19:39:32.428813 Batch  456/6251 train_loss = 0.826\n",
      "2020-06-30T19:39:32.651707 Batch  476/6251 train_loss = 0.793\n",
      "2020-06-30T19:39:32.902475 Batch  496/6251 train_loss = 0.758\n",
      "2020-06-30T19:39:33.245323 Batch  516/6251 train_loss = 0.929\n",
      "2020-06-30T19:39:33.559214 Batch  536/6251 train_loss = 0.931\n",
      "2020-06-30T19:39:33.866242 Batch  556/6251 train_loss = 0.754\n",
      "2020-06-30T19:39:34.194125 Batch  576/6251 train_loss = 1.062\n",
      "2020-06-30T19:39:34.531714 Batch  596/6251 train_loss = 0.768\n",
      "2020-06-30T19:39:34.978287 Batch  616/6251 train_loss = 0.864\n",
      "2020-06-30T19:39:35.386131 Batch  636/6251 train_loss = 0.682\n",
      "2020-06-30T19:39:35.710455 Batch  656/6251 train_loss = 0.876\n",
      "2020-06-30T19:39:36.077487 Batch  676/6251 train_loss = 0.898\n",
      "2020-06-30T19:39:36.368431 Batch  696/6251 train_loss = 1.022\n",
      "2020-06-30T19:39:36.632046 Batch  716/6251 train_loss = 0.836\n",
      "2020-06-30T19:39:36.931326 Batch  736/6251 train_loss = 0.862\n",
      "2020-06-30T19:39:37.233615 Batch  756/6251 train_loss = 0.988\n",
      "2020-06-30T19:39:37.494991 Batch  776/6251 train_loss = 0.979\n",
      "2020-06-30T19:39:37.734702 Batch  796/6251 train_loss = 0.978\n",
      "2020-06-30T19:39:37.993438 Batch  816/6251 train_loss = 0.911\n",
      "2020-06-30T19:39:38.239803 Batch  836/6251 train_loss = 0.727\n",
      "2020-06-30T19:39:38.467604 Batch  856/6251 train_loss = 0.831\n",
      "2020-06-30T19:39:38.732800 Batch  876/6251 train_loss = 0.871\n",
      "2020-06-30T19:39:38.996348 Batch  896/6251 train_loss = 0.828\n",
      "2020-06-30T19:39:39.275159 Batch  916/6251 train_loss = 0.900\n",
      "2020-06-30T19:39:39.539275 Batch  936/6251 train_loss = 0.631\n",
      "2020-06-30T19:39:39.801494 Batch  956/6251 train_loss = 0.822\n",
      "2020-06-30T19:39:40.082937 Batch  976/6251 train_loss = 0.982\n",
      "2020-06-30T19:39:40.356356 Batch  996/6251 train_loss = 0.932\n",
      "2020-06-30T19:39:40.623653 Batch 1016/6251 train_loss = 1.101\n",
      "2020-06-30T19:39:40.886241 Batch 1036/6251 train_loss = 1.053\n",
      "2020-06-30T19:39:41.135852 Batch 1056/6251 train_loss = 0.770\n",
      "2020-06-30T19:39:41.417672 Batch 1076/6251 train_loss = 0.799\n",
      "2020-06-30T19:39:41.637503 Batch 1096/6251 train_loss = 0.712\n",
      "2020-06-30T19:39:41.863820 Batch 1116/6251 train_loss = 1.020\n",
      "2020-06-30T19:39:42.083707 Batch 1136/6251 train_loss = 1.336\n",
      "2020-06-30T19:39:42.310345 Batch 1156/6251 train_loss = 0.883\n",
      "2020-06-30T19:39:42.531760 Batch 1176/6251 train_loss = 0.868\n",
      "2020-06-30T19:39:42.748648 Batch 1196/6251 train_loss = 1.116\n",
      "2020-06-30T19:39:42.966234 Batch 1216/6251 train_loss = 0.693\n",
      "2020-06-30T19:39:43.170159 Batch 1236/6251 train_loss = 0.710\n",
      "2020-06-30T19:39:43.386866 Batch 1256/6251 train_loss = 0.762\n",
      "2020-06-30T19:39:43.594456 Batch 1276/6251 train_loss = 0.943\n",
      "2020-06-30T19:39:43.854221 Batch 1296/6251 train_loss = 0.905\n",
      "2020-06-30T19:39:44.093218 Batch 1316/6251 train_loss = 0.836\n",
      "2020-06-30T19:39:44.333400 Batch 1336/6251 train_loss = 0.978\n",
      "2020-06-30T19:39:44.566146 Batch 1356/6251 train_loss = 0.894\n",
      "2020-06-30T19:39:44.832779 Batch 1376/6251 train_loss = 0.973\n",
      "2020-06-30T19:39:45.144205 Batch 1396/6251 train_loss = 0.902\n",
      "2020-06-30T19:39:45.466076 Batch 1416/6251 train_loss = 0.679\n",
      "2020-06-30T19:39:45.765898 Batch 1436/6251 train_loss = 0.964\n",
      "2020-06-30T19:39:46.114088 Batch 1456/6251 train_loss = 0.965\n",
      "2020-06-30T19:39:46.387621 Batch 1476/6251 train_loss = 1.032\n",
      "2020-06-30T19:39:46.643844 Batch 1496/6251 train_loss = 0.795\n",
      "2020-06-30T19:39:46.928213 Batch 1516/6251 train_loss = 0.743\n",
      "2020-06-30T19:39:47.206130 Batch 1536/6251 train_loss = 0.681\n",
      "2020-06-30T19:39:47.480074 Batch 1556/6251 train_loss = 0.836\n",
      "2020-06-30T19:39:47.740152 Batch 1576/6251 train_loss = 0.727\n",
      "2020-06-30T19:39:48.001297 Batch 1596/6251 train_loss = 0.862\n",
      "2020-06-30T19:39:48.259958 Batch 1616/6251 train_loss = 0.868\n",
      "2020-06-30T19:39:48.502458 Batch 1636/6251 train_loss = 0.650\n",
      "2020-06-30T19:39:48.725472 Batch 1656/6251 train_loss = 1.100\n",
      "2020-06-30T19:39:48.940485 Batch 1676/6251 train_loss = 0.813\n",
      "2020-06-30T19:39:49.154457 Batch 1696/6251 train_loss = 0.903\n",
      "2020-06-30T19:39:49.361186 Batch 1716/6251 train_loss = 0.902\n",
      "2020-06-30T19:39:49.569006 Batch 1736/6251 train_loss = 0.882\n",
      "2020-06-30T19:39:49.796986 Batch 1756/6251 train_loss = 0.817\n",
      "2020-06-30T19:39:50.014286 Batch 1776/6251 train_loss = 1.077\n",
      "2020-06-30T19:39:50.222212 Batch 1796/6251 train_loss = 0.757\n",
      "2020-06-30T19:39:50.467588 Batch 1816/6251 train_loss = 0.805\n",
      "2020-06-30T19:39:50.682156 Batch 1836/6251 train_loss = 1.043\n",
      "2020-06-30T19:39:50.890614 Batch 1856/6251 train_loss = 0.881\n",
      "2020-06-30T19:39:51.104127 Batch 1876/6251 train_loss = 0.800\n",
      "2020-06-30T19:39:51.344975 Batch 1896/6251 train_loss = 0.809\n",
      "2020-06-30T19:39:51.565635 Batch 1916/6251 train_loss = 1.095\n",
      "2020-06-30T19:39:51.804404 Batch 1936/6251 train_loss = 0.989\n",
      "2020-06-30T19:39:52.017648 Batch 1956/6251 train_loss = 0.632\n",
      "2020-06-30T19:39:52.238440 Batch 1976/6251 train_loss = 0.829\n",
      "2020-06-30T19:39:52.465563 Batch 1996/6251 train_loss = 0.768\n",
      "2020-06-30T19:39:52.672991 Batch 2016/6251 train_loss = 0.763\n",
      "2020-06-30T19:39:52.885847 Batch 2036/6251 train_loss = 0.720\n",
      "2020-06-30T19:39:53.101317 Batch 2056/6251 train_loss = 0.982\n",
      "2020-06-30T19:39:53.330147 Batch 2076/6251 train_loss = 0.849\n",
      "2020-06-30T19:39:53.535815 Batch 2096/6251 train_loss = 0.914\n",
      "2020-06-30T19:39:53.751211 Batch 2116/6251 train_loss = 0.880\n",
      "2020-06-30T19:39:53.976423 Batch 2136/6251 train_loss = 0.690\n",
      "2020-06-30T19:39:54.194600 Batch 2156/6251 train_loss = 0.936\n",
      "2020-06-30T19:39:54.410178 Batch 2176/6251 train_loss = 0.776\n",
      "2020-06-30T19:39:54.611657 Batch 2196/6251 train_loss = 1.040\n",
      "2020-06-30T19:39:54.827809 Batch 2216/6251 train_loss = 0.938\n",
      "2020-06-30T19:39:55.051378 Batch 2236/6251 train_loss = 0.896\n",
      "2020-06-30T19:39:55.262563 Batch 2256/6251 train_loss = 0.968\n",
      "2020-06-30T19:39:55.475933 Batch 2276/6251 train_loss = 0.820\n",
      "2020-06-30T19:39:55.681256 Batch 2296/6251 train_loss = 1.007\n",
      "2020-06-30T19:39:55.890421 Batch 2316/6251 train_loss = 0.785\n",
      "2020-06-30T19:39:56.091858 Batch 2336/6251 train_loss = 0.679\n",
      "2020-06-30T19:39:56.305377 Batch 2356/6251 train_loss = 0.732\n",
      "2020-06-30T19:39:56.559469 Batch 2376/6251 train_loss = 0.725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:39:56.762395 Batch 2396/6251 train_loss = 0.838\n",
      "2020-06-30T19:39:56.967845 Batch 2416/6251 train_loss = 0.707\n",
      "2020-06-30T19:39:57.176768 Batch 2436/6251 train_loss = 0.809\n",
      "2020-06-30T19:39:57.391218 Batch 2456/6251 train_loss = 0.903\n",
      "2020-06-30T19:39:57.600738 Batch 2476/6251 train_loss = 1.092\n",
      "2020-06-30T19:39:57.804201 Batch 2496/6251 train_loss = 0.768\n",
      "2020-06-30T19:39:58.014632 Batch 2516/6251 train_loss = 0.870\n",
      "2020-06-30T19:39:58.231362 Batch 2536/6251 train_loss = 0.941\n",
      "2020-06-30T19:39:58.448789 Batch 2556/6251 train_loss = 0.933\n",
      "2020-06-30T19:39:58.657605 Batch 2576/6251 train_loss = 0.941\n",
      "2020-06-30T19:39:58.864577 Batch 2596/6251 train_loss = 0.581\n",
      "2020-06-30T19:39:59.077417 Batch 2616/6251 train_loss = 0.966\n",
      "2020-06-30T19:39:59.294150 Batch 2636/6251 train_loss = 0.694\n",
      "2020-06-30T19:39:59.514249 Batch 2656/6251 train_loss = 0.968\n",
      "2020-06-30T19:39:59.729643 Batch 2676/6251 train_loss = 0.648\n",
      "2020-06-30T19:39:59.942234 Batch 2696/6251 train_loss = 0.750\n",
      "2020-06-30T19:40:00.171039 Batch 2716/6251 train_loss = 1.080\n",
      "2020-06-30T19:40:00.434043 Batch 2736/6251 train_loss = 0.847\n",
      "2020-06-30T19:40:00.652628 Batch 2756/6251 train_loss = 0.809\n",
      "2020-06-30T19:40:00.873908 Batch 2776/6251 train_loss = 0.899\n",
      "2020-06-30T19:40:01.134067 Batch 2796/6251 train_loss = 0.968\n",
      "2020-06-30T19:40:01.406979 Batch 2816/6251 train_loss = 0.863\n",
      "2020-06-30T19:40:01.638044 Batch 2836/6251 train_loss = 0.925\n",
      "2020-06-30T19:40:01.867504 Batch 2856/6251 train_loss = 0.792\n",
      "2020-06-30T19:40:02.142627 Batch 2876/6251 train_loss = 0.941\n",
      "2020-06-30T19:40:02.396588 Batch 2896/6251 train_loss = 0.759\n",
      "2020-06-30T19:40:02.644016 Batch 2916/6251 train_loss = 0.870\n",
      "2020-06-30T19:40:02.880890 Batch 2936/6251 train_loss = 1.082\n",
      "2020-06-30T19:40:03.152355 Batch 2956/6251 train_loss = 0.732\n",
      "2020-06-30T19:40:03.375499 Batch 2976/6251 train_loss = 0.880\n",
      "2020-06-30T19:40:03.604412 Batch 2996/6251 train_loss = 0.925\n",
      "2020-06-30T19:40:03.822240 Batch 3016/6251 train_loss = 0.975\n",
      "2020-06-30T19:40:04.043869 Batch 3036/6251 train_loss = 0.971\n",
      "2020-06-30T19:40:04.259357 Batch 3056/6251 train_loss = 0.919\n",
      "2020-06-30T19:40:04.480675 Batch 3076/6251 train_loss = 0.714\n",
      "2020-06-30T19:40:04.687320 Batch 3096/6251 train_loss = 0.907\n",
      "2020-06-30T19:40:04.898584 Batch 3116/6251 train_loss = 0.718\n",
      "2020-06-30T19:40:05.106529 Batch 3136/6251 train_loss = 0.856\n",
      "2020-06-30T19:40:05.327117 Batch 3156/6251 train_loss = 0.756\n",
      "2020-06-30T19:40:05.536377 Batch 3176/6251 train_loss = 0.882\n",
      "2020-06-30T19:40:05.752866 Batch 3196/6251 train_loss = 0.858\n",
      "2020-06-30T19:40:05.963486 Batch 3216/6251 train_loss = 0.790\n",
      "2020-06-30T19:40:06.169778 Batch 3236/6251 train_loss = 0.891\n",
      "2020-06-30T19:40:06.390851 Batch 3256/6251 train_loss = 0.836\n",
      "2020-06-30T19:40:06.595486 Batch 3276/6251 train_loss = 0.676\n",
      "2020-06-30T19:40:06.808806 Batch 3296/6251 train_loss = 0.998\n",
      "2020-06-30T19:40:07.009971 Batch 3316/6251 train_loss = 0.595\n",
      "2020-06-30T19:40:07.215558 Batch 3336/6251 train_loss = 0.693\n",
      "2020-06-30T19:40:07.418918 Batch 3356/6251 train_loss = 0.968\n",
      "2020-06-30T19:40:07.627384 Batch 3376/6251 train_loss = 0.944\n",
      "2020-06-30T19:40:07.834733 Batch 3396/6251 train_loss = 0.698\n",
      "2020-06-30T19:40:08.038721 Batch 3416/6251 train_loss = 0.818\n",
      "2020-06-30T19:40:08.248117 Batch 3436/6251 train_loss = 0.760\n",
      "2020-06-30T19:40:08.455991 Batch 3456/6251 train_loss = 0.983\n",
      "2020-06-30T19:40:08.663252 Batch 3476/6251 train_loss = 1.089\n",
      "2020-06-30T19:40:08.870232 Batch 3496/6251 train_loss = 0.695\n",
      "2020-06-30T19:40:09.077671 Batch 3516/6251 train_loss = 0.910\n",
      "2020-06-30T19:40:09.286382 Batch 3536/6251 train_loss = 0.765\n",
      "2020-06-30T19:40:09.490468 Batch 3556/6251 train_loss = 0.793\n",
      "2020-06-30T19:40:09.698712 Batch 3576/6251 train_loss = 1.021\n",
      "2020-06-30T19:40:09.903963 Batch 3596/6251 train_loss = 1.070\n",
      "2020-06-30T19:40:10.117013 Batch 3616/6251 train_loss = 0.779\n",
      "2020-06-30T19:40:10.324989 Batch 3636/6251 train_loss = 0.931\n",
      "2020-06-30T19:40:10.535813 Batch 3656/6251 train_loss = 0.671\n",
      "2020-06-30T19:40:10.740979 Batch 3676/6251 train_loss = 0.898\n",
      "2020-06-30T19:40:10.952213 Batch 3696/6251 train_loss = 1.070\n",
      "2020-06-30T19:40:11.159406 Batch 3716/6251 train_loss = 0.833\n",
      "2020-06-30T19:40:11.365124 Batch 3736/6251 train_loss = 0.988\n",
      "2020-06-30T19:40:11.570651 Batch 3756/6251 train_loss = 0.849\n",
      "2020-06-30T19:40:11.780975 Batch 3776/6251 train_loss = 0.821\n",
      "2020-06-30T19:40:11.983295 Batch 3796/6251 train_loss = 0.728\n",
      "2020-06-30T19:40:12.192808 Batch 3816/6251 train_loss = 0.943\n",
      "2020-06-30T19:40:12.403233 Batch 3836/6251 train_loss = 0.800\n",
      "2020-06-30T19:40:12.606837 Batch 3856/6251 train_loss = 0.775\n",
      "2020-06-30T19:40:12.808689 Batch 3876/6251 train_loss = 0.962\n",
      "2020-06-30T19:40:13.032158 Batch 3896/6251 train_loss = 0.799\n",
      "2020-06-30T19:40:13.241674 Batch 3916/6251 train_loss = 0.982\n",
      "2020-06-30T19:40:13.457929 Batch 3936/6251 train_loss = 0.707\n",
      "2020-06-30T19:40:13.666827 Batch 3956/6251 train_loss = 0.939\n",
      "2020-06-30T19:40:13.873517 Batch 3976/6251 train_loss = 0.697\n",
      "2020-06-30T19:40:14.080283 Batch 3996/6251 train_loss = 0.940\n",
      "2020-06-30T19:40:14.302201 Batch 4016/6251 train_loss = 0.732\n",
      "2020-06-30T19:40:14.513641 Batch 4036/6251 train_loss = 0.786\n",
      "2020-06-30T19:40:14.726067 Batch 4056/6251 train_loss = 0.848\n",
      "2020-06-30T19:40:14.941743 Batch 4076/6251 train_loss = 0.835\n",
      "2020-06-30T19:40:15.151782 Batch 4096/6251 train_loss = 0.816\n",
      "2020-06-30T19:40:15.357844 Batch 4116/6251 train_loss = 0.749\n",
      "2020-06-30T19:40:15.567624 Batch 4136/6251 train_loss = 0.672\n",
      "2020-06-30T19:40:15.779011 Batch 4156/6251 train_loss = 0.867\n",
      "2020-06-30T19:40:15.989110 Batch 4176/6251 train_loss = 1.026\n",
      "2020-06-30T19:40:16.214385 Batch 4196/6251 train_loss = 0.933\n",
      "2020-06-30T19:40:16.423316 Batch 4216/6251 train_loss = 0.778\n",
      "2020-06-30T19:40:16.633595 Batch 4236/6251 train_loss = 0.647\n",
      "2020-06-30T19:40:16.836292 Batch 4256/6251 train_loss = 0.844\n",
      "2020-06-30T19:40:17.044997 Batch 4276/6251 train_loss = 0.716\n",
      "2020-06-30T19:40:17.254882 Batch 4296/6251 train_loss = 0.855\n",
      "2020-06-30T19:40:17.462866 Batch 4316/6251 train_loss = 0.702\n",
      "2020-06-30T19:40:17.673511 Batch 4336/6251 train_loss = 0.925\n",
      "2020-06-30T19:40:17.883345 Batch 4356/6251 train_loss = 0.803\n",
      "2020-06-30T19:40:18.092830 Batch 4376/6251 train_loss = 0.871\n",
      "2020-06-30T19:40:18.311168 Batch 4396/6251 train_loss = 0.826\n",
      "2020-06-30T19:40:18.520933 Batch 4416/6251 train_loss = 0.921\n",
      "2020-06-30T19:40:18.735738 Batch 4436/6251 train_loss = 0.761\n",
      "2020-06-30T19:40:18.951234 Batch 4456/6251 train_loss = 0.852\n",
      "2020-06-30T19:40:19.173109 Batch 4476/6251 train_loss = 0.873\n",
      "2020-06-30T19:40:19.396110 Batch 4496/6251 train_loss = 0.859\n",
      "2020-06-30T19:40:19.613064 Batch 4516/6251 train_loss = 1.022\n",
      "2020-06-30T19:40:19.836324 Batch 4536/6251 train_loss = 0.829\n",
      "2020-06-30T19:40:20.126083 Batch 4556/6251 train_loss = 1.002\n",
      "2020-06-30T19:40:20.498361 Batch 4576/6251 train_loss = 0.727\n",
      "2020-06-30T19:40:20.753924 Batch 4596/6251 train_loss = 0.876\n",
      "2020-06-30T19:40:21.110232 Batch 4616/6251 train_loss = 0.909\n",
      "2020-06-30T19:40:21.441980 Batch 4636/6251 train_loss = 0.954\n",
      "2020-06-30T19:40:21.746130 Batch 4656/6251 train_loss = 0.871\n",
      "2020-06-30T19:40:22.142322 Batch 4676/6251 train_loss = 0.785\n",
      "2020-06-30T19:40:22.500306 Batch 4696/6251 train_loss = 0.848\n",
      "2020-06-30T19:40:22.825511 Batch 4716/6251 train_loss = 0.771\n",
      "2020-06-30T19:40:23.152533 Batch 4736/6251 train_loss = 0.866\n",
      "2020-06-30T19:40:23.421744 Batch 4756/6251 train_loss = 1.017\n",
      "2020-06-30T19:40:23.681260 Batch 4776/6251 train_loss = 0.810\n",
      "2020-06-30T19:40:23.940493 Batch 4796/6251 train_loss = 0.647\n",
      "2020-06-30T19:40:24.192814 Batch 4816/6251 train_loss = 0.934\n",
      "2020-06-30T19:40:24.421398 Batch 4836/6251 train_loss = 0.627\n",
      "2020-06-30T19:40:24.645438 Batch 4856/6251 train_loss = 0.682\n",
      "2020-06-30T19:40:24.866381 Batch 4876/6251 train_loss = 0.926\n",
      "2020-06-30T19:40:25.082506 Batch 4896/6251 train_loss = 0.797\n",
      "2020-06-30T19:40:25.295156 Batch 4916/6251 train_loss = 0.985\n",
      "2020-06-30T19:40:25.512812 Batch 4936/6251 train_loss = 1.033\n",
      "2020-06-30T19:40:25.726228 Batch 4956/6251 train_loss = 0.766\n",
      "2020-06-30T19:40:25.931487 Batch 4976/6251 train_loss = 0.776\n",
      "2020-06-30T19:40:26.154135 Batch 4996/6251 train_loss = 1.050\n",
      "2020-06-30T19:40:26.365925 Batch 5016/6251 train_loss = 0.916\n",
      "2020-06-30T19:40:26.573005 Batch 5036/6251 train_loss = 0.880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:40:26.784913 Batch 5056/6251 train_loss = 0.738\n",
      "2020-06-30T19:40:26.995834 Batch 5076/6251 train_loss = 0.873\n",
      "2020-06-30T19:40:27.197194 Batch 5096/6251 train_loss = 1.034\n",
      "2020-06-30T19:40:27.396936 Batch 5116/6251 train_loss = 0.972\n",
      "2020-06-30T19:40:27.607515 Batch 5136/6251 train_loss = 0.866\n",
      "2020-06-30T19:40:27.814726 Batch 5156/6251 train_loss = 0.978\n",
      "2020-06-30T19:40:28.029633 Batch 5176/6251 train_loss = 0.893\n",
      "2020-06-30T19:40:28.235258 Batch 5196/6251 train_loss = 0.888\n",
      "2020-06-30T19:40:28.443087 Batch 5216/6251 train_loss = 0.777\n",
      "2020-06-30T19:40:28.647466 Batch 5236/6251 train_loss = 0.877\n",
      "2020-06-30T19:40:28.861737 Batch 5256/6251 train_loss = 0.824\n",
      "2020-06-30T19:40:29.068774 Batch 5276/6251 train_loss = 0.896\n",
      "2020-06-30T19:40:29.278773 Batch 5296/6251 train_loss = 0.808\n",
      "2020-06-30T19:40:29.491987 Batch 5316/6251 train_loss = 0.761\n",
      "2020-06-30T19:40:29.700197 Batch 5336/6251 train_loss = 0.790\n",
      "2020-06-30T19:40:29.915587 Batch 5356/6251 train_loss = 0.742\n",
      "2020-06-30T19:40:30.165127 Batch 5376/6251 train_loss = 0.631\n",
      "2020-06-30T19:40:30.386136 Batch 5396/6251 train_loss = 0.766\n",
      "2020-06-30T19:40:30.592968 Batch 5416/6251 train_loss = 0.860\n",
      "2020-06-30T19:40:30.805885 Batch 5436/6251 train_loss = 0.743\n",
      "2020-06-30T19:40:31.017084 Batch 5456/6251 train_loss = 0.946\n",
      "2020-06-30T19:40:31.236080 Batch 5476/6251 train_loss = 0.824\n",
      "2020-06-30T19:40:31.447170 Batch 5496/6251 train_loss = 0.936\n",
      "2020-06-30T19:40:31.665188 Batch 5516/6251 train_loss = 0.888\n",
      "2020-06-30T19:40:31.872416 Batch 5536/6251 train_loss = 0.843\n",
      "2020-06-30T19:40:32.083532 Batch 5556/6251 train_loss = 0.852\n",
      "2020-06-30T19:40:32.294622 Batch 5576/6251 train_loss = 1.011\n",
      "2020-06-30T19:40:32.505932 Batch 5596/6251 train_loss = 0.775\n",
      "2020-06-30T19:40:32.719497 Batch 5616/6251 train_loss = 0.870\n",
      "2020-06-30T19:40:32.934993 Batch 5636/6251 train_loss = 1.061\n",
      "2020-06-30T19:40:33.148836 Batch 5656/6251 train_loss = 0.855\n",
      "2020-06-30T19:40:33.363894 Batch 5676/6251 train_loss = 0.615\n",
      "2020-06-30T19:40:33.582114 Batch 5696/6251 train_loss = 0.682\n",
      "2020-06-30T19:40:33.786550 Batch 5716/6251 train_loss = 0.849\n",
      "2020-06-30T19:40:34.000037 Batch 5736/6251 train_loss = 0.973\n",
      "2020-06-30T19:40:34.210939 Batch 5756/6251 train_loss = 0.692\n",
      "2020-06-30T19:40:34.414768 Batch 5776/6251 train_loss = 0.907\n",
      "2020-06-30T19:40:34.631172 Batch 5796/6251 train_loss = 0.831\n",
      "2020-06-30T19:40:34.847114 Batch 5816/6251 train_loss = 0.776\n",
      "2020-06-30T19:40:35.055050 Batch 5836/6251 train_loss = 0.725\n",
      "2020-06-30T19:40:35.261298 Batch 5856/6251 train_loss = 0.841\n",
      "2020-06-30T19:40:35.475406 Batch 5876/6251 train_loss = 1.018\n",
      "2020-06-30T19:40:35.685489 Batch 5896/6251 train_loss = 0.824\n",
      "2020-06-30T19:40:35.902294 Batch 5916/6251 train_loss = 0.943\n",
      "2020-06-30T19:40:36.110611 Batch 5936/6251 train_loss = 0.683\n",
      "2020-06-30T19:40:36.332350 Batch 5956/6251 train_loss = 0.580\n",
      "2020-06-30T19:40:36.539693 Batch 5976/6251 train_loss = 0.687\n",
      "2020-06-30T19:40:36.760085 Batch 5996/6251 train_loss = 0.691\n",
      "2020-06-30T19:40:36.972219 Batch 6016/6251 train_loss = 0.812\n",
      "2020-06-30T19:40:37.195654 Batch 6036/6251 train_loss = 0.905\n",
      "2020-06-30T19:40:37.445046 Batch 6056/6251 train_loss = 0.753\n",
      "2020-06-30T19:40:37.688870 Batch 6076/6251 train_loss = 0.883\n",
      "2020-06-30T19:40:37.921978 Batch 6096/6251 train_loss = 0.696\n",
      "2020-06-30T19:40:38.140730 Batch 6116/6251 train_loss = 0.945\n",
      "2020-06-30T19:40:38.348390 Batch 6136/6251 train_loss = 0.890\n",
      "2020-06-30T19:40:38.558458 Batch 6156/6251 train_loss = 0.721\n",
      "2020-06-30T19:40:38.793667 Batch 6176/6251 train_loss = 0.690\n",
      "2020-06-30T19:40:39.092827 Batch 6196/6251 train_loss = 0.952\n",
      "2020-06-30T19:40:39.373829 Batch 6216/6251 train_loss = 0.899\n",
      "2020-06-30T19:40:39.650233 Batch 6236/6251 train_loss = 0.807\n",
      "2020-06-30T19:40:40.124070:Epoch   4 Batch   16/6251 test_loss = 0.922\n",
      "2020-06-30T19:40:40.383473:Epoch   4 Batch   36/6251 test_loss = 0.660\n",
      "2020-06-30T19:40:40.620833:Epoch   4 Batch   56/6251 test_loss = 0.840\n",
      "2020-06-30T19:40:40.861833:Epoch   4 Batch   76/6251 test_loss = 0.782\n",
      "2020-06-30T19:40:41.117182:Epoch   4 Batch   96/6251 test_loss = 0.917\n",
      "2020-06-30T19:40:41.356236:Epoch   4 Batch  116/6251 test_loss = 0.853\n",
      "2020-06-30T19:40:41.597755:Epoch   4 Batch  136/6251 test_loss = 0.628\n",
      "2020-06-30T19:40:41.826640:Epoch   4 Batch  156/6251 test_loss = 0.811\n",
      "2020-06-30T19:40:42.060176:Epoch   4 Batch  176/6251 test_loss = 0.694\n",
      "2020-06-30T19:40:42.288879:Epoch   4 Batch  196/6251 test_loss = 0.855\n",
      "2020-06-30T19:40:42.528165:Epoch   4 Batch  216/6251 test_loss = 0.777\n",
      "2020-06-30T19:40:42.749438:Epoch   4 Batch  236/6251 test_loss = 0.631\n",
      "2020-06-30T19:40:42.969191:Epoch   4 Batch  256/6251 test_loss = 0.819\n",
      "2020-06-30T19:40:43.177215:Epoch   4 Batch  276/6251 test_loss = 0.883\n",
      "2020-06-30T19:40:43.417822:Epoch   4 Batch  296/6251 test_loss = 0.661\n",
      "2020-06-30T19:40:43.640721:Epoch   4 Batch  316/6251 test_loss = 0.931\n",
      "2020-06-30T19:40:43.866222:Epoch   4 Batch  336/6251 test_loss = 0.701\n",
      "2020-06-30T19:40:44.095223:Epoch   4 Batch  356/6251 test_loss = 0.800\n",
      "2020-06-30T19:40:44.321372:Epoch   4 Batch  376/6251 test_loss = 0.729\n",
      "2020-06-30T19:40:44.544009:Epoch   4 Batch  396/6251 test_loss = 0.831\n",
      "2020-06-30T19:40:44.773741:Epoch   4 Batch  416/6251 test_loss = 0.850\n",
      "2020-06-30T19:40:45.005148:Epoch   4 Batch  436/6251 test_loss = 0.916\n",
      "2020-06-30T19:40:45.219963:Epoch   4 Batch  456/6251 test_loss = 0.835\n",
      "2020-06-30T19:40:45.437459:Epoch   4 Batch  476/6251 test_loss = 0.817\n",
      "2020-06-30T19:40:45.645140:Epoch   4 Batch  496/6251 test_loss = 0.693\n",
      "2020-06-30T19:40:45.862365:Epoch   4 Batch  516/6251 test_loss = 0.807\n",
      "2020-06-30T19:40:46.067874:Epoch   4 Batch  536/6251 test_loss = 0.592\n",
      "2020-06-30T19:40:46.286525:Epoch   4 Batch  556/6251 test_loss = 1.062\n",
      "2020-06-30T19:40:46.496695:Epoch   4 Batch  576/6251 test_loss = 0.774\n",
      "2020-06-30T19:40:46.715023:Epoch   4 Batch  596/6251 test_loss = 0.949\n",
      "2020-06-30T19:40:46.925963:Epoch   4 Batch  616/6251 test_loss = 0.967\n",
      "2020-06-30T19:40:47.130262:Epoch   4 Batch  636/6251 test_loss = 0.726\n",
      "2020-06-30T19:40:47.350042:Epoch   4 Batch  656/6251 test_loss = 0.840\n",
      "2020-06-30T19:40:47.560232:Epoch   4 Batch  676/6251 test_loss = 0.856\n",
      "2020-06-30T19:40:47.765404:Epoch   4 Batch  696/6251 test_loss = 0.803\n",
      "2020-06-30T19:40:47.983352:Epoch   4 Batch  716/6251 test_loss = 0.891\n",
      "2020-06-30T19:40:48.199697:Epoch   4 Batch  736/6251 test_loss = 0.839\n",
      "2020-06-30T19:40:48.414406:Epoch   4 Batch  756/6251 test_loss = 0.773\n",
      "2020-06-30T19:40:48.637100:Epoch   4 Batch  776/6251 test_loss = 0.846\n",
      "2020-06-30T19:40:48.843263:Epoch   4 Batch  796/6251 test_loss = 0.799\n",
      "2020-06-30T19:40:49.056787:Epoch   4 Batch  816/6251 test_loss = 0.865\n",
      "2020-06-30T19:40:49.262745:Epoch   4 Batch  836/6251 test_loss = 0.823\n",
      "2020-06-30T19:40:49.476127:Epoch   4 Batch  856/6251 test_loss = 0.893\n",
      "2020-06-30T19:40:49.690068:Epoch   4 Batch  876/6251 test_loss = 1.196\n",
      "2020-06-30T19:40:49.903227:Epoch   4 Batch  896/6251 test_loss = 0.889\n",
      "2020-06-30T19:40:50.115117:Epoch   4 Batch  916/6251 test_loss = 0.850\n",
      "2020-06-30T19:40:50.317725:Epoch   4 Batch  936/6251 test_loss = 0.941\n",
      "2020-06-30T19:40:50.533713:Epoch   4 Batch  956/6251 test_loss = 0.831\n",
      "2020-06-30T19:40:50.754781:Epoch   4 Batch  976/6251 test_loss = 0.689\n",
      "2020-06-30T19:40:50.960010:Epoch   4 Batch  996/6251 test_loss = 0.732\n",
      "2020-06-30T19:40:51.173008:Epoch   4 Batch 1016/6251 test_loss = 0.797\n",
      "2020-06-30T19:40:51.381467:Epoch   4 Batch 1036/6251 test_loss = 0.735\n",
      "2020-06-30T19:40:51.594560:Epoch   4 Batch 1056/6251 test_loss = 0.846\n",
      "2020-06-30T19:40:51.811548:Epoch   4 Batch 1076/6251 test_loss = 0.846\n",
      "2020-06-30T19:40:52.027550:Epoch   4 Batch 1096/6251 test_loss = 0.899\n",
      "2020-06-30T19:40:52.245387:Epoch   4 Batch 1116/6251 test_loss = 0.776\n",
      "2020-06-30T19:40:52.452948:Epoch   4 Batch 1136/6251 test_loss = 0.756\n",
      "2020-06-30T19:40:52.659513:Epoch   4 Batch 1156/6251 test_loss = 0.838\n",
      "2020-06-30T19:40:52.871512:Epoch   4 Batch 1176/6251 test_loss = 0.874\n",
      "2020-06-30T19:40:53.090052:Epoch   4 Batch 1196/6251 test_loss = 0.943\n",
      "2020-06-30T19:40:53.304421:Epoch   4 Batch 1216/6251 test_loss = 0.855\n",
      "2020-06-30T19:40:53.518088:Epoch   4 Batch 1236/6251 test_loss = 0.738\n",
      "2020-06-30T19:40:53.762961:Epoch   4 Batch 1256/6251 test_loss = 0.823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30T19:40:53.978605:Epoch   4 Batch 1276/6251 test_loss = 0.737\n",
      "2020-06-30T19:40:54.186081:Epoch   4 Batch 1296/6251 test_loss = 0.652\n",
      "2020-06-30T19:40:54.397585:Epoch   4 Batch 1316/6251 test_loss = 0.979\n",
      "2020-06-30T19:40:54.626574:Epoch   4 Batch 1336/6251 test_loss = 0.861\n",
      "2020-06-30T19:40:54.876628:Epoch   4 Batch 1356/6251 test_loss = 0.871\n",
      "2020-06-30T19:40:55.102079:Epoch   4 Batch 1376/6251 test_loss = 0.874\n",
      "2020-06-30T19:40:55.373832:Epoch   4 Batch 1396/6251 test_loss = 0.680\n",
      "2020-06-30T19:40:55.647945:Epoch   4 Batch 1416/6251 test_loss = 0.758\n",
      "2020-06-30T19:40:55.897261:Epoch   4 Batch 1436/6251 test_loss = 0.883\n",
      "2020-06-30T19:40:56.128426:Epoch   4 Batch 1456/6251 test_loss = 0.844\n",
      "2020-06-30T19:40:56.351542:Epoch   4 Batch 1476/6251 test_loss = 0.637\n",
      "2020-06-30T19:40:56.577305:Epoch   4 Batch 1496/6251 test_loss = 0.780\n",
      "2020-06-30T19:40:56.808322:Epoch   4 Batch 1516/6251 test_loss = 0.969\n",
      "2020-06-30T19:40:57.029862:Epoch   4 Batch 1536/6251 test_loss = 0.688\n",
      "2020-06-30T19:40:57.257080:Epoch   4 Batch 1556/6251 test_loss = 0.800\n",
      "it`s over!\n"
     ]
    }
   ],
   "source": [
    "###前馈神经网络搭建结束\n",
    "#训练\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "losses = {'train': [], 'test': []}\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "#     for train_idx, test_idx in kf.split(features, targets_values):\n",
    "#         train_X, train_y = features[train_idx], targets_values[train_idx]\n",
    "        \n",
    "#         test_X, test_y = features[test_idx], targets_values[test_idx]\n",
    "        \n",
    "#         train_batch = get_batch(train_X, train_y, batch_size)\n",
    "#         test_batch = get_batch(test_X, test_y, batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        ###将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X, test_X, train_y, test_y = train_test_split(features, targets_values, test_size=0.2, random_state=0)\n",
    "        \n",
    "        train_batches = get_batch(train_X, train_y, batch_size)\n",
    "        test_batches = get_batch(test_X, test_y, batch_size)\n",
    "   \n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "            \n",
    "            genres = np.zeros([batch_size, 18])\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                genres[i] = x.take(6, 1)[i]\n",
    "                titles[i] = x.take(5, 1)[i]\n",
    "            \n",
    "            feed_dict = {\n",
    "                uid: np.reshape(x.take(0, 1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2, 1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3, 1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4, 1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1, 1), [batch_size, 1]),\n",
    "                movie_genres: genres,\n",
    "                movie_title: titles,\n",
    "                target: np.reshape(y, [batch_size, 1]),\n",
    "                learning_rate: lr\n",
    "            }\n",
    "            \n",
    "            step, train_loss, _ = sess.run([global_step, loss, train_op],feed_dict)\n",
    "            losses['train'].append(train_loss)\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{:>3} Batch {:>4}/{} train_loss = {:.3f}'.format(time_str, batch_i, (len(train_X) // batch_size), train_loss))\n",
    "        for batch_i in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6, 1)[i]\n",
    "            \n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "            \n",
    "            feed_dict = {\n",
    "                uid: np.reshape(x.take(0, 1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2, 1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3, 1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4, 1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1, 1), [batch_size, 1]),\n",
    "                movie_genres: genres,\n",
    "                movie_title: titles,\n",
    "                target: np.reshape(y, [batch_size, 1]),\n",
    "                learning_rate: lr\n",
    "            }\n",
    "            \n",
    "            step, train_loss, _ = sess.run([global_step, loss, train_op], feed_dict=feed_dict)\n",
    "            \n",
    "            losses['test'].append(train_loss)            \n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}:Epoch {:>3} Batch {:>4}/{} test_loss = {:.3f}'.format(time_str, epoch_i, batch_i, (len(train_X) // batch_size), train_loss))\n",
    "    saver.save(sess, './DNN_Rec')\n",
    "    print('it`s over!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "##获取已经计算的张量\n",
    "\"\"\"\n",
    "                uid: np.reshape(x.take(0, 1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2, 1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3, 1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4, 1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1, 1), [batch_size, 1]),\n",
    "                movie_genres: genres,\n",
    "                movie_title: titles,\n",
    "                target: np.reshape(y, [batch_size, 1]),\n",
    "                learning_rate: lr\n",
    "\"\"\"\n",
    "def get_tensor(loaded_graph):\n",
    "    ##特征张量\n",
    "    uid = loaded_graph.get_tensor_by_name('uid:0')\n",
    "    user_gender = loaded_graph.get_tensor_by_name('user_gender:0')\n",
    "    user_age = loaded_graph.get_tensor_by_name('user_age:0')\n",
    "    user_job = loaded_graph.get_tensor_by_name('user_job:0')\n",
    "    movie_id = loaded_graph.get_tensor_by_name('movie_id:0')\n",
    "    movie_genres = loaded_graph.get_tensor_by_name('movie_genres:0')\n",
    "    movie_title = loaded_graph.get_tensor_by_name('movie_title:0')\n",
    "    target = loaded_graph.get_tensor_by_name('rating:0')\n",
    "    learning_rate = loaded_graph.get_tensor_by_name('learning_rate:0')\n",
    "    \n",
    "    ##user矩阵*movie矩阵\n",
    "    inference = loaded_graph.get_tensor_by_name('inference/ExpandDims:0')\n",
    "    \n",
    "    ##user和movie组合特征向量\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name('movie_fc/Reshape:0')\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name('user_fc/Reshape:0')\n",
    "    \n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_genres, movie_title, target, learning_rate, inference, movie_combine_layer_flat, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id_val, movie_id_val):\n",
    "    ###!!!!!!session内的变量名，不可与session外变量名一致\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        ##导入之前训练好的模型\n",
    "        #导入神经网络的计算图结构\n",
    "        loader = tf.train.import_meta_graph('./DNN_Rec' + '.meta')  ##加载已经持久化的计算图\n",
    "        loader.restore(sess, './DNN_Rec') ##恢复模型之前的参数以及计算\n",
    "\n",
    "        uid_, user_gender_, user_age_, user_job_, movie_id_, movie_genres_, movie_title_, target_, learning_rate_, inference_, _, _ = get_tensor(loaded_graph)\n",
    "        \n",
    "        genres = np.zeros([1, 18])\n",
    "        \n",
    "        genres[0] = data[data['MovieID'] == movie_id_val]['Genres'].values[0]\n",
    "        \n",
    "        titles = np.zeros([1, 15])\n",
    "        \n",
    "        titles[0] = data[data['MovieID'] == movie_id_val]['Title'].values[0]\n",
    "        \n",
    "        gender = np.reshape(data[data['UserID'] == user_id_val]['Gender'].values[0], [1, 1])\n",
    "        \n",
    "        age = np.reshape(data[data['UserID'] == user_id_val]['Age'].values[0], [1, 1])\n",
    "        \n",
    "        job = np.reshape(data[data['UserID'] == user_id_val]['JobID'].values[0], [1, 1])\n",
    "        \n",
    "        movie_id = np.reshape([movie_id_val], [1, 1])\n",
    "        \n",
    "        feed_dict = {\n",
    "            uid_: np.reshape([user_id_val], [1, 1]),\n",
    "            user_gender_: gender,\n",
    "            user_age_: age,\n",
    "            user_job_: job,\n",
    "            movie_id_: movie_id,\n",
    "            movie_title_: titles,\n",
    "            movie_genres_: genres\n",
    "        }\n",
    "        \n",
    "        predict = sess.run([inference_], feed_dict=feed_dict)\n",
    "        \n",
    "        return predict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN_Rec\n"
     ]
    }
   ],
   "source": [
    "###计算user特征矩阵和movie特征矩阵\n",
    "loaded_graph = tf.Graph()\n",
    "movie_matrics = []\n",
    "user_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    ##导入之前训练好的模型\n",
    "    #导入神经网络的计算图结构\n",
    "    loader = tf.train.import_meta_graph('./DNN_Rec' + '.meta')  ##加载已经持久化的计算图\n",
    "    loader.restore(sess, './DNN_Rec') ##恢复模型之前的参数以及计算\n",
    "\n",
    "    uid_, user_gender_, user_age_, user_job_, movie_id_, movie_genres_, movie_title_, target_, learning_rate_, inference_, movie_combine_layer_flat_, user_combine_layer_flat_ = get_tensor(loaded_graph)\n",
    "\n",
    "\n",
    "    for movieid in movies['MovieID'].values:\n",
    "        genres = np.zeros([1, 18])\n",
    "        titles = np.zeros([1, 15])\n",
    "        genres[0] = movies.values[movieid2idx[movieid]][2]   ###这里的特征ID处理有问题，作者的处理方式感觉不太科学，暂时先用吧\n",
    "        titles[0] = movies.values[movieid2idx[movieid]][1]\n",
    "        movieid = np.reshape([movieid], [1, 1])\n",
    "        feed_dict = {\n",
    "            movie_id_: movieid,\n",
    "            movie_title_: titles,\n",
    "            movie_genres_: genres\n",
    "        }\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat_], feed_dict=feed_dict)\n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "    \n",
    "    for userid in users['UserID'].values:\n",
    "        gender = np.reshape(data[data['UserID'] == userid]['Gender'].values[0], [1, 1])\n",
    "        \n",
    "        age = np.reshape(data[data['UserID'] == userid]['Age'].values[0], [1, 1])\n",
    "        \n",
    "        job = np.reshape(data[data['UserID'] == userid]['JobID'].values[0], [1, 1])\n",
    "                \n",
    "        feed_dict = {\n",
    "            uid_: np.reshape([userid], [1, 1]),\n",
    "            user_gender_: gender,\n",
    "            user_age_: age,\n",
    "            user_job_: job,\n",
    "        }\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat_], feed_dict=feed_dict)\n",
    "        user_matrics.append(user_combine_layer_flat_val)\n",
    "    \n",
    "    movie_matrics = np.array(movie_matrics).reshape([-1, 200])\n",
    "    user_matrics = np.array(user_matrics).reshape([-1, 200])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN_Rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[array([[3.6275811]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(predict_rating(234, 1401))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "##通过观看的电影来进行推荐(将接收的movieID获取到该电影的特征向量，通过和电影矩阵进行计算相似度，返回相似度最高的n部电影)\n",
    "def recommend_by_movie(movie_id_val, top_k=20):\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        loader = tf.train.import_meta_graph('./DNN_Rec' + '.meta')\n",
    "        loader.restore(sess, './DNN_Rec')\n",
    "        \n",
    "        ##相似度计算(余弦相似度计算: a*b / sqrt(sum(a ** 2)) + sqrt(sum(b ** 2))\n",
    "        \n",
    "        ##分母计算\n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        ##计算a / sqrt(sum(a ** 2))\n",
    "#         normalizerd_matrics = movie_matrics / norm_movie_matrics\n",
    "        \n",
    "        ##相似度计算，用以推荐\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        norm_probs_embeddings = tf.sqrt(tf.reduce_sum(tf.square(probs_embeddings), 1, keep_dims=True))\n",
    "        normalizerd_matrics = movie_matrics / (norm_movie_matrics * norm_probs_embeddings)\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalizerd_matrics))\n",
    "        \n",
    "        sims = (probs_similarity.eval())\n",
    "        rec = np.squeeze(sims)\n",
    "        res = np.argsort(rec)[-1:-(top_k + 1):-1]\n",
    "        print(res)\n",
    "        result = []\n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        \n",
    "        for rec_val in res:\n",
    "#             print(rec_val)\n",
    "            print(movies_orig[rec_val])\n",
    "            result.append(movies_orig[rec_val])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN_Rec\n",
      "[1717  712 2552 1347  645  648 2497 1348 2148  684 2095 2087 2673 1994\n",
      " 1961  753  779  810  845 1463]\n",
      "您看的电影是：[356 'Forrest Gump (1994)' 'Comedy|Romance|War']\n",
      "以下是给您的推荐：\n",
      "[1773 'Tokyo Fist (1995)' 'Action|Drama']\n",
      "[721 'Halfmoon (Paul Bowles - Halbmond) (1995)' 'Drama']\n",
      "[2621 'Xiu Xiu: The Sent-Down Girl (Tian yu) (1998)' 'Drama|Romance']\n",
      "[1368 'Forbidden Christ, The (Cristo proibito, Il) (1950)' 'Drama']\n",
      "[651 'Superweib, Das (1996)' 'Comedy']\n",
      "[654 'Und keiner weint mir nach (1996)' 'Drama|Romance']\n",
      "[2566 \"Doug's 1st Movie (1999)\" \"Animation|Children's\"]\n",
      "[1369 \"I Can't Sleep (J'ai pas sommeil) (1994)\" 'Drama|Thriller']\n",
      "[2217 'Elstree Calling (1930)' 'Comedy|Musical']\n",
      "[693 'Under the Domin Tree (Etz Hadomim Tafus) (1994)' 'Drama']\n",
      "[2164 'Surf Nazis Must Die (1987)' 'Drama']\n",
      "[2156 'Best Man, The (Il Testimone dello sposo) (1997)' 'Comedy|Drama']\n",
      "[2742 'M�nage (Tenue de soir�e) (1986)' 'Comedy|Drama']\n",
      "[2063 'Seventh Heaven (Le Septi�me ciel) (1997)' 'Drama|Romance']\n",
      "[2030 'East Palace West Palace (Dong gong xi gong) (1997)' 'Drama']\n",
      "[763 'Last of the High Kings, The (a.k.a. Summer Fling) (1996)' 'Drama']\n",
      "[789 'I, Worst of All (Yo, la peor de todas) (1990)' 'Drama']\n",
      "[821 'Crude Oasis, The (1995)' 'Romance']\n",
      "[856 'Mille bolle blu (1993)' 'Comedy']\n",
      "[1495 'Turbo: A Power Rangers Movie (1997)' \"Action|Adventure|Children's\"]\n"
     ]
    }
   ],
   "source": [
    "result = recommend_by_movie(356)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def recommend_by_user(user_id_val, top_k=20):\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        loader = tf.train.import_meta_graph('./DNN_Rec' + '.meta')\n",
    "        loader.restore(sess, './DNN_Rec')\n",
    "        \n",
    "        ##相似度计算(余弦相似度计算: a*b / sqrt(sum(a ** 2)) + sqrt(sum(b ** 2))\n",
    "        \n",
    "        ##分母计算\n",
    "        norm_user_matrics = tf.sqrt(tf.reduce_sum(tf.square(user_matrics), 1, keep_dims=True))\n",
    "        ##计算a / sqrt(sum(a ** 2))\n",
    "#         normalizerd_matrics = movie_matrics / norm_movie_matrics\n",
    "        \n",
    "        ##相似度计算，用以推荐\n",
    "        probs_embeddings = (user_matrics[user_id_val -1]).reshape([1, 200])\n",
    "        norm_probs_embeddings = tf.sqrt(tf.reduce_sum(tf.square(probs_embeddings), 1, keep_dims=True))\n",
    "        normalizerd_matrics = user_matrics / (norm_user_matrics * norm_probs_embeddings)\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalizerd_matrics))\n",
    "        \n",
    "        sims = (probs_similarity.eval())\n",
    "        rec = np.squeeze(sims)\n",
    "        res = np.argsort(rec)[-1:-(top_k + 1):-1]\n",
    "#         print(res)\n",
    "        movie_names = []\n",
    "#         print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        results = set()\n",
    "        \n",
    "        ##计算共现矩阵\n",
    "        movie_id_dict = {\n",
    "            user_id_val: data[data['UserID'] == user_id_val]['MovieID'].values\n",
    "        }\n",
    "        for res_user_id in res:\n",
    "            idx = (data[data['UserID'] == res_user_id]).index\n",
    "            movie_id_list = []\n",
    "            for idx_i in idx:\n",
    "                if data.loc[idx_i]['ratings'] > 3:\n",
    "                    movie_id_list.append(data.loc[idx_i]['MovieID'])\n",
    "            movie_id_dict[idx_i] = movie_id_list\n",
    "        while len(results) != 20:\n",
    "            for _userid, _movieid in movie_id_dict.items():\n",
    "                if _userid != user_id_val:\n",
    "                    c = random.choice(_movieid)\n",
    "                    if c not in movie_id_dict[user_id_val]:\n",
    "                        results.add(c)\n",
    "                        break\n",
    "#             break\n",
    "        for rec_val in results:\n",
    "#             print(rec_val)\n",
    "            print(movies_orig[rec_val])\n",
    "            movie_names.append(movies_orig[rec_val])\n",
    "        return movie_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN_Rec\n",
      "以下是给您的推荐：\n",
      "[2761 'Iron Giant, The (1999)' \"Animation|Children's\"]\n",
      "[788 'Nutty Professor, The (1996)' 'Comedy|Fantasy|Romance|Sci-Fi']\n",
      "[17 'Sense and Sensibility (1995)' 'Drama|Romance']\n",
      "[26 'Othello (1995)' 'Drama']\n",
      "[810 'Kazaam (1996)' \"Children's|Comedy|Fantasy\"]\n",
      "[296 'Pulp Fiction (1994)' 'Crime|Drama']\n",
      "[3692 \"Class of Nuke 'Em High (1986)\" 'Comedy|Horror']\n",
      "[303 'Quick and the Dead, The (1995)' 'Action|Adventure|Western']\n",
      "[51 'Guardian Angel (1994)' 'Action|Drama|Thriller']\n",
      "[1498 'Inventing the Abbotts (1997)' 'Drama|Romance']\n",
      "[3585 'Great Locomotive Chase, The (1956)' 'Adventure|War']\n",
      "[1105 'Children of the Corn IV: The Gathering (1996)' 'Horror']\n",
      "[3723 'Hamlet (1990)' 'Drama']\n",
      "[461 'Go Fish (1994)' 'Drama|Romance']\n",
      "[1519 'Broken English (1996)' 'Drama']\n",
      "[1379 'Young Guns II (1990)' 'Action|Comedy|Western']\n",
      "[3367 \"Devil's Brigade, The (1968)\" 'War']\n",
      "[1007 'Apple Dumpling Gang, The (1975)' \"Children's|Comedy|Western\"]\n",
      "[1846 'Nil By Mouth (1997)' 'Drama']\n",
      "[1852 'Love Walked In (1998)' 'Drama|Thriller']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2761, 'Iron Giant, The (1999)', \"Animation|Children's\"],\n",
       "       dtype=object),\n",
       " array([788, 'Nutty Professor, The (1996)',\n",
       "        'Comedy|Fantasy|Romance|Sci-Fi'], dtype=object),\n",
       " array([17, 'Sense and Sensibility (1995)', 'Drama|Romance'], dtype=object),\n",
       " array([26, 'Othello (1995)', 'Drama'], dtype=object),\n",
       " array([810, 'Kazaam (1996)', \"Children's|Comedy|Fantasy\"], dtype=object),\n",
       " array([296, 'Pulp Fiction (1994)', 'Crime|Drama'], dtype=object),\n",
       " array([3692, \"Class of Nuke 'Em High (1986)\", 'Comedy|Horror'],\n",
       "       dtype=object),\n",
       " array([303, 'Quick and the Dead, The (1995)', 'Action|Adventure|Western'],\n",
       "       dtype=object),\n",
       " array([51, 'Guardian Angel (1994)', 'Action|Drama|Thriller'], dtype=object),\n",
       " array([1498, 'Inventing the Abbotts (1997)', 'Drama|Romance'],\n",
       "       dtype=object),\n",
       " array([3585, 'Great Locomotive Chase, The (1956)', 'Adventure|War'],\n",
       "       dtype=object),\n",
       " array([1105, 'Children of the Corn IV: The Gathering (1996)', 'Horror'],\n",
       "       dtype=object),\n",
       " array([3723, 'Hamlet (1990)', 'Drama'], dtype=object),\n",
       " array([461, 'Go Fish (1994)', 'Drama|Romance'], dtype=object),\n",
       " array([1519, 'Broken English (1996)', 'Drama'], dtype=object),\n",
       " array([1379, 'Young Guns II (1990)', 'Action|Comedy|Western'],\n",
       "       dtype=object),\n",
       " array([3367, \"Devil's Brigade, The (1968)\", 'War'], dtype=object),\n",
       " array([1007, 'Apple Dumpling Gang, The (1975)',\n",
       "        \"Children's|Comedy|Western\"], dtype=object),\n",
       " array([1846, 'Nil By Mouth (1997)', 'Drama'], dtype=object),\n",
       " array([1852, 'Love Walked In (1998)', 'Drama|Thriller'], dtype=object)]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###计算与该用户最相似的多个用户，并将这多个用户喜欢的电影推荐给该用户\n",
    "recommend_by_user(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (data[data['UserID'] == 4]).index\n",
    "# print(list(idx))\n",
    "movie_id_list = []\n",
    "for idx_i in idx:\n",
    "    if data.loc[idx_i]['ratings'] > 3:\n",
    "        movie_id_list.append(data.loc[idx_i]['MovieID'])\n",
    "# idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>ratings</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>10</td>\n",
       "      <td>914</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[4817, 5158, 3012, 2407, 2407, 2407, 2407, 240...</td>\n",
       "      <td>[12, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>10</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[2561, 2583, 2407, 2407, 2407, 2407, 2407, 240...</td>\n",
       "      <td>[11, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>10</td>\n",
       "      <td>2355</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[2543, 4295, 1252, 2407, 2407, 2407, 2407, 240...</td>\n",
       "      <td>[14, 0, 3, 18, 18, 18, 18, 18, 18, 18, 18, 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>10</td>\n",
       "      <td>1197</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[4536, 1618, 4455, 2407, 2407, 2407, 2407, 240...</td>\n",
       "      <td>[16, 13, 3, 17, 18, 18, 18, 18, 18, 18, 18, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8223</th>\n",
       "      <td>10</td>\n",
       "      <td>1287</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[1678, 2407, 2407, 2407, 2407, 2407, 2407, 240...</td>\n",
       "      <td>[16, 13, 11, 18, 18, 18, 18, 18, 18, 18, 18, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553727</th>\n",
       "      <td>10</td>\n",
       "      <td>2043</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[512, 4668, 4089, 1907, 639, 1140, 2407, 2407,...</td>\n",
       "      <td>[13, 0, 6, 18, 18, 18, 18, 18, 18, 18, 18, 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553885</th>\n",
       "      <td>10</td>\n",
       "      <td>2045</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[106, 881, 3258, 1252, 2407, 2407, 2407, 2407,...</td>\n",
       "      <td>[13, 0, 11, 17, 18, 18, 18, 18, 18, 18, 18, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553940</th>\n",
       "      <td>10</td>\n",
       "      <td>2046</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[5193, 350, 1907, 3740, 2407, 2407, 2407, 2407...</td>\n",
       "      <td>[13, 0, 1, 18, 18, 18, 18, 18, 18, 18, 18, 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554476</th>\n",
       "      <td>10</td>\n",
       "      <td>2047</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[5028, 4455, 2407, 2407, 2407, 2407, 2407, 240...</td>\n",
       "      <td>[0, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554498</th>\n",
       "      <td>10</td>\n",
       "      <td>2049</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[4993, 2122, 4455, 2407, 2407, 2407, 2407, 240...</td>\n",
       "      <td>[3, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>401 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        UserID  MovieID  ratings  Gender  Age  JobID  \\\n",
       "2252        10      914        5       0    3      1   \n",
       "2890        10     3408        4       0    3      1   \n",
       "4205        10     2355        4       0    3      1   \n",
       "5906        10     1197        5       0    3      1   \n",
       "8223        10     1287        3       0    3      1   \n",
       "...        ...      ...      ...     ...  ...    ...   \n",
       "553727      10     2043        5       0    3      1   \n",
       "553885      10     2045        3       0    3      1   \n",
       "553940      10     2046        4       0    3      1   \n",
       "554476      10     2047        4       0    3      1   \n",
       "554498      10     2049        5       0    3      1   \n",
       "\n",
       "                                                    Title  \\\n",
       "2252    [4817, 5158, 3012, 2407, 2407, 2407, 2407, 240...   \n",
       "2890    [2561, 2583, 2407, 2407, 2407, 2407, 2407, 240...   \n",
       "4205    [2543, 4295, 1252, 2407, 2407, 2407, 2407, 240...   \n",
       "5906    [4536, 1618, 4455, 2407, 2407, 2407, 2407, 240...   \n",
       "8223    [1678, 2407, 2407, 2407, 2407, 2407, 2407, 240...   \n",
       "...                                                   ...   \n",
       "553727  [512, 4668, 4089, 1907, 639, 1140, 2407, 2407,...   \n",
       "553885  [106, 881, 3258, 1252, 2407, 2407, 2407, 2407,...   \n",
       "553940  [5193, 350, 1907, 3740, 2407, 2407, 2407, 2407...   \n",
       "554476  [5028, 4455, 2407, 2407, 2407, 2407, 2407, 240...   \n",
       "554498  [4993, 2122, 4455, 2407, 2407, 2407, 2407, 240...   \n",
       "\n",
       "                                                   Genres  \n",
       "2252    [12, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1...  \n",
       "2890    [11, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1...  \n",
       "4205    [14, 0, 3, 18, 18, 18, 18, 18, 18, 18, 18, 18,...  \n",
       "5906    [16, 13, 3, 17, 18, 18, 18, 18, 18, 18, 18, 18...  \n",
       "8223    [16, 13, 11, 18, 18, 18, 18, 18, 18, 18, 18, 1...  \n",
       "...                                                   ...  \n",
       "553727  [13, 0, 6, 18, 18, 18, 18, 18, 18, 18, 18, 18,...  \n",
       "553885  [13, 0, 11, 17, 18, 18, 18, 18, 18, 18, 18, 18...  \n",
       "553940  [13, 0, 1, 18, 18, 18, 18, 18, 18, 18, 18, 18,...  \n",
       "554476  [0, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18...  \n",
       "554498  [3, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18...  \n",
       "\n",
       "[401 rows x 8 columns]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['UserID'] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
